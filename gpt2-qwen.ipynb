{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d904c6-1312-45a3-b16b-cd8248233355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"D:\\hf-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521022ed-c98f-4af3-bf65-7f980f166cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:07<00:00,  1.96s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from transformers import BitsAndBytesConfig  \n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    ")\n",
    "\n",
    "# ---- Device Setup ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---- Paths ----\n",
    "MODEL_PATH = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\"\n",
    "PEFT_ADAPTER_PATH = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune-peft-adapaters\"\n",
    "REF_MODEL_PATH = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\"\n",
    "\n",
    "# ---- 1) 4-bit Quantization Configuration ----\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# ---- 2) Load Base Model in 4-bit ----\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# Prepare the model for k-bit training (this typically freezes most parameters except adapter ones)\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "base_model.gradient_checkpointing_disable()  # Disable checkpointing\n",
    "\n",
    "# ---- 3) Load Tokenizer ----\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "qwen_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# ---- 4) Load the PEFT Adapter (LoRA) ----\n",
    "# This reloads your fine-tuned adapter weights onto your base model.\n",
    "model_with_lora = PeftModel.from_pretrained(base_model, PEFT_ADAPTER_PATH)\n",
    "\n",
    "# ---- 5) Convert to PPO-Compatible ValueHead Model ----\n",
    "# When converting, pass the peft_config from the adapter model to ensure proper initialization.\n",
    "qwen_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model_with_lora,\n",
    "    peft_config=lora_config\n",
    ").to(device)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6e4e0a6-1ef0-4b29-85cb-0151b8e528db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")\n",
    "\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "task = 'fact'\n",
    "\n",
    "evaluator = get_evaluator(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe71618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def convert_to_json(output_list, src_list=None, ref_list=None, context_list=None, \\\n",
    "            scores=None, doc_id=None, system_id=None):\n",
    "    \"\"\"\n",
    "        Convert the data into the json format.\n",
    "\n",
    "        output_list: a list of model output\n",
    "        src_list: source input for different NLG tasks. For example, source document for summarization\n",
    "                  and dialogue history for dialogue response generation\n",
    "        ref_list: human-annotated groundtruth\n",
    "        context_list: the context needed to evaluate several specific dimension. For example,\n",
    "                      additional factual information when evaluating engagingness and groundedness in dialogues\n",
    "        scores: human scores for evaluating the model output. They can be used to calculate the correlation\n",
    "                between evaluators and human judgements. The scores should be stored in a dictionary. For example,\n",
    "                {'fluency': 2.0, 'coherence': 3.0} could be the human score for a sample.\n",
    "        doc_id: the index of the input source. It can be used to calculate summary-level correlation for summarzation\n",
    "        system_id: the index of the generation system. It can be used to calculate system-level correlation.\n",
    "    \"\"\"\n",
    "    json_data = []\n",
    "    for i in range(len(output_list)):\n",
    "        cur = {}\n",
    "        cur['system_output'] = output_list[i]\n",
    "        if src_list is not None:\n",
    "            cur['source'] = src_list[i]\n",
    "        if ref_list is not None:\n",
    "            cur['reference'] = ref_list[i]\n",
    "        if context_list is not None:\n",
    "            cur['context'] = context_list[i]\n",
    "        if scores is not None:\n",
    "            cur['scores'] = scores[i]\n",
    "        if doc_id is not None:\n",
    "            cur['doc_id'] = doc_id[i]\n",
    "        if system_id is not None:\n",
    "            cur['system_id'] = system_id[i]\n",
    "        json_data.append(cur)\n",
    "    return json_data\n",
    "\n",
    "\n",
    "def add_question(dimension, output, src=None, ref=None, context=None, task=None):\n",
    "    \"\"\"\n",
    "        Add questions to generate input in Bool-QA format for UniEval.\n",
    "\n",
    "        dimension: specific dimension to be evaluated\n",
    "        src: source input for different NLG tasks. For example, source document for summarization\n",
    "             and dialogue history for dialogue response generation.\n",
    "        output: output text generated by the models\n",
    "        ref: human-annotataed groundtruth\n",
    "        context: the context needed to evaluate several specific dimension. For example,\n",
    "                 additional factual information when evaluating engagingness and groundedness in dialogues.\n",
    "    \"\"\"\n",
    "\n",
    "    input_with_question = []\n",
    "    for i in range(len(output)):\n",
    "        # For summarization\n",
    "        if task == 'summarization':\n",
    "            if dimension == 'fluency':\n",
    "                cur_input = 'question: Is this a fluent paragraph? </s> paragraph: ' + output[i]\n",
    "            elif dimension == 'coherence':\n",
    "                cur_input = 'question: Is this a coherent summary to the document? </s> summary: ' + output[i] + ' </s> document: ' + src[i]\n",
    "            elif dimension == 'consistency':\n",
    "                cur_input = 'question: Is this claim consistent with the document? </s> claim: ' + output[i] + ' </s> document: ' + src[i]\n",
    "            elif dimension == 'relevance':\n",
    "                cur_input = 'question: Is this summary relevant to the reference? </s> summary: ' + output[i] + ' </s> reference: ' + ref[i]\n",
    "            else:\n",
    "                raise NotImplementedError('The input format for this dimension is still undefined. Please customize it first.')\n",
    "        # For dialogues\n",
    "        elif task == 'dialogue':\n",
    "            if dimension == 'naturalness':\n",
    "                cur_input = 'question: Is this a natural response in the dialogue? </s> response: ' + output[i]\n",
    "            elif dimension == 'coherence':\n",
    "                cur_input = 'question: Is this a coherent response given the dialogue history? </s> response: '\\\n",
    "                            + output[i] + ' </s> dialogue history: ' + src[i]\n",
    "            elif dimension == 'engagingness':\n",
    "                cur_input = 'question: Is this an engaging and informative response according to the dialogue history and fact? </s> response: '\\\n",
    "                            + output[i] + ' </s> dialogue history: ' + src[i] + ' </s> fact: ' + context[i]\n",
    "            elif dimension == 'groundedness':\n",
    "                cur_input = 'question: Is this response consistent with knowledge in the fact? </s> response: '\\\n",
    "                            + output[i] + ' </s> fact: ' + context[i]\n",
    "            elif dimension == 'understandability':\n",
    "                cur_input = 'question: Is this an understandable response in the dialogue? </s> response: ' + output[i]\n",
    "            else:\n",
    "                raise NotImplementedError('The input format for this dimension is still undefined. Please customize it first.')\n",
    "        # For data-to-text\n",
    "        elif task == 'data2text':\n",
    "            if dimension == 'naturalness':\n",
    "                cur_input = 'question: Is this a fluent utterance? </s> utterance: ' + output[i]\n",
    "            elif dimension == 'informativeness':\n",
    "                cur_input = 'question: Is this sentence informative according to the reference? </s> sentence: '\\\n",
    "                            + output[i] + ' </s> reference: ' + ref[i]\n",
    "            else:\n",
    "                raise NotImplementedError('The input format for this dimension is still undefined. Please customize it first.')\n",
    "        # For factual consistency detection\n",
    "        elif task == 'fact':\n",
    "            if dimension == 'consistency':\n",
    "                cur_input = 'question: Is this claim consistent with the document? </s> claim: ' + output[i] + ' </s> document: ' + src[i]\n",
    "            else:\n",
    "                raise NotImplementedError('No other dimensions for the factual consistency detection task.')\n",
    "        # For new customized tasks\n",
    "        else:\n",
    "            raise NotImplementedError('Other tasks are not implemented, please customize specific tasks here.')\n",
    "        input_with_question.append(cur_input)\n",
    "    return input_with_question\n",
    "\n",
    "\n",
    "def print_scores(scores):\n",
    "    table = PrettyTable(['Dimensions','Score'])\n",
    "    print('\\nEvaluation scores are shown below:')\n",
    "    dims = list(scores[0].keys())\n",
    "    for dim in dims:\n",
    "        cur_score = 0\n",
    "        for i in range(len(scores)):\n",
    "            cur_score += scores[i][dim]\n",
    "        table.add_row([dim, round(cur_score / len(scores), 6)])\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f30faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import sent_tokenize\n",
    "from scorer import UniEvaluator  # Make sure this import works after placing scorer.py in the same directory\n",
    "\n",
    "def evaluate(data, dims=None, overall=True, print_result=False, model_name_or_path=\"t5-small\", task='summarization', device='cuda:0', individual=True):\n",
    "    \"\"\"\n",
    "    Get the scores of all the given dimensions (fluency, consistency, coherence, relevance)\n",
    "\n",
    "    data: A list of dictionaries, where each dictionary contains:\n",
    "          - 'source': The original text\n",
    "          - 'system_output': The generated system output (summary)\n",
    "          - 'reference' (optional): Reference summary for relevance evaluation\n",
    "\n",
    "    dims: A list of dimensions to be evaluated. If dims is None, it evaluates four default dimensions:\n",
    "          coherence, consistency, fluency, relevance.\n",
    "\n",
    "    overall: Boolean to indicate whether the overall score is calculated as the average of all dimensions.\n",
    "\n",
    "    print_result: Boolean to print the results on the screen.\n",
    "\n",
    "    model_name_or_path: The model name or path to use for evaluation, e.g., 't5-small'\n",
    "\n",
    "    task: The task type (used in scoring if needed, like summarization or other NLP tasks).\n",
    "\n",
    "    device: The device to use for evaluation ('cpu' or 'cuda:0').\n",
    "    \"\"\"\n",
    "\n",
    "    # Instantiate the scorer\n",
    "    scorer = UniEvaluator(model_name_or_path=model_name_or_path, device=device)\n",
    "\n",
    "    n_data = len(data)\n",
    "    eval_scores = [{} for _ in range(n_data)]\n",
    "\n",
    "    # Default dimensions if not provided\n",
    "    if dims is None:\n",
    "        dims = ['coherence', 'consistency', 'fluency', 'factual consistency']   #add relevance\n",
    "\n",
    "    for dim in dims:\n",
    "        print(f'Evaluating {dim} of {n_data} samples !!!')\n",
    "\n",
    "        if dim == 'consistency' or dim == 'fluency':\n",
    "            # Sentence-level scores for consistency and fluency\n",
    "            src_list, output_list = [], []\n",
    "            n_sents = []  # number of sentences in each summary\n",
    "\n",
    "            for i in range(n_data):\n",
    "                if dim == 'consistency':\n",
    "                    source = data[i]['source']\n",
    "                else:\n",
    "                    source = ''\n",
    "                system_outputs = sent_tokenize(data[i]['system_output'])\n",
    "                n_sents.append(len(system_outputs))\n",
    "                for j in range(len(system_outputs)):\n",
    "                    src_list.append(source)\n",
    "                    output_list.append(system_outputs[j])\n",
    "\n",
    "            input_list = add_question(dimension=dim, output=output_list, src=src_list, task=task)\n",
    "            sent_score = scorer.score(input_list)\n",
    "\n",
    "            # Calculate average sentence-level scores for each sample\n",
    "            start_idx = 0\n",
    "            score = []\n",
    "            for cur_n_sent in n_sents:\n",
    "                score.append(sum(sent_score[start_idx:start_idx + cur_n_sent]) / cur_n_sent)\n",
    "                start_idx += cur_n_sent\n",
    "\n",
    "        elif dim == 'coherence' or dim == 'relevance':\n",
    "            # Summary-level scores for coherence and relevance\n",
    "            src_list, output_list, ref_list = [], [], []\n",
    "\n",
    "            for i in range(n_data):\n",
    "                src_list.append(data[i]['source'])\n",
    "                output_list.append(data[i]['system_output'])\n",
    "                if dim == 'relevance':\n",
    "                    ref_list.append(data[i]['reference'])\n",
    "\n",
    "            input_list = add_question(dimension=dim, output=output_list, src=src_list, ref=ref_list, task=task)\n",
    "            score = scorer.score(input_list)\n",
    "\n",
    "        elif dim == 'factual consistency':\n",
    "            output_list, src_list = [], []\n",
    "\n",
    "            for i in range(n_data):\n",
    "                src_list.append(data[i]['source'])\n",
    "                output_list.append(data[i]['system_output'])\n",
    "\n",
    "            data = convert_to_json(output_list=output_list, src_list=src_list)\n",
    "            eval_score = evaluator.evaluate(data)\n",
    "            score = []\n",
    "\n",
    "            for i in eval_score:\n",
    "                temp = i['consistency']\n",
    "                score.append(temp)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"The input format for the dimension '{dim}' is still undefined. Please customize it.\")\n",
    "\n",
    "        # Store the scores for the current dimension\n",
    "        for i in range(n_data):\n",
    "            eval_scores[i][dim] = score[i]\n",
    "\n",
    "    # Calculate overall score (average of all evaluated dimensions)\n",
    "    if overall:\n",
    "        for i in range(n_data):\n",
    "            eval_scores[i]['overall'] = np.mean([eval_scores[i][dim] for dim in dims])\n",
    "\n",
    "    # Print the result if requested\n",
    "    if print_result:\n",
    "        print_scores(eval_scores)\n",
    "\n",
    "    if individual:\n",
    "        individual_scores = []\n",
    "        for i in range(n_data):\n",
    "            temp = [eval_scores[i][dim] for dim in dims]\n",
    "            individual_scores.append(temp)\n",
    "\n",
    "        return np.array(individual_scores)\n",
    "\n",
    "    # Calculate average score across all the dimensions except 'overall'\n",
    "    avg_score = []\n",
    "    for i in range(n_data):\n",
    "        # Exclude 'overall' from the averaging\n",
    "        dimensions = [dim for dim in dims if dim != 'overall']\n",
    "        avg_score.append(np.mean([eval_scores[i][dim] for dim in dimensions]))\n",
    "\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "731a7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDICAL_PROMPTS = {\n",
    "    \"CHIEF COMPLAINT\": {\n",
    "        \"instruction\": \"Extract the patient's primary concern or reason for seeking care and keep it short please (e.g., 'chest pain', 'fatigue'). Include duration if noted.\",\n",
    "        \"example\": \"Chief Complaint: 'Sharp headache for 3 days'\"\n",
    "    },\n",
    "    \"HISTORY OF PRESENT ILLNESS\": {\n",
    "        \"instruction\": \"Detail onset, timing, severity, and progression of symptoms. Include aggravating/alleviating factors and prior treatments.\",\n",
    "        \"example\": \"HPI: Headache started 3 days ago, throbbing, worsens with light. Took ibuprofen with partial relief.\"\n",
    "    },\n",
    "    \"MEDICATIONS\": {\n",
    "        \"instruction\": \"List all current medications with dosages and purposes (e.g., 'Lisinopril 10mg daily for HTN').\",\n",
    "        \"example\": \"Meds: Aspirin 81mg daily, Metformin 500mg BID\"\n",
    "    },\n",
    "    \"VITALS\": {\n",
    "        \"instruction\": \"Extract vital signs with units and timing (e.g., 'BP 120/80 mmHg'). Flag abnormalities.\",\n",
    "        \"example\": \"Vitals: Temp 98.6Â°F, HR 72, BP 118/76\"\n",
    "    },\n",
    "    \"PHYSICAL EXAM\": {\n",
    "        \"instruction\": \"Summarize key exam findings (abnormal/normal). Include systems examined (e.g., 'Lungs: clear bilaterally').\",\n",
    "        \"example\": \"PE: Heart RRR, no murmurs. Abdomen soft, non-tender.\"\n",
    "    },\n",
    "    \"PATIENT INSTRUCTIONS\": {\n",
    "        \"instruction\": \"Extract discharge/follow-up instructions (e.g., 'Avoid NSAIDs, call if fever develops').\",\n",
    "        \"example\": \"Instructions: Rest, monitor for confusion, return if headache persists >48h.\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bc2555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model, tokenizer, convo, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    \n",
    "    prompts = []\n",
    "    keys = []\n",
    "\n",
    "    for prompt_key, prompt in MEDICAL_PROMPTS.items():\n",
    "        full_prompt = (\n",
    "            f\"{prompt['instruction']} Extract the information from below conversation. \"\n",
    "            \"Only include the relevant information in your response please. Keep responses \"\n",
    "            \"as short as you can while including all important information.\\n\"\n",
    "            f\"Example: {prompt['example']} Follow the example's format for generating your \"\n",
    "            \"response in that format and don't include any facts from the example in your \"\n",
    "            f\"response. Don't include any dialogues from the conversation\\n\\n\"\n",
    "            f\"{convo}\\n\\n\"\n",
    "        )\n",
    "        prompts.append(full_prompt)\n",
    "        keys.append(prompt_key)\n",
    "\n",
    "    # Tokenize all prompts together (batch)\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    ).to(device)\n",
    "\n",
    "    generation_kwargs = {\n",
    "        \"do_sample\": False,\n",
    "        \"num_beams\": 1,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"max_new_tokens\": 32,\n",
    "        \"return_dict_in_generate\": True,\n",
    "        \"output_scores\": False\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "\n",
    "    # Decode outputs\n",
    "    decoded = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the generated parts (strip prompt)\n",
    "    responses = {}\n",
    "    for i, key in enumerate(keys):\n",
    "        prompt_len = len(prompts[i])\n",
    "        responses[key] = decoded[i][prompt_len:].strip()\n",
    "\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd9dff-d691-4cde-90e5-6fc17244d51a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [00:00<00:00, 267.47 examples/s]\n",
      "WARNING:root:The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n",
      "  0%|                                                                                           | 0/46 [00:00<?, ?it/s]C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:427: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.35it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 13.36it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 67.44it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating factual consistency of 2 samples !!!\n",
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                        | 1/3 [00:00<00:01,  1.38it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 2/3 [00:01<00:00,  1.39it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:01<00:00,  1.50it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing query 0: Tensors must have same number of dimensions: got 1 and 2\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 25.19it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                               | 1/4 [00:00<00:00,  9.83it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 13.80it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 67.12it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating factual consistency of 2 samples !!!\n",
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                               | 1/4 [00:00<00:02,  1.40it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 2/4 [00:01<00:01,  1.40it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 3/4 [00:02<00:00,  1.39it/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.64it/s]\u001b[A\n",
      "  2%|â–ˆâ–‹                                                                              | 1/46 [08:51<6:38:17, 531.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing query 1: Tensors must have same number of dimensions: got 1 and 2\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, TaskType\n",
    "import bitsandbytes as bnb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Add UniEval to path and import\n",
    "sys.path.append(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"combined_clinical_notes.csv\"\n",
    "MODEL_PATH = \"gpt2\" \n",
    "GENERATION_PROMPT = \"Using the context given below, generate a short, concise and fluent section that fits the Header below.\"\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(eval_df.rename(columns={\"dialogue\": \"review\"}))\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_function(example):\n",
    "    # Encode and decode in one function\n",
    "    input_ids = qwen_tokenizer.encode(\n",
    "        example[\"review\"], \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=2693\n",
    "    )[0]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"query\": qwen_tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    }\n",
    "\n",
    "# Apply in one map operation\n",
    "dataset = dataset.map(preprocess_function, batched=False)\n",
    "dataset.set_format(\"pytorch\")\n",
    "\n",
    "# Model configuration\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# PEFT/LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(base_model, peft_config=lora_config).to(\"cuda\")\n",
    "\n",
    "# Reference model\n",
    "ref_model = create_reference_model(model).to(\"cuda\")\n",
    "ref_model.eval()\n",
    "for param in ref_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "NUM_CANDIDATES = 2\n",
    "\n",
    "# PPO Configuration - removed kl_penalty that caused the error\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=MODEL_PATH,\n",
    "    ppo_epochs=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    steps=5,\n",
    "    batch_size=1*NUM_CANDIDATES,\n",
    "    mini_batch_size=1*NUM_CANDIDATES,\n",
    "    learning_rate=2e-5,\n",
    "    log_with=None,\n",
    ")\n",
    "\n",
    "# Initialize PPO Trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    optimizer=bnb.optim.Adam8bit(model.parameters(), lr=ppo_config.learning_rate)\n",
    ")\n",
    "\n",
    "def get_score(src, res):\n",
    "    \"\"\"Calculate rewards based on evaluation scores\"\"\"\n",
    "    data = convert_to_json(\n",
    "        output_list=res,\n",
    "        src_list=src,\n",
    "    )\n",
    "    \n",
    "    raw = evaluate(data, overall=False)\n",
    "    score = [\n",
    "        [d[0], d[1], d[2], d[3]]\n",
    "        for d in raw\n",
    "    ]\n",
    "    scores = np.array(score, dtype=np.float32)\n",
    "    k = len(res)\n",
    "    dom_counts = np.zeros(k)\n",
    "    \n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            if i == j:\n",
    "                continue\n",
    "            # Check dominance: i dominates j if all scores are >= and at least one is >\n",
    "            if np.all(scores[i] >= scores[j]) and np.any(scores[i] > scores[j]):\n",
    "                dom_counts[i] += 1\n",
    "    \n",
    "    # Convert to [-1, 1] range reward\n",
    "    max_dom = k - 1\n",
    "    if max_dom > 0:\n",
    "        rewards = 2 * (dom_counts / max_dom) - 1\n",
    "    else:\n",
    "        rewards = np.zeros(k)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "max_position_embeddings = model.pretrained_model.config.max_position_embeddings \n",
    "\n",
    "# Training loop\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"eos_token_id\": -1,\n",
    "    \"max_length\": max_position_embeddings,\n",
    "}\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "COMPUTE_DTYPE = torch.float32  # Using float32 to avoid dtype issues\n",
    "\n",
    "for epoch in range(1):\n",
    "    for batch_idx, batch in enumerate(tqdm(ppo_trainer.dataloader)):\n",
    "        logs, game_data = dict(), dict()\n",
    "        game_data[\"query\"] = [q for q in batch[\"query\"]]\n",
    "        query_tensors = [input_ids for input_ids in batch[\"input_ids\"]]\n",
    "        \n",
    "        # Process one query at a time to prevent batch size mismatch\n",
    "        for query_idx, query in enumerate(query_tensors):\n",
    "            # Generate NUM_CANDIDATES responses for this query\n",
    "            responses = []\n",
    "            decoded_responses = []\n",
    "            \n",
    "            original_notes = game_data[\"query\"][query_idx]\n",
    "            data = generate_responses(\n",
    "                model=qwen_model,\n",
    "                tokenizer=qwen_tokenizer,\n",
    "                convo=original_notes\n",
    "            )\n",
    "            \n",
    "            for _ in range(NUM_CANDIDATES):\n",
    "                summary = []\n",
    "                temp_res = []\n",
    "\n",
    "                for prompt_key, prompt in data.items():\n",
    "                    full_prompt = f\"{GENERATION_PROMPT}\\nHeader:{prompt_key}\\nContext:{prompt}\"\n",
    "                    full_prompt_tensor = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(\"cuda\").squeeze(0)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        response = ppo_trainer.generate(\n",
    "                            full_prompt_tensor,\n",
    "                            **generation_kwargs\n",
    "                        )\n",
    "                    # Ensure response doesn't exceed max length\n",
    "                    response = response[:, :64]\n",
    "                    temp_res.append(response)\n",
    "\n",
    "                    text = tokenizer.decode(response.squeeze(), skip_special_tokens=True)\n",
    "                    summary.append(f\"{prompt_key}\\n{text}\\n\")\n",
    "\n",
    "                summary = '\\n'.join(summary)\n",
    "                if temp_res:  # Only if temp_res is not empty\n",
    "                    combined_response = torch.cat(temp_res, dim=0)  # Combine along appropriate dimension\n",
    "                    responses.append(combined_response.squeeze())  # Append squeezed version\n",
    "                \n",
    "                # Decode for evaluation\n",
    "                decoded_responses.append(summary)\n",
    "            \n",
    "            # Calculate rewards\n",
    "            try:\n",
    "                rewards = get_score(\n",
    "                    [game_data[\"query\"][query_idx]] * NUM_CANDIDATES, \n",
    "                    decoded_responses\n",
    "                )\n",
    "                \n",
    "                # Prepare data for PPO step\n",
    "                flat_queries = [query] * NUM_CANDIDATES\n",
    "                flat_responses = responses\n",
    "                flat_rewards = [torch.tensor([r], device=DEVICE, dtype=COMPUTE_DTYPE) for r in rewards]\n",
    "                \n",
    "                # Verify sizes match\n",
    "                if len(flat_queries) == NUM_CANDIDATES and len(flat_responses) == NUM_CANDIDATES and len(flat_rewards) == NUM_CANDIDATES:\n",
    "                    # Do PPO step for this query\n",
    "                    stats = ppo_trainer.step(\n",
    "                        queries=flat_queries,\n",
    "                        responses=flat_responses,\n",
    "                        scores=flat_rewards\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"Query {query_idx} in batch {batch_idx} - PPO step successful!\")\n",
    "                    print(f\"Sample output: {decoded_responses[0][:100]}...\")\n",
    "                    avg_reward = np.mean([r.item() for r in flat_rewards])\n",
    "                    print(f\"Average reward: {avg_reward:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Skipping query {query_idx} due to size mismatch: queries={len(flat_queries)}, responses={len(flat_responses)}, rewards={len(flat_rewards)}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # print(f\"Error processing query {query_idx}: {e}\")\n",
    "                continue\n",
    "                \n",
    "    print(f\"âœ… Epoch {epoch+1}/1 complete\")\n",
    "    \n",
    "print(\"ðŸŽ‰ PPO fine-tuning done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d9abf-ba31-4f69-9a1b-1b219279ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo_trainer.model.pretrained_model.save_pretrained(\"D:\\kshitij-weights-folder\\gpt-2-tuned-ppo\")\n",
    "# tokenizer.save_pretrained(\"D:\\kshitij-weights-folder\\gpt-2-tuned-ppo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e958d-e7e3-466a-9961-7a6163a44cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kshitij)",
   "language": "python",
   "name": "kshitij"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
