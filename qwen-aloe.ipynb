{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\bmsce cse.desktop-iub6tha\\appdata\\roaming\\python\\python312\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\bmsce cse.desktop-iub6tha\\appdata\\roaming\\python\\python312\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\bmsce cse.desktop-iub6tha\\appdata\\roaming\\python\\python312\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\bmsce cse.desktop-iub6tha\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True if CUDA is available\n",
    "print(torch.version.cuda)         # Should print the installed CUDA version\n",
    "print(torch.backends.cudnn.enabled)  # Should be True if cuDNN is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 18 17:44:44 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 572.61                 Driver Version: 572.61         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000             WDDM  |   00000000:47:00.0 Off |                  Off |\n",
      "| 30%   35C    P8             21W /  300W |     499MiB /  49140MiB |      3%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2060    C+G   ....0.3065.92\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A            3136    C+G   ...ekyb3d8bbwe\\CopilotNative.exe      N/A      |\n",
      "|    0   N/A  N/A            4048    C+G   ...yb3d8bbwe\\WindowsTerminal.exe      N/A      |\n",
      "|    0   N/A  N/A            6620    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A            9036    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A           10428    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           13456    C+G   ...em32\\ApplicationFrameHost.exe      N/A      |\n",
      "|    0   N/A  N/A           17280    C+G   ...crosoft\\OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A           21068    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Fri_Feb_21_20:42:46_Pacific_Standard_Time_2025\n",
      "Cuda compilation tools, release 12.8, V12.8.93\n",
      "Build cuda_12.8.r12.8/compiler.35583870_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|                                                              | 0/4 [00:00<?, ?it/s]C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.cache\\huggingface\\hub\\models--HPAI-BSC--Qwen2.5-Aloe-Beta-7B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|█████████████████████████████████████████████████████| 4/4 [08:45<00:00, 131.43s/it]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████| 4/4 [00:11<00:00,  2.77s/it]\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.cache\\huggingface\\hub\\datasets--akemiH--NoteChat. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Generating train split: 100%|██████████████████████████████| 207001/207001 [00:13<00:00, 14997.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# 1) 4-bit quant config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# 2) Load base model in 4-bit\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# 3) Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# 4) Enable gradient checkpointing\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# 5) Prepare data with smaller sequence length\n",
    "notechat = load_dataset(\"akemiH/NoteChat\")\n",
    "trainB = notechat[\"train\"].select(range(3000))\n",
    "evalB = notechat[\"train\"].select(range(3000, 3500))\n",
    "\n",
    "MAX_LENGTH = 256  # smaller sequence length\n",
    "\n",
    "class ConversationSummaryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_length=MAX_LENGTH):\n",
    "        self.data = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation = str(self.data[idx][\"conversation\"])\n",
    "        summary = str(self.data[idx][\"data\"])\n",
    "        text = f\"CONVERSATION:\\n{conversation}\\n\\nSUMMARY: {summary}\"\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = encodings[\"input_ids\"].squeeze()\n",
    "        attention_mask = encodings[\"attention_mask\"].squeeze()\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "train_dataset = ConversationSummaryDataset(trainB, tokenizer)\n",
    "eval_dataset = ConversationSummaryDataset(evalB, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\AppData\\Local\\Temp\\ipykernel_12340\\3972098163.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 50:43, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.163300</td>\n",
       "      <td>1.149429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.122600</td>\n",
       "      <td>1.140940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=1.163391393025716, metrics={'train_runtime': 3049.7956, 'train_samples_per_second': 1.967, 'train_steps_per_second': 0.246, 'total_flos': 6.5209332400128e+16, 'train_loss': 1.163391393025716, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./aloe_qwen\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,      # accumulate to compensate for small batch\n",
    "    evaluation_strategy=\"epoch\",       # you can skip or reduce evaluation\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_drop_last=True,\n",
    "    gradient_checkpointing=True,       # from the Trainer side as well\n",
    ")\n",
    "\n",
    "# 7) Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 8) Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./aloe_qwen\\\\tokenizer_config.json',\n",
       " './aloe_qwen\\\\special_tokens_map.json',\n",
       " './aloe_qwen\\\\vocab.json',\n",
       " './aloe_qwen\\\\merges.txt',\n",
       " './aloe_qwen\\\\added_tokens.json',\n",
       " './aloe_qwen\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./aloe_qwen\")\n",
    "tokenizer.save_pretrained(\"./aloe_qwen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████| 4/4 [00:11<00:00,  2.93s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# -- Path to your LoRA weights + tokenizer --\n",
    "model_dir = \"./aloe_qwen\"  \n",
    "\n",
    "# -- 4-bit quantization config (same as training) --\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# -- 1) Load the *base* Qwen2.5 model in 4-bit --\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# -- 2) Load your fine-tuned LoRA adapters into the base model --\n",
    "# The directory should contain adapter_model.bin, adapter_config.json, etc.\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# -- 3) Load the tokenizer you saved to ./aloe_qwen --\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:651: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXTRACTED SUMMARY:\n",
      " - **Symptoms**: Fever, dry cough, dyspnea\n",
      "- **Physical Activities**: Position changes, deep breathing, breathing exercises, strength and walking exercises\n",
      "- **Medical History**: No relevant information provided\n",
      "- **Surgical History**: No relevant information provided\n",
      "- **Treatment Plan History**: Physical therapy, gradual position changes, breathing exercises adapted to avoid coughing and oxygen desaturation, low-level strength and walking exercises with frequent breaks to prevent deoxygenation. Exercise progression increased daily until hospital discharge to a rehabilitation clinic. Doctor will continue to monitor patient's progress and update on any changes in symptoms. 1. Patient presented with symptoms of fever, dry cough, and dyspnea.\n",
      "2. He was diagnosed with moderate ARDS from COVID-19 and hospitalized.\n",
      "3. During physical therapy, he experienced difficulties with position changes and deep breathing, leading to coughing and shortness of breath.\n",
      "4. To avoid rapid deterioration and respiratory failure, the patient was instructed to change positions slowly and step-by-step.\n",
      "5. This approach increased his oxygen saturation, such as on day 5 with 6 L/min of oxygen from 93% to 97%.\n",
      "6. Breathing exercises were adapted to avoid prolonged coughing and oxygen desaturation, with the patient stopping every deep breath before coughing and holding his breath for better air distribution.\n",
      "7. The patient performed the breathing exercises well and managed to increase his oxygen saturation.\n",
      "8. He had difficulty maintaining sufficient oxygen saturation during physical activity, but with close monitoring and frequent breaks, he was able to perform low-level strength and walking exercises without significant deoxygenation.\n",
      "9. The exercise progression was low on days 1 to 5, but then increased daily until the patient's hospital discharge to a rehabilitation clinic on day 10.\n",
      "10. The doctor will continue to monitor the patient's progress and update on any changes in symptoms. 1. Patient presented with symptoms of\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 1) Create a conversation + summary prompt\n",
    "conversation_text = \"\"\"\n",
    "Doctor: Hi, Mr. X, I'm Dr. Y. How are you feeling today?\n",
    "Patient: Not too good, doctor. I've been feeling really sick lately.\n",
    "Doctor: I understand. Can you tell me what symptoms you're experiencing?\n",
    "Patient: Yes, I've been having a fever, a dry cough, and dyspnea.\n",
    "Doctor: I see. You were hospitalized due to moderate ARDS from COVID-19, is that correct?\n",
    "Patient: Yes, that's correct.\n",
    "Doctor: During your physical therapy, we encountered some difficulties. Can you tell me more about that?\n",
    "Patient: Yes, I had trouble with position changes and deep breathing. Every time I tried to change my position or take a deep breath, I would start coughing and it would make me really short of breath.\n",
    "Doctor: I understand. To avoid rapid deterioration and respiratory failure, we instructed you to change positions very slowly and step-by-step, right?\n",
    "Patient: Yes, that's right.\n",
    "Doctor: And I see that this approach increased your oxygen saturation, for example, on day 5 with 6 L/min of oxygen from 93% to 97%.\n",
    "Patient: Yes, that's correct.\n",
    "Doctor: Good. We also had to adapt your breathing exercises to avoid prolonged coughing and oxygen desaturation. Can you tell me more about that?\n",
    "Patient: Yes, I was instructed to stop every deep breath before coughing and to hold my breath for better air distribution.\n",
    "Doctor: I see that you performed the breathing exercises well and managed to increase your oxygen saturation.\n",
    "Patient: Yes, I did my best.\n",
    "Doctor: You also had difficulty maintaining sufficient oxygen saturation during physical activity, is that correct?\n",
    "Patient: Yes, I did. But with close monitoring and frequent breaks, I was able to perform low-level strength and walking exercises without any significant deoxygenation.\n",
    "Doctor: I see that your exercise progression was low on days 1 to 5, but then increased daily until your hospital discharge to a rehabilitation clinic on day 10.\n",
    "Patient: Yes, that's correct.\n",
    "Doctor: Great. I'd like to keep monitoring your progress and see how you're doing. Can you keep me updated on any changes in your symptoms?\n",
    "Patient: Yes, of course, doctor.\n",
    "Doctor: Alright, let's keep in touch. If you have any questions or concerns, don't hesitate to reach out to me.\n",
    "Patient: Thank you, doctor.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "CONVERSATION:\n",
    "{conversation_text}\n",
    "\n",
    "SUMMARY:\n",
    "- **Symptoms**:\n",
    "  (Include symptoms mentioned in the conversation, if any.)\n",
    "\n",
    "- **Physical Activities**:\n",
    "  (Include details about physical activities or excercise habits mentioned in the conversation, if any.)\n",
    "\n",
    "- **Medical History**:\n",
    "  (Include any relavent medical history mentioned in the conversation, if any.)\n",
    "\n",
    "- **Surgical History**\n",
    "  (Include any details about past surgeries mentioned in the conversation, if any.)\n",
    "\n",
    "- **Treatment Plan History**\n",
    "  (Include any details about past treatment plans or suggestions made by healthcare professionals, including medications, surgeries, therapies, or lifestyle changes, if any.)\n",
    "\n",
    "Provide the summary in plain text with the above attributes, and include information only if it is mentioned in the conversation. If any attribute is not discussed, omit it. Only include the summary in the generated response.Format the summary properly.\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "# Create a text-generation pipeline\n",
    "# (You can name it 'summarizer' if you wish, but it's using text-generation.)\n",
    "text_gen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",   # place on GPU if available\n",
    ")\n",
    "# Now generate text using the pipeline\n",
    "response = text_gen_pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=400,\n",
    "    do_sample=False,  # Deterministic for demonstration\n",
    ")\n",
    "\n",
    "# The output is a list of dict(s); each has \"generated_text\"\n",
    "generated_text = response[0][\"generated_text\"]\n",
    "# print(\"FULL OUTPUT:\\n\", generated_text)\n",
    "\n",
    "# If the model replicates your training format, you'll see something like:\n",
    "# \"CONVERSATION: ... SUMMARY: <the actual summary text>\"\n",
    "# You can strip off everything before \"SUMMARY:\" if you only want the summary.\n",
    "if \"Summary:\" in generated_text:\n",
    "    final_summary = generated_text.split(\"Summary:\", 1)[-1].strip()\n",
    "else:\n",
    "    final_summary = generated_text\n",
    "\n",
    "print(\"\\nEXTRACTED SUMMARY:\\n\", final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXTRACTED SUMMARY:\n",
      " - **Symptoms**:\n",
      "  (Include symptoms mentioned in the conversation, if any.)\n",
      "\n",
      "- **Physical Activities**:\n",
      "  (Include details about physical activities or excercise habits mentioned in the conversation, if any.)\n",
      "\n",
      "- **Medical History**:\n",
      "  (Include any relavent medical history mentioned in the conversation, if any.)\n",
      "\n",
      "- **Surgical History**\n",
      "  (Include any details about past surgeries mentioned in the conversation, if any.)\n",
      "\n",
      "- **Treatment Plan History**\n",
      "  (Include any details about past treatment plans or suggestions made by healthcare professionals, including medications, surgeries, therapies, or lifestyle changes, if any.)\n",
      "\n",
      "Provide the summary in plain text with the above attributes, and include information only if it is mentioned in the conversation. If any attribute is not discussed, omit it. Only include the summary in the generated response.Format the summary properly.\n",
      "\n",
      "\n",
      "Summary: The patient presented with a persistent fever, dry cough, and severe breathlessness. She had a history of hypertension, diabetes mellitus, and chronic obstructive pulmonary disease. She was admitted to the hospital due to her worsening condition. The patient reported feeling weak and sick for the past two weeks. She was given physical therapy, which included educating her about dyspnea-relieving positions and the importance of regular mobilization and deep-breathing exercises. However, her anxiety from fear of dying and worries about her future made her dyspnea worse. After receiving education and relaxing breathing exercises, her respiratory rate decreased from 30 breaths/min to 22 breaths/min, and her oxygen saturation increased from 92% to 96% on 4 L/min oxygen. She regained her self-confidence and was able to walk 350 m without a walking aid or supplemental oxygen before her discharge home. No specific treatment plan, medications, surgeries, or lifestyle changes were mentioned in the conversation. The patient's medical history included hypertension, diabetes mellitus, and chronic obstructive pulmonary disease. No surgical history was provided. The patient's treatment plan history included physical therapy, but no other details were mentioned. 1. Patient presented with persistent fever, dry cough, and severe breathlessness.\n",
      "2. History of hypertension, diabetes mellitus, and chronic obstructive pulmonary disease.\n",
      "3. Admitted to the hospital due to worsening condition.\n",
      "4. Physical therapy included educating about dyspnea-relieving positions and deep-breathing exercises.\n",
      "5. Anxiety from fear of dying and worries about future made dyspnea worse.\n",
      "6. Education and relaxing breathing exercises improved respiratory rate and oxygen saturation.\n",
      "7. Regained self-confidence and walked 350 m without aid or supplemental oxygen.\n",
      "8. No specific treatment plan, medications, surgeries, or lifestyle changes mentioned.\n",
      "9. Medical history includes hypertension, diabetes mellitus, and chronic obstructive pulmonary disease.\n",
      "10. No\n"
     ]
    }
   ],
   "source": [
    "conversation_text = \"\"\"\n",
    "Doctor: Hello, I am Dr. Smith. Can you tell me what brings you to the hospital today?\n",
    "Patient: Yes, I have been feeling very weak and sick for the past two weeks. I have a persistent fever and dry cough.\n",
    "Doctor: I see. And how is your breathing?\n",
    "Patient: It's been shallow and rapid, especially when I am at rest. And I get severely breathless even with minor physical activities.\n",
    "Doctor: Okay. I understand. You were given physical therapy, right?\n",
    "Patient: Yes, they focused on educating me about dyspnea-relieving positions and the importance of regular mobilization and deep-breathing exercises.\n",
    "Doctor: That's good. And how did it go?\n",
    "Patient: It became evident that my anxiety from fear of dying and worries about my future was making my dyspnea worse. I was so dyspneic, anxious, and weak that I could barely walk to the toilet.\n",
    "Doctor: I see. But your physical therapist helped you with that, right?\n",
    "Patient: Yes, they listened to me, explained why I was experiencing breathlessness, and tested suitable positions to relieve my dyspnea. I felt better after the education and relaxing breathing exercises.\n",
    "Doctor: That's great to hear. Can you tell me more about the improvement?\n",
    "Patient: On day 2, my respiratory rate reduced from 30 breaths/min to 22 breaths/min and my oxygen saturation increased from 92% to 96% on 4 L/min oxygen after some deep-breathing exercises.\n",
    "Doctor: That's impressive. And how did it go after that?\n",
    "Patient: My dyspnea and anxiety started to alleviate and I regained my self-confidence. The therapy was shifted to walking and strength training, and I was able to walk 350 m without a walking aid or supplemental oxygen before my discharge home.\n",
    "Doctor: That's fantastic. You have made a great progress. I am glad to hear that.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "CONVERSATION:\n",
    "{conversation_text}\n",
    "\n",
    "SUMMARY:\n",
    "- **Symptoms**:\n",
    "  (Include symptoms mentioned in the conversation, if any.)\n",
    "\n",
    "- **Physical Activities**:\n",
    "  (Include details about physical activities or excercise habits mentioned in the conversation, if any.)\n",
    "\n",
    "- **Medical History**:\n",
    "  (Include any relavent medical history mentioned in the conversation, if any.)\n",
    "\n",
    "- **Surgical History**\n",
    "  (Include any details about past surgeries mentioned in the conversation, if any.)\n",
    "\n",
    "- **Treatment Plan History**\n",
    "  (Include any details about past treatment plans or suggestions made by healthcare professionals, including medications, surgeries, therapies, or lifestyle changes, if any.)\n",
    "\n",
    "Provide the summary in plain text with the above attributes, and include information only if it is mentioned in the conversation. If any attribute is not discussed, omit it. Only include the summary in the generated response.Format the summary properly.\\n\\n\n",
    "\"\"\"\n",
    "\n",
    "response = text_gen_pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=400,\n",
    "    do_sample=False,  # Deterministic for demonstration\n",
    ")\n",
    "\n",
    "# The output is a list of dict(s); each has \"generated_text\"\n",
    "generated_text = response[0][\"generated_text\"]\n",
    "# print(\"FULL OUTPUT:\\n\", generated_text)\n",
    "\n",
    "# If the model replicates your training format, you'll see something like:\n",
    "# \"CONVERSATION: ... SUMMARY: <the actual summary text>\"\n",
    "# You can strip off everything before \"SUMMARY:\" if you only want the summary.\n",
    "if \"SUMMARY:\" in generated_text:\n",
    "    final_summary = generated_text.split(\"SUMMARY:\", 1)[-1].strip()\n",
    "else:\n",
    "    final_summary = generated_text\n",
    "\n",
    "print(\"\\nEXTRACTED SUMMARY:\\n\", final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.71s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# -- Path to your LoRA weights + tokenizer --\n",
    "model_dir = \"./aloe_qwen\"  \n",
    "\n",
    "# -- 4-bit quantization config (same as training) --\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# -- 1) Load the *base* Qwen2.5 model in 4-bit --\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# -- 2) Load your fine-tuned LoRA adapters into the base model --\n",
    "# The directory should contain adapter_model.bin, adapter_config.json, etc.\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# -- 3) Load the tokenizer you saved to ./aloe_qwen --\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Generating Summaries:   0%|          | 0/501 [00:00<?, ?it/s]/home/jovyan/miniconda/envs/py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/miniconda/envs/py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jovyan/miniconda/envs/py311/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating Summaries:   2%|▏         | 10/501 [03:33<2:53:49, 21.24s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Generating Summaries:  56%|█████▌    | 280/501 [1:38:41<1:17:57, 21.16s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",  # Automatically place on GPU if available\n",
    ")\n",
    "\n",
    "notechat = load_dataset(\"akemiH/NoteChat\")\n",
    "eval_df = notechat[\"train\"].select(range(5000, 7001)).to_pandas()\n",
    "\n",
    "# 2) Clean up missing data if present\n",
    "if eval_df.isnull().values.any():\n",
    "    print(\"Found missing values in the evaluation set. Dropping them.\")\n",
    "    eval_df = eval_df.dropna()\n",
    "\n",
    "# 3) Prepare batching parameters\n",
    "batch_size = 4\n",
    "num_samples = len(eval_df)\n",
    "num_batches = (num_samples // batch_size) + int(num_samples % batch_size != 0)\n",
    "\n",
    "# Lists to store predictions and references\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# 4) Batching loop to generate summaries\n",
    "for i in tqdm(range(num_batches), desc=\"Generating Summaries\"):\n",
    "    start = i * batch_size\n",
    "    end = min(start + batch_size, num_samples)\n",
    "    \n",
    "    # Extract conversation and reference summary columns\n",
    "    batch_conversations = eval_df[\"conversation\"][start:end].tolist()\n",
    "    batch_refs = eval_df[\"data\"][start:end].tolist()  # \"data\" column for references\n",
    "    \n",
    "    # Prepare prompts replicating training format\n",
    "    prompts = [\n",
    "        f\"CONVERSATION:\\n{conv}\\n\\nSUMMARY:\"\n",
    "        for conv in batch_conversations\n",
    "    ]\n",
    "    \n",
    "    # Generate summaries\n",
    "    results = summarizer(\n",
    "        prompts,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        truncation=True,\n",
    "        num_return_sequences=1  # Default is 1\n",
    "    )\n",
    "    \n",
    "    # Parse results and extract summaries\n",
    "    for item in results:\n",
    "        # 'item' is a list with 1 dict => {\"generated_text\": \"...\"}\n",
    "        output_dict = item[0]\n",
    "        full_text = output_dict[\"generated_text\"]\n",
    "        \n",
    "        # Extract only the part after \"SUMMARY:\"\n",
    "        if \"SUMMARY:\" in full_text:\n",
    "            extracted_summary = full_text.split(\"SUMMARY:\", 1)[-1].strip()\n",
    "        else:\n",
    "            extracted_summary = full_text  # Fallback if marker not found\n",
    "        \n",
    "        predictions.append(extracted_summary)\n",
    "    \n",
    "    # Append the reference texts\n",
    "    references.extend(batch_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROUGE scores\n",
    "results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "print(\"ROUGE scores:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### predictions[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predictions[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The patient presented with a 12-month history of a purple nodular rash on her nasolabial folds. She reported no itching, burning, pain, or bleeding from the area, but did report drainage of clear fluid when applying pressure to the area. She has a history of cirrhosis, iron deficiency anemia, type II diabetes mellitus, hypertension, asthma, and endometrioid carcinoma of the ovary. A 3-mm punch biopsy of the right nasolab'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[1021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list = eval_df[\"conversation\"].tolist()\n",
    "ref_list = eval_df[\"data\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jovyan/UniEval\") \n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "data = convert_to_json(\n",
    "    src_list=src_list,\n",
    "    ref_list=ref_list,\n",
    "    output_list=output_list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = []\n",
    "for pred in predictions:\n",
    "    # Ensure that \"Summary:\" exists in the string to avoid errors\n",
    "    if len(pred) > 0:\n",
    "        output_list.append(pred)\n",
    "    else:\n",
    "        # Handle cases where \"Summary:\" is missing (optional)\n",
    "        output_list.append(\"\")  # Or handle differently based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = [\n",
    "    entry for entry in data\n",
    "    if entry[\"system_output\"].strip()  # Ensure non-empty system_output\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/jovyan/UniEval\") \n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "task = \"summarization\"\n",
    "evaluator = get_evaluator(task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating coherence of 2001 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/251 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "100%|██████████| 251/251 [05:44<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2001 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [21:27<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2001 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171/1171 [01:30<00:00, 12.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2001 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [03:41<00:00,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation scores are shown below:\n",
      "+-------------+----------+\n",
      "|  Dimensions |  Score   |\n",
      "+-------------+----------+\n",
      "|  coherence  | 0.966174 |\n",
      "| consistency | 0.884436 |\n",
      "|   fluency   | 0.886275 |\n",
      "|  relevance  | 0.820143 |\n",
      "|   overall   | 0.889257 |\n",
      "+-------------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_scores = evaluator.evaluate(filtered_data, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to evaluation_data1.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Suppose `filtered_data` is the final list of dicts you want to store:\n",
    "# filtered_data = [\n",
    "#   {\"source\": \"...\", \"reference\": \"...\", \"system_output\": \"...\"},\n",
    "#   ...\n",
    "# ]\n",
    "\n",
    "with open(\"evaluation_data1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    # Write list of dictionaries to JSON file\n",
    "    json.dump(filtered_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Data saved to evaluation_data1.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"evaluation_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    filtered_data = json.load(f)\n",
    "\n",
    "print(\"Data loaded. Number of samples:\", len(filtered_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kshitij)",
   "language": "python",
   "name": "kshitij"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
