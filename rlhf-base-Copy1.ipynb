{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZL9wcvcle3iT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZL9wcvcle3iT",
    "outputId": "6926df5c-52dd-4601-8af5-f324fc285fd2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install trl==0.7.4\n",
    "# !pip install datasets\n",
    "# !pip install transformers==4.38.2\n",
    "# !pip install peft==0.10.0\n",
    "# !pip install accelerate==0.28.0\n",
    "# test commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf67fc5a-c458-484f-869a-bfb2154e6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"D:\\hf-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa80b0e",
   "metadata": {
    "id": "2fa80b0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "def set_seed(seed_val=42):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "#Configuration options\n",
    "train_batch_size = 16\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 1e-5\n",
    "eval_batch_size = 1\n",
    "eval_steps = 500\n",
    "max_input_length = 550\n",
    "save_steps = 1000\n",
    "num_train_epochs = 20\n",
    "random.seed(42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6b3d82-34f1-4718-9d02-ef80e8ca1625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018c6ca",
   "metadata": {
    "id": "4018c6ca"
   },
   "source": [
    "## Creating the policy model for human Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ccc7e5-8a0e-4fb5-8cc6-4d9f4b9cac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined_clinical_notes.csv\")\n",
    "\n",
    "# First split: 60% train, 40% temp (eval + test)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "\n",
    "# Second split: 50% of temp (which is 20% of total) for eval, 50% for test\n",
    "eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d885cf72-3d15-48a5-b1a3-8e885b2537a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(column)  # Prints each column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670ff5e",
   "metadata": {
    "id": "0670ff5e"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1) 4-bit quant config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# 2) Load base model in 4-bit\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# 3) Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# 4) Enable gradient checkpointing\n",
    "# model.enable_input_require_grads()\n",
    "# model.gradient_checkpointing_enable()\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# 5) Prepare data with smaller sequence length\n",
    "# notechat = load_dataset(\"akemiH/NoteChat\")\n",
    "# trainB = notechat[\"train\"].select(range(3000))\n",
    "# evalB = notechat[\"train\"].select(range(3000, 3500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efa4266-3b5d-4c72-a1ef-663004a4ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLDRDataset(Dataset):\n",
    "    def __init__(self, train_path, tokenizer, split, max_length=4000):\n",
    "        self.post_list = []\n",
    "        dataset = train_df\n",
    "        self.labels = []\n",
    "\n",
    "        for sample in dataset.iterrows():\n",
    "            self.post_list.append(sample[1][\"dialogue\"])\n",
    "            self.labels.append(sample[1][\"note\"])\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.post_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.post_list[idx]\n",
    "        summary = self.labels[idx]\n",
    "        # label = self.labels[idx]\n",
    "\n",
    "        # encodings_dict = self.tokenizer(txt, truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "        # input_ids = torch.tensor(encodings_dict[\"input_ids\"])\n",
    "        # attn_masks = torch.tensor(encodings_dict[\"attention_mask\"])\n",
    "        # return {\n",
    "        #     \"input_ids\": input_ids,\n",
    "        #     \"attention_mask\": attn_masks,\n",
    "        #     \"labels\": labels_ids,\n",
    "        # }\n",
    "\n",
    "        txt = f\"CONVERSATION:\\n{conversation}\\n\\nSUMMARY: \\n{summary}\" #IMPORTANT!!!!!!!!!!\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            txt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "         # Labels should be the same as input_ids for causal LM training\n",
    "        # The model will automatically shift labels internally\n",
    "        encodings[\"labels\"] = encodings[\"input_ids\"].clone()\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": encodings[\"labels\"].squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a73ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328,
     "referenced_widgets": [
      "c0d5a5aca66640fabb8e8b4c6fb59392",
      "0d238f7dc3544a248e568b01b2b8f68b",
      "360c56bdbb1a4cf698cd6ee02fc0377c",
      "ec18abfee9fb4920919fff399355653a",
      "17774ec2b2814c989e559645c66c16dc",
      "3e9a90d9c01e41faa39e765d077940ad",
      "a7a3771791064414b7baa55dea4f9307",
      "f9140d1590e4477fa2a0f6ee0f0a29d4",
      "4f64e2615eb34e9e8bb496845194c4f0",
      "cec141a0a52a44029af02766578a7a86",
      "dd692ebfdd374157a28212e3c58dfdee",
      "edb52e11236a466ba3180a6700fcbe47",
      "d8806f44019a4be995bf8a00575a884c",
      "cd702006f8d64566b59db92ced9fa9a8",
      "010b68f7a4e74db1a691e73c6c0b668d",
      "effdcdf181e4451695eeef03a6994093",
      "d8d51ab046d8479fb6c55d58beaeab69",
      "dce9a626e1ac48faa5b3da071cf39bf1",
      "9a88ceb27ac54478b1806f1417e3f56e",
      "4b1628d06c674e7e9542e4de56942440",
      "5b80c804ea87468aa681a4346c9cfb3e",
      "54e1f5359144444faad92a5af1208b20",
      "3c9db78225634f4ba56dd620875a6152",
      "6df03bc0feae4dcf8c0f5f539e837aa3",
      "98e9fab21039484cb336ce0f781d1f7f",
      "a7ee81d9ba61402fb8f18bb17416f640",
      "3c7c8b864d5942a58a476a7123e4c919",
      "24d9d364d21b47d8b19217cf28958b98",
      "4ff64e6f540e4900b40e564330dd0192",
      "e408dbc1a67a4dcf89e3876437a35da2",
      "697def36105e47f987bc8595102a45ce",
      "8b78c25aecda45db921b0351401d0bf9",
      "d08ee3cbabfd44819328f076bb075337",
      "68cb590000b94ab397dc97cdfc05c4b6",
      "154be340981b436c9caf3289aa6a39b4",
      "f088edd1f0474d1a97ad0c008f4cfd43",
      "76104008746b45a59c7065a90fef5ffd",
      "09c0865087fb4cadb82bedd9e4fe224f",
      "32467bbd99394b86b3d5db9cc6572791",
      "b9f148a70e67439aa7208c8580d3f843",
      "f323e77044ae48c6a3d88ea620735366",
      "37429d08cb6a48adadab9e1459d274d2",
      "cde30c20c0774408851a356a23412864",
      "e3b0bb095a1e405cbc48908ca48bdb7b",
      "69a7a00f8f1b48948ada90f62fcdc9a6",
      "04dd6f2423b04009b0b53e45eb24b1a7",
      "74734805983e42c9906bf8cd10b2974e",
      "fc633082a04e4480bafec26d1c2ccb7a",
      "bb9ddba4dd9b4ddbbf65cfa91f2b521d",
      "281ed0eb1bed44cb837b0977b6c50e0b",
      "c9587d181b8f499ea983f53d38e4d1f5",
      "e6e5ade438ab45c8a05e9f68a1838380",
      "7db18b1334424e84ac6c64003b7a91ed",
      "25ca2e5e59134e45ae4addccde4dc618",
      "ee3c992c640c481cbdb5927cb5b113da",
      "92382a78fc554b2a94f3a99fa064bf3a",
      "437363a2959441e19842e19b67388db1",
      "d8a8c1f8b5a347408ffe1bed07e87c6d",
      "00ebae8e2b0a46539b3bc297976a1f35",
      "dd35c1969a73488789164f4c59d7b1de",
      "b2b1a073a7b14f759bda8df3bae6db1f",
      "ffe0b0cc000444c98d0d43a1f9d4843c",
      "6ea4187ab5124426b662badc73ac1b10",
      "b2e22b238f214dca87827d6b3b5eaeca",
      "d29cfa7e295f4949b100d3a21126174f",
      "2850146e7d3f42109539343d97c936c6",
      "666db4066bec4d7581cc19e6f5bc98bc",
      "72df76a9cdf8459cb63f67d992b10749",
      "f6409c1155eb4b92998e20999b642c04",
      "58f0850759b8457b9657d36b326ddf2e",
      "1bc25a94b13b4189b8fc3f7ce41b9086",
      "6ee1b4bc76cb4106962fb29f05cc0825",
      "76d96ad5ee9344978487022753a3adfa",
      "6f272cac47734ea9abb212d32a65c382",
      "a5ec7fadd5bc439db5119626b8e6afbb",
      "71e7360d713c4c70a33e9fd9e0cb8412",
      "c21e2109534b4e1cb176332e1db6e59f",
      "7a473fedaf4645cdaf78189b1bfbdd6d",
      "5afba4b64a114f948309aa11d3ba5c34",
      "2accfcbededc4afa817fc4beedaee153",
      "477d5050997045318236932dad8d8418",
      "cfa386b4863f4fd29120e79be0e317ce",
      "788c6efc3c41484d9559b57690401142",
      "b026473caa4f4234b8b90c2d7965115b",
      "b6109109cd9746dfb361860ae6d26cac",
      "ee699d35db9d4b6ab0a74713ad5929d4",
      "812bf7bc5a6f4985b0ecc0de236b45a0",
      "c592e8dcb00349c6817b05382263c784"
     ]
    },
    "id": "7f0a73ab",
    "outputId": "24038f05-5422-492e-9a38-dca119a91b2d"
   },
   "outputs": [],
   "source": [
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# model.config.end_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Disable caching (already done, but double-check)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Enable gradient checkpointing (already done, but confirm)\n",
    "# model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6aad5",
   "metadata": {
    "id": "dbc6aad5"
   },
   "outputs": [],
   "source": [
    "# Set up the datasets\n",
    "data_path = \"NA\"\n",
    "train_dataset = TLDRDataset(\n",
    "    data_path,\n",
    "    tokenizer,\n",
    "    \"train\",\n",
    "    max_length=256,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a19aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f16a19aa",
    "outputId": "37314f8f-f319-4d58-d968-cfc62bf5ab89"
   },
   "outputs": [],
   "source": [
    "for i in train_dataset:\n",
    "    print(i[\"input_ids\"], i[\"labels\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500301ac",
   "metadata": {
    "id": "500301ac"
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rPWzhVtfjXQf",
   "metadata": {
    "id": "rPWzhVtfjXQf"
   },
   "outputs": [],
   "source": [
    "output_dir = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune-logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3801788",
   "metadata": {
    "id": "d3801788"
   },
   "outputs": [],
   "source": [
    "# Prepare the trainer and start training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "#     per_device_eval_batch_size=eval_batch_size,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_drop_last=True,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefacaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baefacaf",
    "outputId": "917e4e82-2c45-4cc7-da64-356b9f5a5dfb"
   },
   "outputs": [],
   "source": [
    "training_args.device.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba4710",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "90ba4710",
    "outputId": "398ff6cf-790c-4b2c-eae7-c4e92a1776f1"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     data_collator=default_data_collator,\n",
    "#     preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "# trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc50c2",
   "metadata": {
    "id": "3dfc50c2"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\")   ##path to save policy model\n",
    "tokenizer.save_pretrained(r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\")\n",
    "model.save_pretrained(r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune-peft-adapaters\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373c19c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b373c19c",
    "outputId": "caf7af75-5a02-47b4-b1ef-d5421c6e5ae8"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune-logs\")\n",
    "model_path = \"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation=True, max_length=256, padding=\"max_length\")\n",
    "text = train_df.iloc[2][\"dialogue\"]\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\", max_length=256)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92769534",
   "metadata": {
    "id": "92769534"
   },
   "source": [
    "# Policy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da46d612-6e91-4dd6-a60b-2866f28c58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"D:\\hf-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5960a04b-8333-4f8a-a994-69a3ffc99d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.38.2\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft, trl\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ransformers (C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb9cb217",
   "metadata": {
    "id": "bb9cb217"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from trl import RewardTrainer\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a848656c-e395-4f29-9f09-abb5f954f169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>note</th>\n",
       "      <th>source_file</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>aci</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doctor] so tyler is a 56 -year-old male who p...</td>\n",
       "      <td>SUBJECTIVE\\n\\nDifficulty swallowing. Tyler Gre...</td>\n",
       "      <td>src_experiment_data\\test1_aci_asrcorr.csv</td>\n",
       "      <td>ACI084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset encounter_id                                           dialogue  \\\n",
       "246     aci          NaN  [doctor] so tyler is a 56 -year-old male who p...   \n",
       "\n",
       "                                                  note  \\\n",
       "246  SUBJECTIVE\\n\\nDifficulty swallowing. Tyler Gre...   \n",
       "\n",
       "                                   source_file      id  \n",
       "246  src_experiment_data\\test1_aci_asrcorr.csv  ACI084  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA = r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\"\n",
    "df = pd.read_csv(DATA)\n",
    "\n",
    "# First split: 60% train, 40% temp (eval + test)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "\n",
    "# Second split: 50% of temp (which is 20% of total) for eval, 50% for test\n",
    "eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acdf91a2-6afb-4851-9a94-d19f84b3a83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in the combined DataFrame:\n",
      "['dataset', 'encounter_id', 'dialogue', 'note', 'source_file', 'id']\n"
     ]
    }
   ],
   "source": [
    "# Print column names\n",
    "print(\"\\nColumns in the combined DataFrame:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50d0acdb",
   "metadata": {
    "id": "50d0acdb"
   },
   "outputs": [],
   "source": [
    "##model path\n",
    "# MODEL_PATH = \"/content/drive/MyDrive/Medical Dialogue Summarization using PPO/rm_model\"\n",
    "MODEL_PATH = r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\bart_clinical_ft-20250422T192130Z-001\\bart_clinical_ft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "221cfe68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "221cfe68",
    "outputId": "1af608fc-8caa-4ecd-a8af-01a8a93b0e44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dataset', 'encounter_id', 'dialogue', 'note', 'source_file', 'id', '__index_level_0__'],\n",
       "    num_rows: 93\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_parquet(DATA_PATH)\n",
    "df = eval_df\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05f02097",
   "metadata": {
    "id": "05f02097"
   },
   "outputs": [],
   "source": [
    "sentiment_pipe_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\"}\n",
    "project_kwargs={\"logging_dir\": r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\bart_clinical_ft-20250422T192130Z-001\\bart_clinical_ft\"}\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=MODEL_PATH, ppo_epochs=1, project_kwargs=project_kwargs, gradient_accumulation_steps=2, steps=5, batch_size=2, mini_batch_size=1, learning_rate=2e-5, log_with='tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40d0ef6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "f59bcdcc99824470838d6c093f6061fa",
      "d1e7370235914644b4e34f8e356f1ebe",
      "8c131a1b756548d5939a8a5894fbba7c",
      "d66be74e2016462e9baee27000846749",
      "be99a761d5e1420183726cbe2f2b55fb",
      "52e28851f0734471aea1ec9499beb604",
      "d081bb434b5b4c8686cc619daf882f66",
      "36e71ffb4c3f42d7975df82640e9c516",
      "6643e7adf5c244e381c006481eb35bc5",
      "14d521c73308473a8d2545161f94803f",
      "98c188482dfe4186b4c773b71e04f38e",
      "0a59b21aae274e0fb5ee9d157f597eaf",
      "ac068c8e2b3c49e8ae5f992242e78d62",
      "66f8821275514240bdcf57de251b4c79",
      "127e339498a044fcaaac227870016a5d",
      "9f7cca39981d4f0c8337e61ff21051a0",
      "addd3e7d9ed04683abbd41e0d25fd688",
      "8b3f7c16d26345c6bae32f6b34a21b34",
      "b10bad707ecb404489f17c6b17e55e45",
      "b97db836726546e6b43c8bb7dfc90a88",
      "69f8ca185b9745b99021f46aacc0356a",
      "babe18d669f74c3cb3b86366ca30077e"
     ]
    },
    "id": "40d0ef6a",
    "outputId": "6b1c6a00-25c1-468f-921e-323c4f2f4b97"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.rename_columns({\"dialogue\": \"review\"})\n",
    "# dataset = dataset.filter(lambda x: len(x[\"review\"]) > 500, batched=False)\n",
    "# dataset = dataset.map(lambda x: {\"review\": x[\"review\"][:1000]}, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "96d7e8b8",
   "metadata": {
    "id": "96d7e8b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\bart_clinical_ft-20250422T192130Z-001\\bart_clinical_ft and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Windows path — make sure to use raw string (r\"\") or double backslashes\n",
    "model_path = r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\bart_clinical_ft-20250422T192130Z-001\\bart_clinical_ft\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3adda4dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "bf1bf1a8c2814ecfaebff9ce79f86353",
      "6aebb82a54b94c87abef4fd6cee0c937",
      "5eafb42972e547daaeef0b61f1d618ff",
      "ef80d6a48e954ff7ae331b023ab58d22",
      "ecf7babf2d6b48c1bb6926744e48c2e8",
      "0a1b881a6adf451c807e87691e5a1400",
      "bc6fc3d2cccc4913802b246607faac33",
      "746ed22975c94ad082c96b9af706f3ad",
      "092e5609feae49298fc5a532063136b7",
      "0d594dbec6234e3ead4f0805ce5c543c",
      "d9ca0062679e43479df26ace0ffe516c",
      "1d66a0fa2f3c4cf8a058fbf8e1110cd6",
      "7cb98c108517437bbbf26f0c4762089a",
      "eef5a555586c4469824cfc65d5442782",
      "fff6d847783447e6af81f9210c21d37e",
      "3a54b543eb9b4ae9a090d930cc551105",
      "591433576ea0480baac815922c4ee5db",
      "9cab486ed55440508317cf085b0f6030",
      "b4dd31eeab314ee2a6e62bb8a7f2d650",
      "f8fc58edbfbe4d50ab2e8a68d61a52dc",
      "4afbf8d1750b465b9ac084e4bc2123f3",
      "cacbbb65aaf4437584288cfb5fc71ec5"
     ]
    },
    "id": "3adda4dc",
    "outputId": "48f4f27a-c03d-4d01-8769-cb26883a3eb9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████| 93/93 [00:00<00:00, 163.28 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████| 93/93 [00:00<00:00, 649.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "txt_in_len = 5\n",
    "txt_out_len = 32\n",
    "seed = 1\n",
    "\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"input_ids\": tokenizer.encode(x[\"review\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=2693)[0]},\n",
    "    batched=False,\n",
    ")\n",
    "dataset = dataset.map(lambda x: {\"query\": tokenizer.decode(x[\"input_ids\"], skip_special_tokens=True)}, batched=False)\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# dataset = Dataset.from_dict(dataset)\n",
    "dataset.set_format(\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e189a22b",
   "metadata": {
    "id": "e189a22b"
   },
   "outputs": [],
   "source": [
    "def collator(data):\n",
    "    # Create base batch with all existing fields\n",
    "    \n",
    "    batch = {\n",
    "        key: [d[key] for d in data] \n",
    "        for key in data[0]  # Gets all keys from first item\n",
    "    }\n",
    "    \n",
    "    # Add 'note' field (replace None with your default value if needed)\n",
    "    # batch['note'] = [d.get('note', None) for d in data]\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6b9db94-0421-4632-98e1-50437cdf4e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
    "        \"query\": [item[\"query\"] for item in batch],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e36852d7-2923-420a-a4da-c2464a7aecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9af5cda-6919-4b1c-bedb-28f86f9c4d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 4/4 [00:09<00:00,  2.41s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     17\u001b[39m bnb_config = BitsAndBytesConfig(\n\u001b[32m     18\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     19\u001b[39m     bnb_4bit_compute_dtype=torch.float16,\n\u001b[32m     20\u001b[39m     bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     21\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ---- 2) Load Base Model in 4-bit ----\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m base_model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Prepare the model for k-bit training (this typically freezes most parameters except adapter ones)\u001b[39;00m\n\u001b[32m     31\u001b[39m base_model = prepare_model_for_kbit_training(base_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:561\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    560\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    565\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    566\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    567\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\modeling_utils.py:3558\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   3556\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mskip_keys\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect.signature(dispatch_model).parameters:\n\u001b[32m   3557\u001b[39m         device_map_kwargs[\u001b[33m\"\u001b[39m\u001b[33mskip_keys\u001b[39m\u001b[33m\"\u001b[39m] = model._skip_keys_device_placement\n\u001b[32m-> \u001b[39m\u001b[32m3558\u001b[39m     \u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdevice_map_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3560\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3561\u001b[39m     hf_quantizer.postprocess_model(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\accelerate\\big_modeling.py:499\u001b[39m, in \u001b[36mdispatch_model\u001b[39m\u001b[34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[39m\n\u001b[32m    497\u001b[39m     device = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmusa:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device != \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    501\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    503\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\modeling_utils.py:2534\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2530\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(torch.nn.Module.to)\n\u001b[32m   2531\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   2532\u001b[39m     \u001b[38;5;66;03m# Checks if the model has been loaded in 8-bit\u001b[39;00m\n\u001b[32m   2533\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquantization_method\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == QuantizationMethod.BITS_AND_BYTES:\n\u001b[32m-> \u001b[39m\u001b[32m2534\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2535\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2536\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2537\u001b[39m         )\n\u001b[32m   2538\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquantization_method\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == QuantizationMethod.GPTQ:\n\u001b[32m   2539\u001b[39m         \u001b[38;5;66;03m# For GPTQ models, we prevent users from casting the model to another dytpe to restrict unwanted behaviours.\u001b[39;00m\n\u001b[32m   2540\u001b[39m         \u001b[38;5;66;03m# the correct API should be to load the model with the desired dtype directly through `from_pretrained`.\u001b[39;00m\n\u001b[32m   2541\u001b[39m         dtype_present_in_args = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: `.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the model has already been set to the correct devices and casted to the correct `dtype`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from transformers import BitsAndBytesConfig  \n",
    "\n",
    "# ---- Device Setup ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---- Paths ----\n",
    "#MODEL_PATH = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\"\n",
    "PEFT_ADAPTER_PATH = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune-peft-adapaters\"\n",
    "REF_MODEL_PATH = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\"\n",
    "\n",
    "# ---- 1) 4-bit Quantization Configuration ----\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# ---- 2) Load Base Model in 4-bit ----\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# Prepare the model for k-bit training (this typically freezes most parameters except adapter ones)\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "base_model.gradient_checkpointing_disable()  # Disable checkpointing\n",
    "\n",
    "# ---- 3) Load Tokenizer ----\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# ---- 4) Load the PEFT Adapter (LoRA) ----\n",
    "# This reloads your fine-tuned adapter weights onto your base model.\n",
    "#model_with_lora = PeftModel.from_pretrained(base_model, PEFT_ADAPTER_PATH)\n",
    "\n",
    "# ---- 5) Convert to PPO-Compatible ValueHead Model ----\n",
    "# When converting, pass the peft_config from the adapter model to ensure proper initialization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a38d5-02b9-456c-9cbc-31f07bf6f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "starcoder_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH) ## tokenizer of step 1 model., here since we are using same model for step 1 and 2 it doesnot matter\n",
    "starcoder_tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc612f5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc612f5f",
    "outputId": "90e94b73-72fb-4459-b57a-079b6c2cb6db"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "\n",
    "# # starcoder_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"/content/drive/MyDrive/Medical Dialogue Summarization using PPO/summarization_policy_new\")  ##policy model from step 1\n",
    "# starcoder_model = AutoModelForCausalLMWithValueHead.from_pretrained(MODEL_PATH).to(device)\n",
    "\n",
    "# # IMPORTANT!!!!\n",
    "# # model = AutoModelForCausalLMWithValueHead.from_pretrained(peft_model, peft_config=lora_config)\n",
    "\n",
    "# # starcoder_model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(rf_model_path).to(device) ## reward model from step 2\n",
    "\n",
    "\n",
    "# starcoder_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH) ## tokenizer of step 1 model., here since we are using same model for step 1 and 2 it doesnot matter\n",
    "# starcoder_tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2836e5cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2836e5cc",
    "outputId": "06a8772a-dbbf-4a46-a318-72de186beb71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset',\n",
       " 'encounter_id',\n",
       " 'review',\n",
       " 'note',\n",
       " 'source_file',\n",
       " 'id',\n",
       " '__index_level_0__',\n",
       " 'input_ids',\n",
       " 'query']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5ca919b8-0142-4384-8c77-0fba508afc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dataset', 'encounter_id', 'review', 'note', 'source_file', 'id', '__index_level_0__', 'input_ids', 'query'],\n",
       "    num_rows: 93\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2a6de6a5-5fe0-4a79-9d21-17401a986a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import AutoModelForSeq2SeqLMWithValueHead\n",
    "\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2e1a2e38",
   "metadata": {
    "id": "2e1a2e38"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "# optimizer = torch.optim.SGD(starcoder_model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "optimizer = bnb.optim.Adam8bit(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=config,\n",
    "    model=model,\n",
    "    ref_model=model,  # Or use a separate reference model for KL divergence\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    data_collator=custom_collate_fn,\n",
    "    optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3366a32e-601a-44f8-9dbd-6038c72b23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c658238-98df-4d35-ba86-9077d782c1d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 579.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(ppo_trainer.dataloader):\n",
    "    print(len(batch['query']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e329e292",
   "metadata": {
    "id": "e329e292"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[145]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m ctrl_str = [\u001b[33m\"\u001b[39m\u001b[33m[negative]\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m[positive]\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      4\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# this should be handled by accelerate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m ctrl_tokens = \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarcoder_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mctrl_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[145]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      3\u001b[39m ctrl_str = [\u001b[33m\"\u001b[39m\u001b[33m[negative]\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m[positive]\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      4\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# this should be handled by accelerate\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m ctrl_tokens = \u001b[38;5;28mdict\u001b[39m((s, \u001b[43mstarcoder_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m ctrl_str)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "#NOT NECESSARY RIGHT NOW\n",
    "\n",
    "ctrl_str = [\"[negative]\", \"[positive]\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # this should be handled by accelerate\n",
    "ctrl_tokens = dict((s, starcoder_tokenizer.encode(s, return_tensors=\"pt\").squeeze().to(device)) for s in ctrl_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "597ac197",
   "metadata": {
    "id": "597ac197"
   },
   "outputs": [],
   "source": [
    "#NOT NECESSARY RIGHT NOW\n",
    "\n",
    "def pos_logit_to_reward(logit, task):\n",
    "    \"\"\"\n",
    "    Take the positive sentiment logit and scale it for the task.\n",
    "        task [negative]: reward = -logit\n",
    "        task [neutral]: reward = -2*abs(logit)+4\n",
    "        task [positive]: reward = logit\n",
    "    \"\"\"\n",
    "    for i in range(len(logit)):\n",
    "        if task[i] == \"[negative]\":\n",
    "            pass\n",
    "        elif task[i] == \"[positive]\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"task has to be in [0, 1, 2]!\")\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8a916542",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8a916542",
    "outputId": "19a2edc3-91f8-4de2-8e35-a3b534d7ab3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 4.])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_logit_to_reward(torch.Tensor([4, 4]), ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2974cb44",
   "metadata": {
    "id": "2974cb44"
   },
   "outputs": [],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 32,\n",
    "    \"eos_token_id\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "Zfm2IAxW9TQo",
   "metadata": {
    "id": "Zfm2IAxW9TQo"
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def convert_to_json(output_list, src_list=None, ref_list=None, context_list=None, \\\n",
    "            scores=None, doc_id=None, system_id=None):\n",
    "    \"\"\"\n",
    "        Convert the data into the json format.\n",
    "\n",
    "        output_list: a list of model output\n",
    "        src_list: source input for different NLG tasks. For example, source document for summarization\n",
    "                  and dialogue history for dialogue response generation\n",
    "        ref_list: human-annotated groundtruth\n",
    "        context_list: the context needed to evaluate several specific dimension. For example,\n",
    "                      additional factual information when evaluating engagingness and groundedness in dialogues\n",
    "        scores: human scores for evaluating the model output. They can be used to calculate the correlation\n",
    "                between evaluators and human judgements. The scores should be stored in a dictionary. For example,\n",
    "                {'fluency': 2.0, 'coherence': 3.0} could be the human score for a sample.\n",
    "        doc_id: the index of the input source. It can be used to calculate summary-level correlation for summarzation\n",
    "        system_id: the index of the generation system. It can be used to calculate system-level correlation.\n",
    "    \"\"\"\n",
    "    json_data = []\n",
    "    for i in range(len(output_list)):\n",
    "        cur = {}\n",
    "        cur['system_output'] = output_list[i]\n",
    "        if src_list is not None:\n",
    "            cur['source'] = src_list[i]\n",
    "        if ref_list is not None:\n",
    "            cur['reference'] = ref_list[i]\n",
    "        if context_list is not None:\n",
    "            cur['context'] = context_list[i]\n",
    "        if scores is not None:\n",
    "            cur['scores'] = scores[i]\n",
    "        if doc_id is not None:\n",
    "            cur['doc_id'] = doc_id[i]\n",
    "        if system_id is not None:\n",
    "            cur['system_id'] = system_id[i]\n",
    "        json_data.append(cur)\n",
    "    return json_data\n",
    "\n",
    "\n",
    "def add_question(dimension, output, src=None, ref=None, context=None, task=None):\n",
    "    \"\"\"\n",
    "        Add questions to generate input in Bool-QA format for UniEval.\n",
    "\n",
    "        dimension: specific dimension to be evaluated\n",
    "        src: source input for different NLG tasks. For example, source document for summarization\n",
    "             and dialogue history for dialogue response generation.\n",
    "        output: output text generated by the models\n",
    "        ref: human-annotataed groundtruth\n",
    "        context: the context needed to evaluate several specific dimension. For example,\n",
    "                 additional factual information when evaluating engagingness and groundedness in dialogues.\n",
    "    \"\"\"\n",
    "\n",
    "    input_with_question = []\n",
    "    for i in range(len(output)):\n",
    "        # For summarization\n",
    "        if task == 'summarization':\n",
    "            if dimension == 'fluency':\n",
    "                cur_input = 'question: Is this a fluent paragraph? </s> paragraph: ' + output[i]\n",
    "            elif dimension == 'coherence':\n",
    "                cur_input = 'question: Is this a coherent summary to the document? </s> summary: ' + output[i] + ' </s> document: ' + src[i]\n",
    "            elif dimension == 'consistency':\n",
    "                cur_input = 'question: Is this claim consistent with the document? </s> claim: ' + output[i] + ' </s> document: ' + src[i]\n",
    "            elif dimension == 'relevance':\n",
    "                cur_input = 'question: Is this summary relevant to the reference? </s> summary: ' + output[i] + ' </s> reference: ' + ref[i]\n",
    "            else:\n",
    "                raise NotImplementedError('The input format for this dimension is still undefined. Please customize it first.')\n",
    "        # For dialogues\n",
    "        elif task == 'dialogue':\n",
    "            if dimension == 'naturalness':\n",
    "                cur_input = 'question: Is this a natural response in the dialogue? </s> response: ' + output[i]\n",
    "            elif dimension == 'coherence':\n",
    "                cur_input = 'question: Is this a coherent response given the dialogue history? </s> response: '\\\n",
    "                            + output[i] + ' </s> dialogue history: ' + src[i]\n",
    "            elif dimension == 'engagingness':\n",
    "                cur_input = 'question: Is this an engaging and informative response according to the dialogue history and fact? </s> response: '\\\n",
    "                            + output[i] + ' </s> dialogue history: ' + src[i] + ' </s> fact: ' + context[i]\n",
    "            elif dimension == 'groundedness':\n",
    "                cur_input = 'question: Is this response consistent with knowledge in the fact? </s> response: '\\\n",
    "                            + output[i] + ' </s> fact: ' + context[i]\n",
    "            elif dimension == 'understandability':\n",
    "                cur_input = 'question: Is this an understandable response in the dialogue? </s> response: ' + output[i]\n",
    "            else:\n",
    "                raise NotImplementedError('The input format for this dimension is still undefined. Please customize it first.')\n",
    "        # For data-to-text\n",
    "        elif task == 'data2text':\n",
    "            if dimension == 'naturalness':\n",
    "                cur_input = 'question: Is this a fluent utterance? </s> utterance: ' + output[i]\n",
    "            elif dimension == 'informativeness':\n",
    "                cur_input = 'question: Is this sentence informative according to the reference? </s> sentence: '\\\n",
    "                            + output[i] + ' </s> reference: ' + ref[i]\n",
    "            else:\n",
    "                raise NotImplementedError('The input format for this dimension is still undefined. Please customize it first.')\n",
    "        # For factual consistency detection\n",
    "        elif task == 'fact':\n",
    "            if dimension == 'consistency':\n",
    "                cur_input = 'question: Is this claim consistent with the document? </s> claim: ' + output[i] + ' </s> document: ' + src[i]\n",
    "            else:\n",
    "                raise NotImplementedError('No other dimensions for the factual consistency detection task.')\n",
    "        # For new customized tasks\n",
    "        else:\n",
    "            raise NotImplementedError('Other tasks are not implemented, please customize specific tasks here.')\n",
    "        input_with_question.append(cur_input)\n",
    "    return input_with_question\n",
    "\n",
    "\n",
    "def print_scores(scores):\n",
    "    table = PrettyTable(['Dimensions','Score'])\n",
    "    print('\\nEvaluation scores are shown below:')\n",
    "    dims = list(scores[0].keys())\n",
    "    for dim in dims:\n",
    "        cur_score = 0\n",
    "        for i in range(len(scores)):\n",
    "            cur_score += scores[i][dim]\n",
    "        table.add_row([dim, round(cur_score / len(scores), 6)])\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5c4f2634-7435-484d-a08b-00ca76293527",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")\n",
    "\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "task = 'fact'\n",
    "\n",
    "evaluator = get_evaluator(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a373e27-4048-4fd8-811e-740d12a55c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import convert_to_json\n",
    "# from metric.evaluator import get_evaluator\n",
    "\n",
    "# task = 'fact'\n",
    "\n",
    "# # a list of source documents\n",
    "# src_list = ['Peter and Elizabeth took a taxi to attend the night party in the city. \\\n",
    "#              While in the party, Elizabeth collapsed and was rushed to the hospital.', 'Peter and Elizabeth took a taxi to attend the night party in the city. \\\n",
    "#              While in the party, Elizabeth collapsed and was rushed to the hospital.']\n",
    "# # a list of model outputs (claims) to be evaluataed\n",
    "# output_list = ['Tom was rushed to hospital.', 'Elizabeth was rushed to hospital.']\n",
    "\n",
    "# # Prepare data for pre-trained evaluators\n",
    "# data = convert_to_json(output_list=output_list, src_list=src_list)\n",
    "# # Get factual consistency scores\n",
    "# eval_scores = evaluator.evaluate(data)\n",
    "\n",
    "# eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8aaa5a54-02e2-449b-af1f-cd34998f10cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import sent_tokenize\n",
    "from scorer import UniEvaluator  # Make sure this import works after placing scorer.py in the same directory\n",
    "\n",
    "def evaluate(data, dims=None, overall=True, print_result=False, model_name_or_path=\"t5-small\", task='summarization', device='cuda:0', individual=True):\n",
    "    \"\"\"\n",
    "    Get the scores of all the given dimensions (fluency, consistency, coherence, relevance)\n",
    "\n",
    "    data: A list of dictionaries, where each dictionary contains:\n",
    "          - 'source': The original text\n",
    "          - 'system_output': The generated system output (summary)\n",
    "          - 'reference' (optional): Reference summary for relevance evaluation\n",
    "\n",
    "    dims: A list of dimensions to be evaluated. If dims is None, it evaluates four default dimensions:\n",
    "          coherence, consistency, fluency, relevance.\n",
    "\n",
    "    overall: Boolean to indicate whether the overall score is calculated as the average of all dimensions.\n",
    "\n",
    "    print_result: Boolean to print the results on the screen.\n",
    "\n",
    "    model_name_or_path: The model name or path to use for evaluation, e.g., 't5-small'\n",
    "\n",
    "    task: The task type (used in scoring if needed, like summarization or other NLP tasks).\n",
    "\n",
    "    device: The device to use for evaluation ('cpu' or 'cuda:0').\n",
    "    \"\"\"\n",
    "\n",
    "    # Instantiate the scorer\n",
    "    scorer = UniEvaluator(model_name_or_path=model_name_or_path, device=device)\n",
    "\n",
    "    n_data = len(data)\n",
    "    eval_scores = [{} for _ in range(n_data)]\n",
    "\n",
    "    # Default dimensions if not provided\n",
    "    if dims is None:\n",
    "        dims = ['coherence', 'consistency', 'fluency', 'factual consistency']   #add relevance\n",
    "\n",
    "    for dim in dims:\n",
    "        print(f'Evaluating {dim} of {n_data} samples !!!')\n",
    "\n",
    "        if dim == 'consistency' or dim == 'fluency':\n",
    "            # Sentence-level scores for consistency and fluency\n",
    "            src_list, output_list = [], []\n",
    "            n_sents = []  # number of sentences in each summary\n",
    "\n",
    "            for i in range(n_data):\n",
    "                if dim == 'consistency':\n",
    "                    source = data[i]['source']\n",
    "                else:\n",
    "                    source = ''\n",
    "                system_outputs = sent_tokenize(data[i]['system_output'])\n",
    "                n_sents.append(len(system_outputs))\n",
    "                for j in range(len(system_outputs)):\n",
    "                    src_list.append(source)\n",
    "                    output_list.append(system_outputs[j])\n",
    "\n",
    "            input_list = add_question(dimension=dim, output=output_list, src=src_list, task=task)\n",
    "            sent_score = scorer.score(input_list)\n",
    "\n",
    "            # Calculate average sentence-level scores for each sample\n",
    "            start_idx = 0\n",
    "            score = []\n",
    "            for cur_n_sent in n_sents:\n",
    "                score.append(sum(sent_score[start_idx:start_idx + cur_n_sent]) / cur_n_sent)\n",
    "                start_idx += cur_n_sent\n",
    "\n",
    "        elif dim == 'coherence' or dim == 'relevance':\n",
    "            # Summary-level scores for coherence and relevance\n",
    "            src_list, output_list, ref_list = [], [], []\n",
    "\n",
    "            for i in range(n_data):\n",
    "                src_list.append(data[i]['source'])\n",
    "                output_list.append(data[i]['system_output'])\n",
    "                if dim == 'relevance':\n",
    "                    ref_list.append(data[i]['reference'])\n",
    "\n",
    "            input_list = add_question(dimension=dim, output=output_list, src=src_list, ref=ref_list, task=task)\n",
    "            score = scorer.score(input_list)\n",
    "\n",
    "        elif dim == 'factual consistency':\n",
    "            output_list, src_list = [], []\n",
    "\n",
    "            for i in range(n_data):\n",
    "                src_list.append(data[i]['source'])\n",
    "                output_list.append(data[i]['system_output'])\n",
    "\n",
    "            data = convert_to_json(output_list=output_list, src_list=src_list)\n",
    "            eval_score = evaluator.evaluate(data)\n",
    "            score = []\n",
    "\n",
    "            for i in eval_score:\n",
    "                temp = i['consistency']\n",
    "                score.append(temp)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"The input format for the dimension '{dim}' is still undefined. Please customize it.\")\n",
    "\n",
    "        # Store the scores for the current dimension\n",
    "        for i in range(n_data):\n",
    "            eval_scores[i][dim] = score[i]\n",
    "\n",
    "    # Calculate overall score (average of all evaluated dimensions)\n",
    "    if overall:\n",
    "        for i in range(n_data):\n",
    "            eval_scores[i]['overall'] = np.mean([eval_scores[i][dim] for dim in dims])\n",
    "\n",
    "    # Print the result if requested\n",
    "    if print_result:\n",
    "        print_scores(eval_scores)\n",
    "\n",
    "    if individual:\n",
    "        individual_scores = []\n",
    "        for i in range(n_data):\n",
    "            temp = [eval_scores[i][dim] for dim in dims]\n",
    "            individual_scores.append(temp)\n",
    "\n",
    "        return np.array(individual_scores)\n",
    "\n",
    "    # Calculate average score across all the dimensions except 'overall'\n",
    "    avg_score = []\n",
    "    for i in range(n_data):\n",
    "        # Exclude 'overall' from the averaging\n",
    "        dimensions = [dim for dim in dims if dim != 'overall']\n",
    "        avg_score.append(np.mean([eval_scores[i][dim] for dim in dimensions]))\n",
    "\n",
    "    return avg_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "27cf0762",
   "metadata": {
    "id": "27cf0762"
   },
   "outputs": [],
   "source": [
    "def get_score(game_data):\n",
    "    weights = np.array([0.1, 0.2, 0.3, 0.4]) #'coherence', 'consistency', 'fluency', 'factual consistency'\n",
    "\n",
    "    sample_data = []\n",
    "\n",
    "    for q,r in zip(game_data[\"query\"], game_data[\"response\"]):\n",
    "        temp = {}\n",
    "    \n",
    "        temp[\"source\"] = q\n",
    "        temp[\"system_output\"] = r\n",
    "    \n",
    "        sample_data.append(temp)\n",
    "    \n",
    "    score = evaluate(sample_data, overall=False)\n",
    "\n",
    "    weighted_score = []\n",
    "\n",
    "    for array1 in score:\n",
    "        result = np.where(\n",
    "            array1 < 0.5,          # Condition\n",
    "            -array1 * weights,     # If True: make product negative\n",
    "            array1 * weights       # If False: normal multiplication\n",
    "        )\n",
    "        \n",
    "        sum_products = np.sum(result)\n",
    "        final_result = sum_products/3\n",
    "        \n",
    "        weighted_score.append(final_result)\n",
    "\n",
    "    scores = [torch.tensor([x], dtype=torch.float64) for x in weighted_score]\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ik_AAXW44I1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ik_AAXW44I1",
    "outputId": "40e2a2b4-e46b-4668-be89-ff6ce7865709"
   },
   "outputs": [],
   "source": [
    "# Check if models are on GPU\n",
    "print(next(starcoder_model.parameters()).device)  # Should print: cuda:0\n",
    "# print(next(starcoder_model_ref.parameters()).device)  # Should print: cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d3a744ce-d939-49a2-9914-94d853750476",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDICAL_PROMPT = \"\"\"\n",
    "Please generate a medical summary based on the following clinical notes. The summary should include the following sections: \n",
    "\n",
    "CHIEF COMPLAINT\n",
    "A concise statement of the patient's primary concern or reason for visiting the clinic.\n",
    "\n",
    "HISTORY OF PRESENT ILLNESS\n",
    "A detailed narrative about the patient's symptoms, their onset, duration, and any relevant medical history or previous treatments.\n",
    "\n",
    "VITALS\n",
    "Include any relevant vital signs (e.g., oxygen saturation, blood pressure) if available.\n",
    "\n",
    "PHYSICAL EXAM \n",
    "Summarize the findings from the physical examination, including any notable abnormalities.\n",
    "\n",
    "RESULTS \n",
    "Summarize the results of any diagnostic tests performed (e.g., lab work, imaging studies).\n",
    "\n",
    "ASSESSMENT\n",
    "The doctor's assessment of the patient's condition or diagnosis.\n",
    "\n",
    "PLAN\n",
    "The treatment plan, including prescribed medications, lifestyle recommendations, and follow-up instructions.\n",
    "\n",
    "INSTRUCTIONS\n",
    "Specific instructions for the patient regarding their treatment plan and follow-up care.\n",
    "\n",
    "Important Note: If any section lacks relevant information, omit that section from the generated summary. Only include sections for which there is sufficient information available.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ddb955ce-411b-4a89-ac33-3cb2024bcd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dataset', 'encounter_id', 'review', 'note', 'source_file', 'id', '__index_level_0__', 'input_ids', 'query'],\n",
       "    num_rows: 93\n",
       "})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6c774433-cf61-4192-92f5-1383eb3d4976",
   "metadata": {},
   "outputs": [],
   "source": [
    "starcder_model = model\n",
    "starcoder_tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ec47f697-a24d-4cae-81f9-d04cc3f1f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "70972aff-ccfb-4f01-8b93-8902400b162a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[165]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m send_to_device\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mppo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# just get one batch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBatch keys:\u001b[39m\u001b[33m\"\u001b[39m, batch.keys())\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Try moving each part of batch to CUDA manually\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\accelerate\\data_loader.py:575\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[32m    574\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m         current_batch = \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m    577\u001b[39m     next_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\accelerate\\utils\\operations.py:178\u001b[39m, in \u001b[36msend_to_device\u001b[39m\u001b[34m(tensor, device, non_blocking, skip_keys)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    176\u001b[39m         skip_keys = []\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m         \u001b[43m{\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    182\u001b[39m     )\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\accelerate\\utils\\operations.py:179\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    176\u001b[39m         skip_keys = []\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[32m    178\u001b[39m         {\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor.items()\n\u001b[32m    181\u001b[39m         }\n\u001b[32m    182\u001b[39m     )\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\accelerate\\utils\\operations.py:153\u001b[39m, in \u001b[36msend_to_device\u001b[39m\u001b[34m(tensor, device, non_blocking, skip_keys)\u001b[39m\n\u001b[32m    151\u001b[39m     device = \u001b[33m\"\u001b[39m\u001b[33mnpu:0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor.to(device)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from accelerate.utils import send_to_device\n",
    "\n",
    "batch = next(iter(ppo_trainer.dataloader))  # just get one batch\n",
    "print(\"Batch keys:\", batch.keys())\n",
    "\n",
    "# Try moving each part of batch to CUDA manually\n",
    "for key, value in batch.items():\n",
    "    try:\n",
    "        print(f\"Testing key: {key}\")\n",
    "        value_cuda = value.to(device)\n",
    "        print(f\"✅ {key} moved to {device} successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in key '{key}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "253d6cc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "253d6cc8",
    "outputId": "696512ba-ab95-481c-e4d8-ef7d3065b28f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/46 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[187]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mppo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBatch keys:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\accelerate\\data_loader.py:575\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[32m    574\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m         current_batch = \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_state_dict()\n\u001b[32m    577\u001b[39m     next_batch = \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\accelerate\\utils\\operations.py:178\u001b[39m, in \u001b[36msend_to_device\u001b[39m\u001b[34m(tensor, device, non_blocking, skip_keys)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    176\u001b[39m         skip_keys = []\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m         \u001b[43m{\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    182\u001b[39m     )\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\accelerate\\utils\\operations.py:179\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m skip_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    176\u001b[39m         skip_keys = []\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensor)(\n\u001b[32m    178\u001b[39m         {\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m             k: t \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m skip_keys \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensor.items()\n\u001b[32m    181\u001b[39m         }\n\u001b[32m    182\u001b[39m     )\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\accelerate\\utils\\operations.py:153\u001b[39m, in \u001b[36msend_to_device\u001b[39m\u001b[34m(tensor, device, non_blocking, skip_keys)\u001b[39m\n\u001b[32m    151\u001b[39m     device = \u001b[33m\"\u001b[39m\u001b[33mnpu:0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor.to(device)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "from random import choices\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "for epoch in range(1):\n",
    "    for batch in tqdm(ppo_trainer.dataloader):\n",
    "        print(\"Batch keys:\", batch.keys())\n",
    "    \n",
    "        for i, input_ids in enumerate(batch[\"input_ids\"]):\n",
    "            if not isinstance(input_ids, torch.Tensor):\n",
    "                print(f\"[!] Input {i} is not a tensor.\")\n",
    "            elif input_ids.dtype != torch.long:\n",
    "                print(f\"[!] Input {i} has wrong dtype: {input_ids.dtype}\")\n",
    "            elif torch.any(input_ids >= tokenizer.vocab_size):\n",
    "                print(f\"[!] Input {i} has token ID >= vocab size!\")\n",
    "    for batch in tqdm(ppo_trainer.dataloader):\n",
    "        (logs, game_data,) = (\n",
    "            dict(),\n",
    "            dict(),\n",
    "        )\n",
    "\n",
    "        # task_list = choices(ctrl_str, k=config.batch_size)\n",
    "        # game_data[\"query\"] = [t + q for t, q in zip(task_list, batch[\"query\"])]\n",
    "        game_data[\"query\"] = [q for q in batch[\"query\"]]\n",
    "        # query_tensors = [torch.cat((ctrl_tokens[t], input_ids)) for t, input_ids in zip(task_list, batch[\"input_ids\"])]\n",
    "        assert all(\n",
    "            torch.all(input_id < tokenizer.vocab_size).item()\n",
    "            for input_id in batch[\"input_ids\"]\n",
    "        )\n",
    "        assert all(input_id.dtype == torch.long for input_id in batch[\"input_ids\"])\n",
    "\n",
    "        query_tensors = [input_ids for input_ids in batch[\"input_ids\"]]\n",
    "        \n",
    "        response_tensors = []\n",
    "        for query in query_tensors:\n",
    "            original_notes = tokenizer.decode(query)\n",
    "            \n",
    "            # Combine with medical prompt only during generation\n",
    "            full_prompt = original_notes + \"\\n\\n\" + MEDICAL_PROMPT\n",
    "            full_prompt_tensor = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(device).squeeze(0)\n",
    "\n",
    "            # Fix: truncate to model max length\n",
    "            max_length = tokenizer.model_max_length\n",
    "            if full_prompt_tensor.size(0) > max_length:\n",
    "                full_prompt_tensor = full_prompt_tensor[-max_length:]\n",
    "                response = ppo_trainer.generate(\n",
    "                    full_prompt_tensor,\n",
    "                    **generation_kwargs\n",
    "                )\n",
    "                response_tensors.append(response.squeeze())\n",
    "#         print(response_tensors)\n",
    "        game_data[\"response\"] = [tokenizer.decode(r) for r in response_tensors]\n",
    "\n",
    "        print(\"check\")\n",
    "\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], game_data[\"response\"])]\n",
    "        logits = get_score(game_data)\n",
    "        rewards = [torch.tensor([l], device=query_tensors[0].device) for l in logits]\n",
    "        # rewards = pos_logit_to_reward(logits, task_list)\n",
    "        # rewards = [torch.tensor([1.0], device=query_tensors[0].device) for _ in range(len(texts))]\n",
    "\n",
    "        #### Run PPO training\n",
    "        t = time.time()\n",
    "        rewards = [torch.tensor([float(logit)], device=query_tensors[0].device) for logit in logits]\n",
    "        query_tensors = [q.to(device) for q in query_tensors]\n",
    "        response_tensors = [r.to(device) for r in response_tensors]\n",
    "        print(\"query shapes:\", [q.shape for q in query_tensors])\n",
    "        print(\"response shapes:\", [r.shape for r in response_tensors])\n",
    "        print(\"reward values:\", rewards)\n",
    "\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "\n",
    "        # Log for mean reward and KL Divergence!!!!!!!!!!!!!!!\n",
    "        # for cs in ctrl_str:\n",
    "        #     key = \"env/reward_\" + cs.strip(\"[]\")\n",
    "        #     stats[key] = np.mean([r.cpu().numpy() for r, t in zip(rewards, task_list) if t == cs])\n",
    "        # ppo_trainer.log_stats(stats, game_data, rewards)\n",
    "\n",
    "        key = \"env/reward_mean\"\n",
    "        stats[key] = rewards\n",
    "        ppo_trainer.log_stats(stats, game_data, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d0bad48f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0bad48f",
    "outputId": "ccde4117-6c85-49d3-db21-e0d77a29cd45"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[183]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m###saving the model\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# starcoder_model.save_pretrained(\"rhlfmodel/\")\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# starcoder_tokenizer.save_pretrained(\"rhlfmodel/\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mkshitij-weights-folder\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mqwen-aloe-rl-12-4-ppo-tuned\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m starcoder_tokenizer.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mkshitij-weights-folder\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mqwen-aloe-rl-12-4-ppo-tuned\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ppo_trainer.model.pretrained_model, PeftModel):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\modeling_utils.py:2448\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   2444\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m shard_file, shard \u001b[38;5;129;01min\u001b[39;00m shards.items():\n\u001b[32m   2445\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   2446\u001b[39m         \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   2447\u001b[39m         \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2448\u001b[39m         \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2449\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2450\u001b[39m         save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\safetensors\\torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata=metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\safetensors\\torch.py:496\u001b[39m, in \u001b[36m_flatten\u001b[39m\u001b[34m(tensors)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    489\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[33m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    493\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    494\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshape\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_tobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\safetensors\\torch.py:500\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[32m    488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    489\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[33m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    493\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    497\u001b[39m     k: {\n\u001b[32m    498\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v.dtype).split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m],\n\u001b[32m    499\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m: v.shape,\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43m_tobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    501\u001b[39m     }\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors.items()\n\u001b[32m    503\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\safetensors\\torch.py:422\u001b[39m, in \u001b[36m_tobytes\u001b[39m\u001b[34m(tensor, name)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    415\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are trying to save a non contiguous tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` which is not allowed. It either means you\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m are trying to save tensors which are reference of each other in which case it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms recommended to save\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m pack it before saving.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    419\u001b[39m     )\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensor.device.type != \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    421\u001b[39m     \u001b[38;5;66;03m# Moving tensor to cpu before saving\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     tensor = \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctypes\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "###saving the model\n",
    "# starcoder_model.save_pretrained(\"rhlfmodel/\")\n",
    "# starcoder_tokenizer.save_pretrained(\"rhlfmodel/\")\n",
    "\n",
    "ppo_trainer.model.pretrained_model.save_pretrained(\"D:\\kshitij-weights-folder\\qwen-aloe-rl-12-4-ppo-tuned\")\n",
    "starcoder_tokenizer.save_pretrained(\"D:\\kshitij-weights-folder\\qwen-aloe-rl-12-4-ppo-tuned\")\n",
    "\n",
    "if isinstance(ppo_trainer.model.pretrained_model, PeftModel):\n",
    "    ppo_trainer.model.pretrained_model.save_adapter(\n",
    "        \"D:/kshitij-weights-folder/qwen-aloe-rl-12-4-ppo-tuned-lora\",\n",
    "        \"lora_adapter\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48706b19-356f-463c-b07b-fb12cfb4c747",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907f5127-9db1-4fbc-94cc-8eed816aefa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"D:\\hf-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0cb2c-cddb-46f3-bfaf-7187cb15c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from trl import RewardTrainer    \n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig  \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0c73a-f47b-4375-b6c6-794ca2f13cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\"\n",
    "df = pd.read_csv(DATA)\n",
    "\n",
    "# First split: 60% train, 40% temp (eval + test)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "\n",
    "# Second split: 50% of temp (which is 20% of total) for eval, 50% for test\n",
    "eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e0a26b-d34a-41c4-83f4-f2ee6b577e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Path to your LoRA weights + tokenizer --\n",
    "model_dir = \"D:\\kshitij-weights-folder\\qwen-aloe-rl-9-4-ppo-tuned\"  \n",
    "\n",
    "# -- 4-bit quantization config (same as training) --\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# -- 1) Load the *base* Qwen2.5 model in 4-bit --\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "# model = PeftModel.from_pretrained(model, adapter_model_name)\n",
    "\n",
    "# -- 2) Load your fine-tuned LoRA adapters into the base model --\n",
    "# The directory should contain adapter_model.bin, adapter_config.json, etc.\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# -- 3) Load the tokenizer you saved to ./aloe_qwen --\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce493f64-d50b-47f0-9ea7-32275b714475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prompt(conv):\n",
    "    prompt = f\"\"\"\n",
    "    CONVERSATION:\n",
    "    {conv}\n",
    "    \n",
    "    SUMMARY:\n",
    "    Please generate a medical summary based on the following clinical notes. The summary should include the following sections: Chief Complaint, History of Present Illness, Vitals, Physical Exam, Results, Assessment, Plan, and Instructions.\n",
    "    Please format the response as plain text, without using markdown or special formatting, and with clear headings for each section, like this:\n",
    "    \n",
    "    \n",
    "    CHIEF COMPLAINT\n",
    "    A concise statement of the patient's primary concern or reason for visiting the clinic.\n",
    "    \n",
    "    HISTORY OF PRESENT ILLNESS\n",
    "    A detailed narrative about the patient's symptoms, their onset, duration, and any relevant medical history or previous treatments.\n",
    "    \n",
    "    VITALS\n",
    "    Include any relevant vital signs (e.g., oxygen saturation, blood pressure) if available.\n",
    "    \n",
    "    PHYSICAL EXAM \n",
    "    Summarize the findings from the physical examination, including any notable abnormalities.\n",
    "    \n",
    "    RESULTS \n",
    "    Summarize the results of any diagnostic tests performed (e.g., lab work, imaging studies).\n",
    "    \n",
    "    ASSESSMENT\n",
    "    The doctor's assessment of the patient's condition or diagnosis.\n",
    "    \n",
    "    PLAN\n",
    "    The treatment plan, including prescribed medications, lifestyle recommendations, and follow-up instructions.\n",
    "    \n",
    "    INSTRUCTIONS\n",
    "    Specific instructions for the patient regarding their treatment plan and follow-up care.\n",
    "    \n",
    "    Important Note: If any section lacks relevant information or if specific details are not provided (e.g., vitals are not mentioned, no abnormal findings in the physical exam), omit that section from the generated summary. Only include sections for which there is sufficient information available.\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c025c7-a863-4c92-8a93-ee663ebe8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = test_df\n",
    "\n",
    "eval_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20bc36-7ec9-44c5-b88c-aec01c326fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_second_occurrence(text, keyword=\"CHIEF COMPLAINT\"):\n",
    "    # Find all the starting indexes of the keyword\n",
    "    occurrences = [i for i in range(len(text)) if text.startswith(keyword, i)]\n",
    "\n",
    "    # Make sure there are at least two occurrences\n",
    "    if len(occurrences) < 2:\n",
    "        print(\"Less than two occurrences found.\")\n",
    "        return None\n",
    "\n",
    "    # Get text from second occurrence\n",
    "    second_index = occurrences[1]\n",
    "    return text[second_index:]\n",
    "\n",
    "\n",
    "# for i in data:\n",
    "#   result = extract_from_second_occurrence(i['system_output'])\n",
    "#   if result:\n",
    "#       i['system_output'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdfc560-3f25-4034-91cc-dc5d8dfbcfa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",  # Automatically place on GPU if available\n",
    ")\n",
    "\n",
    "# notechat = load_dataset(\"akemiH/NoteChat\")\n",
    "# eval_df = notechat[\"train\"].select(range(5000, 7001)).to_pandas()\n",
    "eval_df = test_df[:1]\n",
    "\n",
    "# 2) Clean up missing data if present\n",
    "# if eval_df.isnull().values.any():\n",
    "#     print(\"Found missing values in the evaluation set. Dropping them.\")\n",
    "#     eval_df = eval_df.dropna()\n",
    "\n",
    "\n",
    "# 3) Prepare batching parameters\n",
    "batch_size = 4\n",
    "num_samples = len(eval_df)\n",
    "num_batches = (num_samples // batch_size) + int(num_samples % batch_size != 0)\n",
    "\n",
    "# Lists to store predictions and references\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# 4) Batching loop to generate summaries\n",
    "for i in tqdm(range(num_batches), desc=\"Generating Summaries\"):\n",
    "    start = i * batch_size\n",
    "    end = min(start + batch_size, num_samples)\n",
    "    \n",
    "    # Extract conversation and reference summary columns\n",
    "    batch_conversations = eval_df[\"dialogue\"][start:end].tolist()\n",
    "    batch_refs = eval_df[\"note\"][start:end].tolist()  # \"data\" column for references\n",
    "\n",
    "    # Prepare prompts replicating training format\n",
    "    prompts = [\n",
    "        return_prompt(conv)\n",
    "        for conv in batch_conversations\n",
    "    ]\n",
    "    \n",
    "    # Generate summaries\n",
    "    results = summarizer(\n",
    "        prompts,\n",
    "        max_new_tokens=900,\n",
    "        do_sample=False,\n",
    "        # truncation=True,\n",
    "        # num_return_sequences=1  # Default is 1\n",
    "    )\n",
    "    \n",
    "    # Parse results and extract summaries\n",
    "    for item in results:\n",
    "        # 'item' is a list with 1 dict => {\"generated_text\": \"...\"}\n",
    "        output_dict = item[0]\n",
    "        generated_text = output_dict[\"generated_text\"]\n",
    "        \n",
    "        # Extract only the part after \"SUMMARY:\"\n",
    "        # if \"SUMMARY:\" in full_text:\n",
    "        #     extracted_summary = generated_text.split(\"SUMMARY:\", 1)[-1].strip()\n",
    "        # else:\n",
    "        #     extracted_summary = generated_text  # Fallback if marker not found\n",
    "\n",
    "        # first_occurrence = generated_text.lower().find(\"chief complaint\")\n",
    "        # second_occurrence = generated_text.lower().find(\"chief complaint\", first_occurrence + 1)\n",
    "        # third_occurrence = generated_text.lower().find(\"chief complaint\", second_occurrence + 1)\n",
    "        # fourth_occurrence = generated_text.lower().find(\"chief complaint\", third_occurrence + 1)\n",
    "        \n",
    "        # if fourth_occurrence != -1:\n",
    "        #     # Extract everything from the second occurrence of \"CHIEF COMPLAINT\" onward\n",
    "        #     extracted_summary = generated_text[fourth_occurrence:].strip()\n",
    "        # else:\n",
    "        #     # If the second \"CHIEF COMPLAINT\" is not found, just use the original text\n",
    "        #     extracted_summary = generated_text.strip()\n",
    "\n",
    "        cleaned = extract_from_second_occurrence(generated_text)\n",
    "        predictions.append(cleaned)\n",
    "    \n",
    "    # Append the reference texts\n",
    "    references.extend(batch_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7818f3e-f7b7-4fad-896d-df4b7c9f3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3fdb7c-91af-4cad-9649-13d028a754af",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list = eval_df[\"dialogue\"].tolist()\n",
    "ref_list = eval_df[\"note\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105b41e9-d8ce-44aa-85f5-9529a2db946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = []\n",
    "for pred in predictions:\n",
    "    # Ensure that \"Summary:\" exists in the string to avoid errors\n",
    "    if len(pred) > 0:\n",
    "        output_list.append(pred)\n",
    "    else:\n",
    "        # Handle cases where \"Summary:\" is missing (optional)\n",
    "        output_list.append(\"\")  # Or handle differently based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad16043-d882-4e18-b014-e79ea16323cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_json(output_list, src_list=None, ref_list=None, context_list=None, \\\n",
    "            scores=None, doc_id=None, system_id=None):\n",
    "    \"\"\"\n",
    "        Convert the data into the json format.\n",
    "\n",
    "        output_list: a list of model output\n",
    "        src_list: source input for different NLG tasks. For example, source document for summarization\n",
    "                  and dialogue history for dialogue response generation\n",
    "        ref_list: human-annotated groundtruth\n",
    "        context_list: the context needed to evaluate several specific dimension. For example,\n",
    "                      additional factual information when evaluating engagingness and groundedness in dialogues\n",
    "        scores: human scores for evaluating the model output. They can be used to calculate the correlation\n",
    "                between evaluators and human judgements. The scores should be stored in a dictionary. For example,\n",
    "                {'fluency': 2.0, 'coherence': 3.0} could be the human score for a sample.\n",
    "        doc_id: the index of the input source. It can be used to calculate summary-level correlation for summarzation\n",
    "        system_id: the index of the generation system. It can be used to calculate system-level correlation.\n",
    "    \"\"\"\n",
    "    json_data = []\n",
    "    for i in range(len(output_list)):\n",
    "        cur = {}\n",
    "        cur['system_output'] = output_list[i]\n",
    "        if src_list is not None:\n",
    "            cur['source'] = src_list[i]\n",
    "        if ref_list is not None:\n",
    "            cur['reference'] = ref_list[i]\n",
    "        if context_list is not None:\n",
    "            cur['context'] = context_list[i]\n",
    "        if scores is not None:\n",
    "            cur['scores'] = scores[i]\n",
    "        if doc_id is not None:\n",
    "            cur['doc_id'] = doc_id[i]\n",
    "        if system_id is not None:\n",
    "            cur['system_id'] = system_id[i]\n",
    "        json_data.append(cur)\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efead8-7a4c-4937-b7de-6a5fa448fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_to_json(\n",
    "    src_list=src_list,\n",
    "    ref_list=ref_list,\n",
    "    output_list=output_list\n",
    ")\n",
    "\n",
    "filtered_data = [\n",
    "    entry for entry in data\n",
    "    if entry[\"system_output\"].strip()  # Ensure non-empty system_output\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218e277-c0fa-4255-a3af-e2af86924bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nlg_evaluation_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf2489",
   "metadata": {
    "id": "23bf2489"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "model_path = \"/content/drive/MyDrive/Medical Dialogue Summarization using PPO/rhlfmodel\"\n",
    "set_seed(42)\n",
    "pipe = pipeline(\"text-generation\",model=model_path, tokenizer=model_path, max_length=40, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e2c6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "026e2c6c",
    "outputId": "7eaea838-051e-48b5-c578-619e0e5b9a08"
   },
   "outputs": [],
   "source": [
    "text = dataset[\"rejected\"][0]\n",
    "print(text)\n",
    "pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NpppcYveAzWa",
   "metadata": {
    "id": "NpppcYveAzWa"
   },
   "outputs": [],
   "source": [
    "save_directory = \"/content/drive/MyDrive/Medical Dialogue Summarization using PPO/rhlfmodel\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(save_directory).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llCvj_yCJY9S",
   "metadata": {
    "id": "llCvj_yCJY9S"
   },
   "outputs": [],
   "source": [
    "conversation = '''\n",
    "Doctor: Hi, Mr. X, I'm Dr. Y. How are you feeling today?\n",
    "\n",
    "Patient: Not too good, doctor. I've been feeling really sick lately.\n",
    "\n",
    "Doctor: I understand. Can you tell me what symptoms you're experiencing?\n",
    "\n",
    "Patient: Yes, I've been having a fever, a dry cough, and dyspnea.\n",
    "\n",
    "Doctor: I see. You were hospitalized due to moderate ARDS from COVID-19, is that correct?\n",
    "\n",
    "Patient: Yes, that's correct.\n",
    "\n",
    "Doctor: During your physical therapy, we encountered some difficulties. Can you tell me more about that?\n",
    "\n",
    "Patient: Yes, I had trouble with position changes and deep breathing. Every time I tried to change my position or take a deep breath, I would start coughing and it would make me really short of breath.\n",
    "\n",
    "Doctor: I understand. To avoid rapid deterioration and respiratory failure, we instructed you to change positions very slowly and step-by-step, right?\n",
    "\n",
    "Patient: Yes, that's right. It took about 30 minutes to change to the prone position.\n",
    "\n",
    "Doctor: And I see that this approach increased your oxygen saturation, for example, on day 5 with 6 L/min of oxygen from 93% to 97%.\n",
    "\n",
    "Patient: Yes, that's correct.\n",
    "\n",
    "Doctor: Good. We also had to adapt your breathing exercises to avoid prolonged coughing and oxygen desaturation. Can you tell me more about that?\n",
    "\n",
    "Patient: Yes, I was instructed to stop every deep breath before coughing and to hold my breath for better air distribution.\n",
    "\n",
    "Doctor: I see that you performed the breathing exercises well and managed to increase your oxygen saturation.\n",
    "\n",
    "Patient: Yes, I did my best.\n",
    "\n",
    "Doctor: You also had difficulty maintaining sufficient oxygen saturation during physical activity, is that correct?\n",
    "\n",
    "Patient: Yes, I did. But with close monitoring and frequent breaks, I was able to perform low-level strength and walking exercises without any significant deoxygenation.\n",
    "\n",
    "Doctor: I see that your exercise progression was low on days 1 to 5, but then increased daily until your hospital discharge to a rehabilitation clinic on day 10.\n",
    "\n",
    "Patient: Yes, that's correct.\n",
    "\n",
    "Doctor: Great. I'd like to keep monitoring your progress and see how you're doing. Can you keep me updated on any changes in your symptoms?\n",
    "\n",
    "Patient: Yes, of course, doctor.\n",
    "\n",
    "Doctor: Alright, let's keep in touch. If you have any questions or concerns, don't hesitate to reach out to me.\n",
    "\n",
    "Patient: Thank you, doctor.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DOxLTH1QAKh7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOxLTH1QAKh7",
    "outputId": "ceb93bc4-3129-41de-c6dc-7380c7a32a95"
   },
   "outputs": [],
   "source": [
    "def generate_response(prompt, model, tokenizer, max_new_tokens=1000, temperature=0.1):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"Generate a summary for the below conversation. Dont give me the prompt back. I just want the summary to be returned to me\\n\\n\" + conversation\n",
    "response = generate_response(prompt, model, tokenizer)\n",
    "print(\"Generated Response:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j9DpRTv5DCkF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9DpRTv5DCkF",
    "outputId": "b5ca83f0-951d-4607-f9db-34e6cdbb1e21"
   },
   "outputs": [],
   "source": [
    "print(dataset[\"review\"][0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dH7ZXN5XGf3q",
   "metadata": {
    "id": "dH7ZXN5XGf3q"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"rlhfmodel/\")\n",
    "model_path = \"bigcode/tiny_starcoder_py\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation=True, max_length=256, padding=\"max_length\")\n",
    "text = df.iloc[2][\"prompt\"]\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\", max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7239817f-7f97-458f-92c5-554a998e2ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        'source': \"Doctor: Hello, how are you feeling today?\\nPatient: I've been feeling a bit tired and dizzy.\\nDoctor: How long has this been happening?\\nPatient: For about a week now. I also have trouble sleeping.\\nDoctor: I see. Have you been under a lot of stress lately?\\nPatient: Yes, work has been quite stressful.\\nDoctor: That could be contributing. Let’s do some tests to rule out other issues.\",\n",
    "        'system_output': \"Patient reports tiredness, dizziness, and difficulty sleeping for a week. Work-related stress may be a factor. Doctor will conduct tests to check for other problems.\"\n",
    "    },\n",
    "    {\n",
    "        'source': \"Doctor: What brings you in today?\\nPatient: I’ve been having some chest pain and shortness of breath.\\nDoctor: How severe is the pain?\\nPatient: It’s sharp, and it comes and goes.\\nDoctor: When did it start?\\nPatient: It started two days ago.\\nDoctor: Any history of heart problems?\\nPatient: Yes, my father had heart disease.\\nDoctor: We’ll need to do an ECG and some blood tests to check your heart health.\",\n",
    "        'system_output': \"Patient has sharp chest pain and shortness of breath for two days. Family history of heart disease. Doctor will perform an ECG and blood tests to assess heart health.\"\n",
    "    },\n",
    "    {\n",
    "        'source': \"Doctor: How are you feeling today?\\nPatient: I’ve had a sore throat and a cough for the past few days.\\nDoctor: Any fever or difficulty swallowing?\\nPatient: Yes, I’ve had a low fever, but swallowing is fine.\\nDoctor: Any history of allergies or similar symptoms?\\nPatient: Not really.\\nDoctor: It could be a viral infection. I recommend rest, fluids, and maybe some over-the-counter medicine.\",\n",
    "        'system_output': \"Patient reports sore throat, cough, and a low fever. Doctor advises rest, fluids, and over-the-counter medication as the symptoms suggest a viral infection.\"\n",
    "    },\n",
    "    {\n",
    "        'source': \"Doctor: What’s bothering you today?\\nPatient: I’ve been experiencing frequent headaches and some nausea.\\nDoctor: How often do you get the headaches?\\nPatient: It’s been almost every day for the past week.\\nDoctor: Any other symptoms like blurred vision or dizziness?\\nPatient: No, just the headache and nausea.\\nDoctor: We’ll schedule an MRI to get a better understanding of the issue.\",\n",
    "        'system_output': \"Patient complains of daily headaches and nausea for the past week. No blurred vision or dizziness. Doctor will schedule an MRI for further evaluation.\"\n",
    "    }\n",
    "]\n",
    "score = evaluate(data, print_result=True)\n",
    "print(score)\n",
    "\n",
    "weights = np.array([1, 2, 3]) #'coherence', 'consistency', 'fluency'\n",
    "weighted_score = []\n",
    "\n",
    "for array1 in score:\n",
    "    result = np.where(\n",
    "        array1 < 0.5,          # Condition\n",
    "        -array1 * weights,     # If True: make product negative\n",
    "        array1 * weights       # If False: normal multiplication\n",
    "    )\n",
    "    \n",
    "    sum_products = np.sum(result)\n",
    "    final_result = sum_products/3\n",
    "    \n",
    "    weighted_score.append(final_result)\n",
    "\n",
    "        \n",
    "print(weighted_score)\n",
    "\n",
    "scores = [torch.tensor([x], dtype=torch.float64) for x in weighted_score]\n",
    "print(scores)  # Output: torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a748299d-0934-4815-8f1a-b419b6ada364",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = []\n",
    "\n",
    "for q,r in zip(game_data[\"query\"], game_data[\"response\"]):\n",
    "    temp = {}\n",
    "\n",
    "    temp[\"source\"] = q\n",
    "    temp[\"system_output\"] = r\n",
    "\n",
    "    sample_data.append(temp)\n",
    "\n",
    "    break\n",
    "\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b777e40f-73d3-4f64-a929-6f970cb13860",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate(sample_data, print_result=True, overall=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f41aea3-e0c6-4a36-ad2a-eb868e1a18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa56fbc-1420-4f56-9f2b-388a1a2f98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b264aba-ae95-4b4b-973a-b2228152fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb976e9-67e9-44de-b6be-febfd34e79c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(row['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2899e284-416c-45e0-b431-10bb68db7316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(row['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831af21-569c-47fd-8fce-9fbe82d63d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "def analyze_review_lengths(dataset: Dataset, tokenizer, sample_size=None) -> dict:\n",
    "    \"\"\"\n",
    "    Analyzes tokenized lengths of reviews in a HuggingFace dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: HF Dataset containing 'review' column\n",
    "        tokenizer: Pre-trained tokenizer\n",
    "        sample_size: Optional number of samples to analyze (None for full dataset)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with length statistics (in tokens)\n",
    "    \"\"\"\n",
    "    # Get subset if sampling\n",
    "    if sample_size and len(dataset) > sample_size:\n",
    "        dataset = dataset.select(np.random.choice(len(dataset), sample_size, replace=False))\n",
    "    \n",
    "    lengths = []\n",
    "    \n",
    "    # Process with progress bar\n",
    "    for item in tqdm(dataset, desc=\"Analyzing lengths\"):\n",
    "        encoded = tokenizer.encode(item[\"review\"])\n",
    "        lengths.append(len(encoded))\n",
    "    \n",
    "    return {\n",
    "        \"max\": max(lengths),\n",
    "        \"median\": int(np.median(lengths)),\n",
    "        \"mean\": float(np.mean(lengths)),\n",
    "        \"95th_percentile\": int(np.percentile(lengths, 95)),\n",
    "        \"histogram\": np.histogram(lengths, bins=20)\n",
    "    }\n",
    "\n",
    "# Usage example:\n",
    "stats = analyze_review_lengths(dataset, tokenizer)\n",
    "print(f\"\"\"\n",
    "Length Statistics (in tokens):\n",
    "- Maximum: {stats['max']}\n",
    "- Median: {stats['median']}\n",
    "- Mean: {stats['mean']}\n",
    "- 95th percentile: {stats['95th_percentile']}\n",
    "\"\"\")\n",
    "\n",
    "# Suggested max_length based on 95th percentile\n",
    "suggested_max_length = min(2048, stats['95th_percentile'] + 50)  # Don't exceed 2048\n",
    "print(f\"Suggested max_length: {suggested_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bcedf0-f3d6-4b8f-b2d1-02f9559369a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef4c1da-dc68-478a-8f9a-363c52128653",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade \"transformers>=4.42.0\" \"accelerate>=0.26.0\" -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0524b9d7-f88f-4285-b643-cca9840f7bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════\n",
    "# 1‑CLICK SET‑UP  (≈2‑3 min)\n",
    "# ════════════════════════════════════════════════════════════════\n",
    "# !pip -q install --upgrade \"transformers>=4.40.0\" \"datasets\" \"peft>=0.10.0\" \\\n",
    "#                 \"bitsandbytes\" \"accelerate\" \"evaluate\" \"sentencepiece\" \"tqdm\" \\\n",
    "#                 \"pandas\"\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 0) CONSTANTS\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "import torch\n",
    "\n",
    "HF_TOKEN        = \"\" \n",
    "BASE_ID         = \"meta-llama/Llama-3.2-1B\"\n",
    "CACHE_DIR       = \"/content/llama3.2-1B\"          # anything writable\n",
    "MED_FT_DIR      = \"/content/llama_medmcqa_ft\"\n",
    "FINAL_FT_DIR    = \"/content/llama_clinical_ft\"\n",
    "CSV_PATH        = \"/clinical_notes.csv\"   # dialogue/note columns\n",
    "DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import os, torch, pandas as pd\n",
    "from datasets          import load_dataset, Dataset as HFDataset\n",
    "from torch.utils.data  import Dataset\n",
    "from peft              import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers      import (AutoTokenizer, AutoModelForCausalLM,\n",
    "                               BitsAndBytesConfig, TrainingArguments,\n",
    "                               DataCollatorForLanguageModeling, Trainer,\n",
    "                               GenerationConfig, pipeline)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1) LOAD BASE MODEL IN 4‑bit + PREPARE FOR LoRA\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "print(\"step 1 done\\n\")\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit         = True,\n",
    "    bnb_4bit_quant_type  = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_ID, token=HF_TOKEN, cache_dir=CACHE_DIR,\n",
    "    padding_side=\"left\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token      # llama has <|end_of_text|> as eos\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_ID,\n",
    "    token               = HF_TOKEN,\n",
    "    cache_dir           = CACHE_DIR,\n",
    "    quantization_config = bnb_cfg,\n",
    "    device_map          = \"auto\",\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "base_model.config.use_cache = False\n",
    "base_model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"step 2 done\\n\")\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type     = \"CAUSAL_LM\",\n",
    "    r             = 8,\n",
    "    lora_alpha    = 32,\n",
    "    lora_dropout  = 0.05,\n",
    "    target_modules= [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
    ")\n",
    "model = get_peft_model(base_model, lora_cfg).to(DEVICE)\n",
    "\n",
    "print(\"step 3 done\\n\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2) DATASET WRAPPERS  (causal‑LM style)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def pack_example(prompt:str, answer:str) -> str:\n",
    "    \"\"\"\n",
    "    Single string with prompt + answer separated by a special tag.\n",
    "    The model is trained to reproduce everything **after** the prompt,\n",
    "    so we put the answer right after a sentinel.\n",
    "    \"\"\"\n",
    "    return f\"<s>[PROMPT]\\n{prompt}\\n[ANSWER]\\n{answer}</s>\"\n",
    "\n",
    "class CausalDS(Dataset):\n",
    "    def __init__(self, pairs, tok, max_len=1024):\n",
    "        self.examples, self.tok, self.max_len = pairs, tok, max_len\n",
    "    def __len__(self): return len(self.examples)\n",
    "    def __getitem__(self, i):\n",
    "        text = self.examples[i]\n",
    "        ids  = self.tok(\n",
    "                 text, truncation=True, padding=\"max_length\",\n",
    "                 max_length=self.max_len, return_tensors=\"pt\"\n",
    "               ).input_ids.squeeze()\n",
    "        labels       = ids.clone()\n",
    "        labels[labels == self.tok.pad_token_id] = -100\n",
    "        return {\"input_ids\": ids, \"labels\": labels}\n",
    "\n",
    "print(\"step 4 done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a2254-6949-4f8e-9e8e-bb760a38b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3) STAGE‑1  (MedMCQA  → multiple‑choice reasoning)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "print(\"▶ Stage‑1 fine‑tune on MedMCQA\")\n",
    "\n",
    "med_raw  = load_dataset(\"openlifescienceai/medmcqa\", split=\"train[:100]\")\n",
    "pairs_1  = []\n",
    "for ex in med_raw:\n",
    "    q   = ex[\"question\"]\n",
    "    ops = [ex[k] for k in (\"opa\",\"opb\",\"opc\",\"opd\")]\n",
    "    ans = ex[\"cop\"]              # like \"A\"\n",
    "    prompt  = f\"Q: {q}\\nOptions:\\nA. {ops[0]}\\nB. {ops[1]}\\nC. {ops[2]}\\nD. {ops[3]}\"\n",
    "    pairs_1.append(pack_example(prompt, ans))\n",
    "\n",
    "train_ds1 = CausalDS(pairs_1, tokenizer)\n",
    "args1 = TrainingArguments(\n",
    "    output_dir               = MED_FT_DIR,\n",
    "    num_train_epochs         = 1,\n",
    "    per_device_train_batch_size = 8,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    optim                    = \"paged_adamw_32bit\",\n",
    "    logging_steps            = 50,\n",
    "    save_strategy            = \"epoch\",\n",
    "    bf16                     = torch.cuda.is_bf16_supported(),\n",
    ")\n",
    "trainer1 = Trainer(\n",
    "    model           = model,\n",
    "    args            = args1,\n",
    "    train_dataset   = train_ds1,\n",
    "    data_collator   = DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "print(\"training started\\n\")\n",
    "trainer1.train()\n",
    "trainer1.save_model(MED_FT_DIR)\n",
    "tokenizer.save_pretrained(MED_FT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d097b-5e18-4fc3-840a-2bc4a9a96ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4) STAGE‑2  (dialogue → structured summary)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "print(\"\\n▶ Stage‑2 fine‑tune on clinical notes\")\n",
    "CSV_PATH = \"clinical_notes.csv\"   # dialogue/note columns\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          MED_FT_DIR, device_map=\"auto\", quantization_config=bnb_cfg, token = HF_TOKEN)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_cfg).to(DEVICE)       # reuse same adapter\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)[[\"dialogue\",\"note\"]]\n",
    "\n",
    "print(\"loaded df\")\n",
    "pairs_2 = []\n",
    "for d,n in df.itertuples(index=False):\n",
    "    prompt = f\"Summarize the following conversation into structured medical note:\\n{d}\"\n",
    "    pairs_2.append(pack_example(prompt, n))\n",
    "\n",
    "print(\"stage 1\")\n",
    "train_ds2 = CausalDS(pairs_2[:400], tokenizer)           # 800 for demo\n",
    "eval_ds2  = CausalDS(pairs_2[400:464], tokenizer)\n",
    "\n",
    "args2 = TrainingArguments(\n",
    "    output_dir               = FINAL_FT_DIR,\n",
    "    num_train_epochs         = 1,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    optim                    = \"paged_adamw_32bit\",\n",
    "    logging_steps            = 50,\n",
    "    save_strategy            = \"epoch\",\n",
    "    bf16                     = torch.cuda.is_bf16_supported(),\n",
    ")\n",
    "\n",
    "print(\"stage2\")\n",
    "trainer2 = Trainer(\n",
    "    model           = model,\n",
    "    args            = args2,\n",
    "    train_dataset   = train_ds2,\n",
    "    eval_dataset    = eval_ds2,\n",
    "    data_collator   = DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "print(\"start training\")\n",
    "trainer2.train()\n",
    "trainer2.save_model(FINAL_FT_DIR)\n",
    "tokenizer.save_pretrained(FINAL_FT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3145ee3e-298c-4dc6-b400-d14abd8ce85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5) BATCH INFERENCE  (same interface as before)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "print(\"\\n▶ Batch inference\")\n",
    "gen_cfg = GenerationConfig(\n",
    "    max_new_tokens = 120,\n",
    "    do_sample      = False,\n",
    "    pad_token_id   = tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model        = model,\n",
    "    tokenizer    = tokenizer,\n",
    "    generation_config = gen_cfg,\n",
    ")\n",
    "\n",
    "# build prompts\n",
    "eval_prompts = []\n",
    "eval_refs    = []\n",
    "for d,n in df.iloc[400:464].itertuples(index=False):\n",
    "    eval_prompts.append(f\"Summarize the following conversation into\"\n",
    "                        f\" structured medical note:\\n{d}\")\n",
    "    eval_refs.append(n)\n",
    "\n",
    "batch_size = 4\n",
    "preds = []\n",
    "for i in range(0, len(eval_prompts), batch_size):\n",
    "    outs = summarizer(eval_prompts[i:i+batch_size])\n",
    "    preds.extend([str(o[\"generated_text\"]) for out in outs for o in out])\n",
    "    print(i)\n",
    "\n",
    "print(f\"Generated {len(preds)} summaries ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350a8588-8311-4b04-bf27-06c26d052478",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72d4ecc-1ba4-46eb-82a3-80f9616eb9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab92c86-5a0f-49a1-baa7-2e0e9b5adc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"UniEval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee338a-9b82-4abd-a30c-eb860804f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d852765-0ebf-48ca-a991-b81e7c1a741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils            import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# ── 1. load the SAME slice you summarised ──────────────────────────────\n",
    "CSV_PATH = \"clinical_notes.csv\"\n",
    "START, END = 400, 464                    # ← whatever slice you used\n",
    "\n",
    "df_slice = pd.read_csv(CSV_PATH).iloc[START:END]\n",
    "\n",
    "src_list  = df_slice[\"dialogue\"].astype(str).tolist()   # source dialogue\n",
    "ref_list  = df_slice[\"note\"].astype(str).tolist()       # gold summary\n",
    "hyp_list  = preds                                 # ← your 64 outputs\n",
    "\n",
    "assert len(src_list) == len(ref_list) == len(hyp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd06bd8-adad-4656-8155-cf46671362fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 2. instantiate evaluators (CPU is fine) ───────────────────────────\n",
    "sum_eval  = get_evaluator(\"summarization\", device=\"cpu\")\n",
    "fact_eval = get_evaluator(\"fact\",          device=\"cpu\")\n",
    "\n",
    "# ── 3. prepare JSON for UniEval ───────────────────────────────────────\n",
    "data_json = convert_to_json(\n",
    "    output_list = hyp_list,\n",
    "    src_list    = src_list,\n",
    "    ref_list    = ref_list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbfbc64-cb61-460f-8511-4ad416b77c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"unieval_data.json\", \"w\") as f:\n",
    "    json.dump(data_json, f, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22f20d-fbc3-414e-a336-cc2ae9c9d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4‑a. coherence • consistency • fluency • relevance ────────────────\n",
    "tri = sum_eval.evaluate(\n",
    "        data_json,\n",
    "        dims=[\"coherence\",\"consistency\",\"fluency\",\"relevance\"],\n",
    "        overall=False, print_result = True)                 # ndarray (N,4)\n",
    "\n",
    "# ── 4‑b. factual consistency ──────────────────────────────────────────\n",
    "fc = np.array([d[\"consistency\"] for d in fact_eval.evaluate(data_json)])\\\n",
    "       .reshape(-1, 1)\n",
    "\n",
    "scores      = np.concatenate([tri, fc], axis=1)         # (N,5)\n",
    "dim_names   = [\"coherence\",\"consistency\",\"fluency\",\n",
    "               \"relevance\",\"factual\"]\n",
    "mean_scores = dict(zip(dim_names, scores.mean(0)))\n",
    "\n",
    "# ── 5. print nice summary ─────────────────────────────────────────────\n",
    "print(\"── UniEval mean scores over\", len(hyp_list), \"summaries ──\")\n",
    "for k, v in mean_scores.items():\n",
    "    print(f\"{k:11s}: {v:7.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff58c102-2dc2-4b52-8417-76fabc4bd0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = get_evaluator(\"fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f77c3-9e1b-4a90-be7a-6cb5913f039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores = evaluator.evaluate(data_json, print_result = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f93fc-37dc-4c9b-a863-0fb5cc31e399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (kshitij)",
   "language": "python",
   "name": "kshitij"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
