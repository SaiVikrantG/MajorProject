{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmcpE6YmbYY-"
   },
   "outputs": [],
   "source": [
    "pip install -q \"transformers==4.38.2\" datasets torch pandas bitsandbytes tqdm \"accelerate==0.28.0\" \"trl==0.7.4\" \"peft==0.10.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w2l9lD9iDGTY",
    "outputId": "b6b08e8b-7ad5-450d-ef01-4c9694181166"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iClazXalGyUJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ──────────┐\n",
    "# 0) Monkey‑patch accelerate to drop dispatch_batches (fixes Trainer bug on 4.38.2)\n",
    "# ──────────┘\n",
    "import accelerate\n",
    "from accelerate import Accelerator\n",
    "_acc_init = Accelerator.__init__\n",
    "def _patched_acc_init(self, *args, **kwargs):\n",
    "    kwargs.pop(\"dispatch_batches\", None)\n",
    "    return _acc_init(self, *args, **kwargs)\n",
    "Accelerator.__init__ = _patched_acc_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcDwBz5ORwQ3"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import transformers.modeling_utils as _mod_utils\n",
    "\n",
    "# if it's already there (unlikely), skip\n",
    "if not hasattr(_mod_utils, \"EncoderDecoderCache\"):\n",
    "    class EncoderDecoderCache:\n",
    "        \"\"\"\n",
    "        Dummy placeholder so Seq2SeqTrainer can import it.\n",
    "        No functional cache behavior — Trainer won’t actually use it.\n",
    "        \"\"\"\n",
    "        def __init__(self, **kwargs): pass\n",
    "\n",
    "    # inject into both the submodule and top‐level namespace\n",
    "    _mod_utils.EncoderDecoderCache    = EncoderDecoderCache\n",
    "    transformers.EncoderDecoderCache  = EncoderDecoderCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNstmBGKDGV9"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import transformers.modeling_utils as _mod_utils\n",
    "\n",
    "# if it's already there (unlikely), skip\n",
    "if not hasattr(_mod_utils, \"EncoderDecoderCache\"):\n",
    "    class EncoderDecoderCache:\n",
    "        \"\"\"\n",
    "        Dummy placeholder so Seq2SeqTrainer can import it.\n",
    "        No functional cache behavior — Trainer won’t actually use it.\n",
    "        \"\"\"\n",
    "        def __init__(self, **kwargs): pass\n",
    "\n",
    "    # inject into both the submodule and top‐level namespace\n",
    "    _mod_utils.EncoderDecoderCache    = EncoderDecoderCache\n",
    "    transformers.EncoderDecoderCache  = EncoderDecoderCache\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    pipeline\n",
    ")\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BASE_MODEL   = \"facebook/bart-base\"\n",
    "MED_FT_DIR   = r\"D:\\kshitij-weights-folder\\bart-med-ft\"\n",
    "FINAL_FT_DIR = r\"D:\\kshitij-weights-folder\\bart-final-ft\"\n",
    "CSV_PATH     = r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\"  # must have columns: dialogue, note\n",
    "\n",
    "# ──────────┐\n",
    "# 2) Dataset wrappers\n",
    "# ──────────┘\n",
    "class MedMCQADataset(TorchDataset):\n",
    "    def __init__(self, hf_ds, tok, max_src=256, max_tgt=16):\n",
    "        self.tok, self.max_src, self.max_tgt = tok, max_src, max_tgt\n",
    "        self.examples = []\n",
    "        for row in hf_ds:\n",
    "            q    = str(row[\"question\"])\n",
    "            opts = [str(row[f\"op{c}\"]) for c in (\"a\",\"b\",\"c\",\"d\")]\n",
    "            ans  = str(row[\"cop\"])\n",
    "            prompt = f\"Question: {q} Options: A){opts[0]} B){opts[1]} C){opts[2]} D){opts[3]}\"\n",
    "            self.examples.append((prompt, ans))\n",
    "    def __len__(self): return len(self.examples)\n",
    "    def __getitem__(self,i):\n",
    "        prompt, ans = self.examples[i]\n",
    "        src = self.tok(prompt,\n",
    "                       truncation=True, padding=\"max_length\",\n",
    "                       max_length=self.max_src, return_tensors=\"pt\")\n",
    "        tgt = self.tok(ans,\n",
    "                       truncation=True, padding=\"max_length\",\n",
    "                       max_length=self.max_tgt, return_tensors=\"pt\")\n",
    "        labels = tgt.input_ids.clone()\n",
    "        labels[labels==self.tok.pad_token_id] = -100\n",
    "        return {\n",
    "          \"input_ids\":      src.input_ids.squeeze(),\n",
    "          \"attention_mask\": src.attention_mask.squeeze(),\n",
    "          \"labels\":         labels.squeeze(),\n",
    "        }\n",
    "\n",
    "class DialogueSummaryDataset(TorchDataset):\n",
    "    def __init__(self, hf_ds, tok, max_src=512, max_tgt=256):\n",
    "        self.ds, self.tok = hf_ds, tok\n",
    "        self.max_src, self.max_tgt = max_src, max_tgt\n",
    "    def __len__(self): return len(self.ds)\n",
    "    def __getitem__(self,i):\n",
    "        row     = self.ds[i]\n",
    "        src_txt = str(row[\"dialogue\"])\n",
    "        tgt_txt = str(row[\"note\"])\n",
    "        src = self.tok(src_txt,\n",
    "                       truncation=True, padding=\"max_length\",\n",
    "                       max_length=self.max_src, return_tensors=\"pt\")\n",
    "        tgt = self.tok(tgt_txt,\n",
    "                       truncation=True, padding=\"max_length\",\n",
    "                       max_length=self.max_tgt, return_tensors=\"pt\")\n",
    "        labels = tgt.input_ids.clone()\n",
    "        labels[labels==self.tok.pad_token_id] = -100\n",
    "        return {\n",
    "          \"input_ids\":      src.input_ids.squeeze(),\n",
    "          \"attention_mask\": src.attention_mask.squeeze(),\n",
    "          \"labels\":         labels.squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 613
    },
    "id": "jof_zsM1DGYp",
    "outputId": "7ea8aabf-acec-4356-c122-a4f0c1c5bc56"
   },
   "outputs": [],
   "source": [
    "# ──────────┐\n",
    "# 3) Stage 1: MedMCQA fine‑tuning\n",
    "# ──────────┘\n",
    "print(\"=== Stage 1: MedMCQA fine‑tuning ===\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL).to(DEVICE)\n",
    "\n",
    "med_ds    = load_dataset(\"openlifescienceai/medmcqa\")\n",
    "train_med = med_ds[\"train\"].select(range(5000))\n",
    "eval_med  = med_ds[\"validation\"].select(range(500))\n",
    "\n",
    "train1 = MedMCQADataset(train_med, tokenizer)\n",
    "eval1  = MedMCQADataset(eval_med,  tokenizer)\n",
    "\n",
    "args1 = TrainingArguments(\n",
    "    output_dir           = MED_FT_DIR,\n",
    "    num_train_epochs     = 1,\n",
    "    per_device_train_batch_size = 8,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy        = \"epoch\",\n",
    "    logging_steps        = 50,\n",
    "    fp16                 = torch.cuda.is_available(),\n",
    ")\n",
    "trainer1 = Trainer(\n",
    "    model            = model,\n",
    "    args             = args1,\n",
    "    train_dataset    = train1,\n",
    "    eval_dataset     = eval1,\n",
    "    tokenizer        = tokenizer,\n",
    ")\n",
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sN6S9q0FDGbd",
    "outputId": "c045f455-fdfa-44f2-eb4c-13dbbfaaedb4"
   },
   "outputs": [],
   "source": [
    "trainer1.save_model(MED_FT_DIR)\n",
    "model.save_pretrained(\n",
    "    MED_FT_DIR,\n",
    "    safe_serialization=False  # Crucial for preserving buffers\n",
    ")\n",
    "tokenizer.save_pretrained(MED_FT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "GdBjcv0GDGeG",
    "outputId": "dc2dee9e-e610-4bc0-ab5b-22209c813248"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ──────────┐\n",
    "# 4) Stage 2: Clinical‑notes fine‑tuning\n",
    "# ──────────┘\n",
    "print(\"=== Stage 2: Clinical notes fine‑tuning ===\")\n",
    "# reload on CPU then send to DEVICE\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(MED_FT_DIR, device_map=\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MED_FT_DIR)\n",
    "\n",
    "# fix pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "df      = pd.read_csv(CSV_PATH)[[\"dialogue\",\"note\"]]\n",
    "hf_clin = HFDataset.from_pandas(df)\n",
    "\n",
    "train_clin = hf_clin.shuffle(seed=42).select(range(400))\n",
    "eval_clin  = hf_clin.shuffle(seed=42).select(range(400,464))\n",
    "\n",
    "train2 = DialogueSummaryDataset(train_clin, tokenizer)\n",
    "eval2  = DialogueSummaryDataset(eval_clin,  tokenizer)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "args2 = TrainingArguments(\n",
    "    output_dir               = FINAL_FT_DIR,\n",
    "    num_train_epochs         = 1,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    evaluation_strategy            = \"epoch\",\n",
    "    save_strategy            = \"epoch\",\n",
    "    logging_steps            = 50,\n",
    "    fp16                     = torch.cuda.is_available(),\n",
    ")\n",
    "trainer2 = Trainer(\n",
    "    model            = model,\n",
    "    args             = args2,\n",
    "    train_dataset    = train2,\n",
    "    eval_dataset     = eval2,\n",
    "    tokenizer        = tokenizer,\n",
    "    data_collator    = data_collator,\n",
    ")\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Li2EWPf4DGg1",
    "outputId": "20cd4828-a0ea-49a3-d03e-635454732eca"
   },
   "outputs": [],
   "source": [
    "trainer2.save_model(FINAL_FT_DIR)\n",
    "tokenizer.save_pretrained(FINAL_FT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYr0V4kaDGjV",
    "outputId": "af0bafa6-ece4-4f0b-8b51-4fe1544f3761"
   },
   "outputs": [],
   "source": [
    "# ──────────┐\n",
    "# 5) Batch inference & evaluation (manual)\n",
    "# ──────────┘\n",
    "print(\"=== Stage 3: Batch inference ===\")\n",
    "\n",
    "batch_size  = 4\n",
    "num_samples = len(eval_clin)\n",
    "num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "predictions, references = [], []\n",
    "for i in range(num_batches):\n",
    "    start, end = i*batch_size, min((i+1)*batch_size, num_samples)\n",
    "    convs = [str(x) for x in eval_clin[\"dialogue\"][start:end]]\n",
    "    refs  = [str(x) for x in eval_clin[\"note\"][start:end]]\n",
    "\n",
    "    prompts = [\n",
    "        f\"Summarize the following conversation:\\n\\n{c}\"\n",
    "        for c in convs if len(c.strip()) > 10\n",
    "    ]\n",
    "    if not prompts:\n",
    "        continue\n",
    "\n",
    "    # 1) Tokenize on GPU\n",
    "    enc = tokenizer(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")                     # <-- send inputs to GPU\n",
    "\n",
    "    # 2) Generate on GPU\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            input_ids      = enc.input_ids,\n",
    "            attention_mask = enc.attention_mask,\n",
    "            max_new_tokens = 120,\n",
    "            do_sample      = False\n",
    "        )\n",
    "\n",
    "    # 3) Decode back on CPU\n",
    "    dec = tokenizer.batch_decode(out_ids, skip_special_tokens=True)\n",
    "    predictions.extend(dec)\n",
    "    references.extend(refs)\n",
    "\n",
    "print(f\"✅ Generated {len(predictions)} summaries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "lYfmO0mLDGmJ",
    "outputId": "e9079991-9cd8-4246-9bfc-027f85c3ed84"
   },
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVqA21JGDGo8",
    "outputId": "80e16ca6-9fcf-45c6-a8a0-19e88fcfbe9b"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")  # if needed to make sure your Python can import from the UniEval folder\n",
    "\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "# on CUDA since you asked for it\n",
    "sum_eval  = get_evaluator(\"summarization\", device=\"cuda\")\n",
    "# fact_eval = get_evaluator(\"fact\",          device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pPCdyjODGr5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yL2lqVF8DGum",
    "outputId": "244ba35c-4ac2-466f-bd26-cb0f270e9e57"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\unieval_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "sum_scores = sum_eval.evaluate(\n",
    "    data,\n",
    "    print_result=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "io-CW0r2DGxB",
    "outputId": "f3f330ac-cd72-445c-ec35-4df52274adf3"
   },
   "outputs": [],
   "source": [
    "fact_dicts = fact_eval.evaluate(data, print_result = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSugVKbsDGzs"
   },
   "outputs": [],
   "source": [
    "from utils            import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "import os, sys, gc, torch, pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# sum_eval  = get_evaluator(\"summarization\", device=\"cpu\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def unieval_4way(src, hyp):\n",
    "    \"\"\"\n",
    "    src, hyp: list[str] of equal length → (B,4) Tensor on CPU:\n",
    "      [coherence, consistency, fluency, factual]\n",
    "    \"\"\"\n",
    "    data = convert_to_json(output_list=hyp, src_list=src)\n",
    "    arr = sum_eval.evaluate(data, print_result=False)  # shape (B,4) but only first 3 used\n",
    "    # the summarization evaluator by default returns [coherence, consistency, fluency, relevance]\n",
    "    coh = arr[:,0].tolist()\n",
    "    con = arr[:,1].tolist()\n",
    "    flu = arr[:,2].tolist()\n",
    "    rel = arr[:,3].tolist()\n",
    "    return torch.tensor([coh,con,flu,rel]).T  # (B,4) on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vN1f039IWvZP",
    "outputId": "6fa420cd-2c6f-49bb-9fa1-2fdf83fb6a30"
   },
   "outputs": [],
   "source": [
    "from utils            import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "import os, sys, gc, torch, pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "sum_eval  = get_evaluator(\"summarization\", device=\"cpu\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def unieval_4way(src, hyp):\n",
    "     \"\"\"\n",
    "     src, hyp: list[str] → (B,4) Tensor on CPU:\n",
    "       [coherence, consistency, fluency, relevance]\n",
    "     \"\"\"\n",
    "\n",
    "     data = convert_to_json(output_list=hyp, src_list=src)\n",
    "     arr = sum_eval.evaluate(data, print_result=False)   # → numpy (B,4)\n",
    "     coh, con, flu, rel = arr.T.tolist()\n",
    "     return torch.tensor([coh, con, flu, rel]).T       # (B,4) on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEeJ_JNIDG1_",
    "outputId": "ec3bd3f3-be8e-42c6-b62b-4e55a0ba6873"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from trl import (\n",
    "    PPOConfig,\n",
    "    PPOTrainer,\n",
    "    AutoModelForSeq2SeqLMWithValueHead\n",
    ")\n",
    "\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SFT_DIR  = r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\bart_clinical_ft-20250422T192130Z-001\\bart_clinical_ft\"  # your SFT BART folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UIvd4DQMDG4t",
    "outputId": "b2aa343d-70ef-48f0-8132-ef77b87f9519"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "# 2a) 4‑bit quant config\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit            = True,\n",
    "    bnb_4bit_quant_type     = \"nf4\",\n",
    "    bnb_4bit_compute_dtype  = torch.float16,\n",
    ")\n",
    "\n",
    "# 2b) causal‐LM load (avoids the Seq2Seq final_logits_bias mismatch)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_DIR,\n",
    "    quantization_config = bnb_cfg,\n",
    "    device_map          = \"auto\",\n",
    ")\n",
    "# freeze + prepare for k‑bit + gradient checkpoint\n",
    "base = prepare_model_for_kbit_training(base)\n",
    "base.gradient_checkpointing_enable()\n",
    "base.config.use_cache = False\n",
    "\n",
    "# 2c) attach LoRA for causal‑LM\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r          = 8,\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg).to(DEVICE)\n",
    "\n",
    "# 2d) tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(SFT_DIR, use_fast=False)\n",
    "tok.pad_token = tok.eos_token\n",
    "model.config.pad_token_id = tok.eos_token_id\n",
    "model.resize_token_embeddings(len(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11iQyS9YDG7a",
    "outputId": "1a60f4a9-0504-428e-c79d-c41ea46bfd1a"
   },
   "outputs": [],
   "source": [
    "# 2e) wrap for PPO\n",
    "ppo_model     = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model, peft_config=lora_cfg\n",
    ").to(DEVICE)\n",
    "ppo_ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model, peft_config=lora_cfg\n",
    ").to(DEVICE).eval()\n",
    "for p in ppo_ref_model.parameters(): p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKWgJ4cyDG-V",
    "outputId": "6d7ef1b3-4009-4658-9548-fd79aed9eba8"
   },
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────\n",
    "# 3) same ClinDS + loader as before\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "# [unchanged code defining ClinDS and loader...]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "# 4) PPO setup (same)\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "ppo_cfg = PPOConfig(\n",
    "    batch_size      = 2,\n",
    "    mini_batch_size = 1,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, ppo_model.parameters()),\n",
    "    lr=2e-5\n",
    ")\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config     = ppo_cfg,\n",
    "    model      = ppo_model,\n",
    "    ref_model  = ppo_ref_model,\n",
    "    tokenizer  = tok,\n",
    "    optimizer  = optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTB6RII0VBRK"
   },
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────\n",
    "# 5) PC‑Grad utilities\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "def pc_merge(flats: list[torch.Tensor]) -> torch.Tensor:\n",
    "    # Yu et al. 2020 PC‑Grad\n",
    "    for i in range(len(flats)):\n",
    "        for j in range(i+1, len(flats)):\n",
    "            dot = torch.dot(flats[i], flats[j])\n",
    "            if dot < 0:\n",
    "                flats[i] -= (dot / (flats[j].norm()**2 + 1e-12)) * flats[j]\n",
    "    return torch.stack(flats).mean(0)\n",
    "\n",
    "def flat_param_grads(model) -> torch.Tensor:\n",
    "    return torch.cat([\n",
    "        p.grad.detach().flatten()\n",
    "        for p in model.parameters() if p.grad is not None\n",
    "    ])\n",
    "\n",
    "def scatter_flat_grads(model, flat: torch.Tensor):\n",
    "    idx = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None: continue\n",
    "        n = p.grad.numel()\n",
    "        p.grad.data = flat[idx:idx+n].view_as(p).clone()\n",
    "        idx += n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qlz_Nu_fVPy1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "# 3) Dataset & DataLoader (clinical_notes.csv must have 'dialogue','note')\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\")[[\"dialogue\",\"note\"]]\n",
    "\n",
    "class ClinDS(Dataset):\n",
    "    def __init__(self, df, tok, max_len=512):\n",
    "        self.df, self.tok, self.L = df.reset_index(drop=True), tok, max_len\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        txt = str(self.df.iloc[i][\"dialogue\"])\n",
    "        enc = self.tok(txt,\n",
    "                       truncation=True,\n",
    "                       max_length=self.L,\n",
    "                       padding=\"max_length\",\n",
    "                       return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\":      enc.input_ids.squeeze(),\n",
    "            \"attention_mask\": enc.attention_mask.squeeze(),\n",
    "            \"src_txt\":        txt\n",
    "        }\n",
    "\n",
    "loader = DataLoader(\n",
    "    ClinDS(df.sample(200, random_state=0), tok),\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqgkOB3tthPw"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")  # if needed to make sure your Python can import from the UniEval folder\n",
    "\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LnEn7lxwuz8"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import transformers.modeling_utils as _mod_utils\n",
    "\n",
    "# if it's already there (unlikely), skip\n",
    "if not hasattr(_mod_utils, \"EncoderDecoderCache\"):\n",
    "    class EncoderDecoderCache:\n",
    "        \"\"\"\n",
    "        Dummy placeholder so Seq2SeqTrainer can import it.\n",
    "        No functional cache behavior — Trainer won’t actually use it.\n",
    "        \"\"\"\n",
    "        def __init__(self, **kwargs): pass\n",
    "\n",
    "    # inject into both the submodule and top‐level namespace\n",
    "    _mod_utils.EncoderDecoderCache    = EncoderDecoderCache\n",
    "    transformers.EncoderDecoderCache  = EncoderDecoderCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YE6TsU_4wvV4"
   },
   "source": [
    "run these 2 cells above first, before running the RL code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeYdSvniuxW6"
   },
   "source": [
    "REINFORCEMENT LEARNING CODE !!! (NEW TECHNIQUE USED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRt-2cYSwnca"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from utils            import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "sum_eval = get_evaluator(\"summarization\", device=\"cuda\")\n",
    "\n",
    "# @torch.inference_mode()\n",
    "# def unieval_4way(src: list[str], hyp: list[str], ref: list[str]) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     src, hyp, ref: lists of strings, length B\n",
    "#     Returns a (B,4) CPU tensor with columns [coherence, consistency, fluency, relevance].\n",
    "#     \"\"\"\n",
    "#     data = convert_to_json(\n",
    "#         output_list = hyp,\n",
    "#         src_list    = src,\n",
    "#         ref_list    = ref,\n",
    "#     )\n",
    "#     # This returns a list of dicts: [{'coherence':…, 'consistency':…, 'fluency':…, 'relevance':…}, …]\n",
    "#     raw = sum_eval.evaluate(data, print_result=False)\n",
    "\n",
    "#     dims = [\"coherence\", \"consistency\", \"fluency\", \"relevance\"]\n",
    "#     arr  = np.array([[d[dim] for dim in dims] for d in raw], dtype=np.float32)\n",
    "#     return torch.from_numpy(arr)  # shape (B,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "accelerator.state.skip_key = [\"src_txt\", \"ref_txt\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqebEJgvVBNz",
    "outputId": "c96e113e-b687-41da-f562-37e824ca9ae6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════\n",
    "# Requirements:\n",
    "#   pip install trl==0.7.4 transformers==4.38.2 peft==0.10.0 \\\n",
    "#               accelerate==0.28.0 bitsandbytes datasets evaluate pandas\n",
    "# ════════════════════════════════════════════════════════════════\n",
    "import os, gc, torch, pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.device('cpu')\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1) UniEval multi‑dim evaluator (CPU only, load once)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")  # if needed to make sure your Python can import from the UniEval folder\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "sum_eval = get_evaluator(\"summarization\", device=\"cpu\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def unieval_4way(src, hyp, ref):\n",
    "    \"\"\"\n",
    "    src, hyp, ref: lists of strings, length B\n",
    "    returns: Tensor (B,4) with [coherence, consistency, fluency, relevance]\n",
    "    \"\"\"\n",
    "    data = convert_to_json(\n",
    "        output_list=hyp,\n",
    "        src_list=src,\n",
    "        ref_list=ref,\n",
    "    )\n",
    "    raw = sum_eval.evaluate(data)\n",
    "    scores = [\n",
    "        [d[\"coherence\"], d[\"consistency\"], d[\"fluency\"], d[\"relevance\"]]\n",
    "        for d in raw\n",
    "    ]\n",
    "    return torch.tensor(scores, dtype=torch.float32).cpu()  # CPU (B,4)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2) Load your SFT‑finetuned BART in 4‑bit + LoRA\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"cuda\"\n",
    "SFT_DIR = r\"D:\\kshitij-weights-folder\\bart_clinical_ft-20250422T192130Z-001\\bart_clinical_ft\"\n",
    "\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit            = True,\n",
    "    bnb_4bit_quant_type     = \"nf4\",\n",
    "    bnb_4bit_compute_dtype  = torch.float32,   # ← here\n",
    ")\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_DIR,\n",
    "    quantization_config=bnb,\n",
    "    device_map=\"auto\",\n",
    "    max_memory={0: \"16GiB\"},\n",
    ")\n",
    "\n",
    "\n",
    "base = prepare_model_for_kbit_training(base)\n",
    "base.gradient_checkpointing_enable()\n",
    "base.config.use_cache = False\n",
    "\n",
    "# 2b) Attach fresh LoRA\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg).to(DEVICE)\n",
    "\n",
    "# 2c) Tokenizer (decoder‑only → left‑pad)\n",
    "tok = AutoTokenizer.from_pretrained(SFT_DIR, use_fast=False)\n",
    "tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"left\"\n",
    "model.config.pad_token_id = tok.eos_token_id\n",
    "model.resize_token_embeddings(len(tok))\n",
    "\n",
    "# … after you do your existing wrap …\n",
    "\n",
    "ppo_model     = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model, peft_config=lora_cfg\n",
    ").to(DEVICE)\n",
    "ppo_ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model, peft_config=lora_cfg\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "# Here’s the key: cast **both** to fp16 so the value head and the backbone match\n",
    "ppo_model     = ppo_model\n",
    "ppo_ref_model = ppo_ref_model\n",
    "\n",
    "for p in ppo_ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3) Prepare your DataLoader (with references)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\")[[\"dialogue\", \"note\"]]\n",
    "\n",
    "class ClinDS(Dataset):\n",
    "    def __init__(self, df, tok, max_len=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tok = tok\n",
    "        self.L = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        conv = str(self.df.iloc[i][\"dialogue\"])\n",
    "        ref = str(self.df.iloc[i][\"note\"])\n",
    "        prompt = f\"Summarize the following conversation:\\n\\n{conv}\"\n",
    "        enc = self.tok(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.L,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc.input_ids.squeeze(),\n",
    "            \"attention_mask\": enc.attention_mask.squeeze(),\n",
    "            \"src_txt\": prompt,\n",
    "            \"ref_txt\": ref,\n",
    "        }\n",
    "\n",
    "loader = DataLoader(\n",
    "    ClinDS(df.sample(200, random_state=0), tok),\n",
    "    batch_size=1, shuffle=True, pin_memory=False, drop_last=True\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4) Build PPOTrainer + optimizer\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "NUM_CANDIDATES = 2\n",
    "ppo_cfg = PPOConfig(\n",
    "  batch_size      = loader.batch_size * NUM_CANDIDATES,  # e.g. 1 * 2 = 2\n",
    "  mini_batch_size = 2,          # or split it if you like\n",
    "  # log_with        = \"tensorboard\"\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, ppo_model.parameters()),\n",
    "    lr=2e-5\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_cfg,\n",
    "    model=ppo_model,\n",
    "    ref_model=ppo_ref_model,\n",
    "    tokenizer=tok,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5) Training loop with candidate generation and dominance rewards\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 64,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tok.eos_token_id,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "for epoch in range(1):\n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        ids       = batch[\"input_ids\"].to(DEVICE)\n",
    "        attn_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        src_txt    = batch[\"src_txt\"]\n",
    "        ref_txt    = batch[\"ref_txt\"]\n",
    "\n",
    "        # Generate multiple candidates per prompt\n",
    "        NUM_CANDIDATES = 2\n",
    "        all_outs = []\n",
    "        for _ in range(NUM_CANDIDATES):\n",
    "            with torch.no_grad():\n",
    "                out = ppo_model.generate(\n",
    "                    input_ids=ids,\n",
    "                    attention_mask=attn_mask,\n",
    "                    **gen_kwargs\n",
    "                )\n",
    "            all_outs.append(out)\n",
    "\n",
    "        # Stack outputs (B, K, L)\n",
    "        outs = torch.stack(all_outs, dim=1)\n",
    "\n",
    "        # Decode all candidates\n",
    "        hyps = [\n",
    "            [tok.decode(outs[b, k], skip_special_tokens=True)\n",
    "            for k in range(NUM_CANDIDATES)]\n",
    "            for b in range(outs.size(0))\n",
    "        ]\n",
    "\n",
    "        # Compute rewards using UniEval and dominance scoring\n",
    "        rewards = []\n",
    "        for b in range(len(src_txt)):\n",
    "            # Get scores for all candidates (K, 4)\n",
    "            scores = unieval_4way(\n",
    "                [src_txt[b]] * NUM_CANDIDATES,\n",
    "                hyps[b],\n",
    "                [ref_txt[b]] * NUM_CANDIDATES\n",
    "            ).numpy()\n",
    "\n",
    "            # Compute dominance counts\n",
    "            dom_counts = np.zeros(NUM_CANDIDATES)\n",
    "            for i in range(NUM_CANDIDATES):\n",
    "                for j in range(NUM_CANDIDATES):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    # Check if i dominates j\n",
    "                    if np.all(scores[i] >= scores[j]) and np.any(scores[i] > scores[j]):\n",
    "                        dom_counts[i] += 1\n",
    "\n",
    "            # Normalize to [-1, 1]\n",
    "            max_dom = NUM_CANDIDATES - 1\n",
    "            scalar_rewards = 2 * (dom_counts / max_dom) - 1\n",
    "            rewards.append(scalar_rewards)\n",
    "\n",
    "        # Flatten for PPO\n",
    "        flat_queries = []\n",
    "        flat_responses = []\n",
    "        flat_rewards = []\n",
    "\n",
    "        for b in range(len(src_txt)):\n",
    "            for k in range(NUM_CANDIDATES):\n",
    "                flat_queries.append(ids[b])\n",
    "                flat_responses.append(outs[b, k])\n",
    "                flat_rewards.append(torch.tensor([rewards[b][k]], device=DEVICE))\n",
    "\n",
    "        # PPO step\n",
    "        # … after your `for b in …` loops that populate flat_queries, flat_responses, flat_rewards …\n",
    "\n",
    "# make sure your PPOConfig was set to batch_size = loader.batch_size * NUM_CANDIDATES\n",
    "# so here batch_size == len(flat_queries) == len(flat_responses) == len(flat_rewards)\n",
    "\n",
    "        stats = ppo_trainer.step(\n",
    "            queries   = flat_queries,    # e.g. [ q0, q0 ]\n",
    "            responses = flat_responses,  # e.g. [ r0, r1 ]\n",
    "            scores    = flat_rewards     # e.g. [ s0, s1 ]\n",
    "        )\n",
    "\n",
    "\n",
    "        # Logging\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}\")\n",
    "            print(f\"Sample output: {hyps[0][0][:100]}...\")\n",
    "            print(f\"Average reward: {np.mean([r.item() for r in flat_rewards]):.4f}\")\n",
    "\n",
    "    print(f\"✅ Epoch {epoch+1}/3 complete\")\n",
    "\n",
    "print(\"🎉 PPO fine-tuning done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at D:\\kshitij-weights-folder\\bart_clinical_ft-20250422T192130Z-001\\bart_clinical_ft and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py:254: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\generation\\utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Processing batch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries: 2, Responses: 2, Rewards: 2\n",
      "Error in PPO step: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "Continuing to encounter CUDA errors. Try two options:\n",
      "1. Change DEVICE = 'cpu' at the top of the script\n",
      "2. Or use the non-quantized model version\n",
      "\n",
      "Processing batch 1\n",
      "Error processing batch 1: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 2\n",
      "Error processing batch 2: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 3\n",
      "Error processing batch 3: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 4\n",
      "Error processing batch 4: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 5\n",
      "Error processing batch 5: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 6\n",
      "Error processing batch 6: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 7\n",
      "Error processing batch 7: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 8\n",
      "Error processing batch 8: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 9\n",
      "Error processing batch 9: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 10\n",
      "Error processing batch 10: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 11\n",
      "Error processing batch 11: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 12\n",
      "Error processing batch 12: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 13\n",
      "Error processing batch 13: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 14\n",
      "Error processing batch 14: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 15\n",
      "Error processing batch 15: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 16\n",
      "Error processing batch 16: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 17\n",
      "Error processing batch 17: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 18\n",
      "Error processing batch 18: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 19\n",
      "Error processing batch 19: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 20\n",
      "Error processing batch 20: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 21\n",
      "Error processing batch 21: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 22\n",
      "Error processing batch 22: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 23\n",
      "Error processing batch 23: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 24\n",
      "Error processing batch 24: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 25\n",
      "Error processing batch 25: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 26\n",
      "Error processing batch 26: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 27\n",
      "Error processing batch 27: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 28\n",
      "Error processing batch 28: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 29\n",
      "Error processing batch 29: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 30\n",
      "Error processing batch 30: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 31\n",
      "Error processing batch 31: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 32\n",
      "Error processing batch 32: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 33\n",
      "Error processing batch 33: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 34\n",
      "Error processing batch 34: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 35\n",
      "Error processing batch 35: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 36\n",
      "Error processing batch 36: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 37\n",
      "Error processing batch 37: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 38\n",
      "Error processing batch 38: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 39\n",
      "Error processing batch 39: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 40\n",
      "Error processing batch 40: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 41\n",
      "Error processing batch 41: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 42\n",
      "Error processing batch 42: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 43\n",
      "Error processing batch 43: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 44\n",
      "Error processing batch 44: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 45\n",
      "Error processing batch 45: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 46\n",
      "Error processing batch 46: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 47\n",
      "Error processing batch 47: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 48\n",
      "Error processing batch 48: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Processing batch 49\n",
      "Error processing batch 49: CUDA error: device-side assert triggered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "✅ Epoch 1 complete\n",
      "🎉 PPO fine-tuning done\n"
     ]
    }
   ],
   "source": [
    "# ════════════════════════════════════════════════════════════════\n",
    "# Requirements:\n",
    "#   pip install trl==0.7.4 transformers==4.38.2 peft==0.10.0 \\\n",
    "#               accelerate==0.28.0 bitsandbytes datasets evaluate pandas\n",
    "# ════════════════════════════════════════════════════════════════\n",
    "import os, gc, torch, pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set environment variable to debug CUDA issues\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1) UniEval multi‑dim evaluator (CPU only, load once)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "# Initialize evaluator on CPU to avoid device conflicts\n",
    "sum_eval = get_evaluator(\"summarization\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def unieval_4way(src, hyp, ref):\n",
    "    \"\"\"\n",
    "    src, hyp, ref: lists of strings, length B\n",
    "    returns: Tensor (B,4) with [coherence, consistency, fluency, relevance]\n",
    "    \"\"\"\n",
    "    data = convert_to_json(\n",
    "        output_list=hyp,\n",
    "        src_list=src,\n",
    "        ref_list=ref,\n",
    "    )\n",
    "    raw = sum_eval.evaluate(data)\n",
    "    scores = [\n",
    "        [d[\"coherence\"], d[\"consistency\"], d[\"fluency\"], d[\"relevance\"]]\n",
    "        for d in raw\n",
    "    ]\n",
    "    return torch.tensor(scores, dtype=torch.float32).cpu()  # CPU (B,4)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2) Load your SFT‑finetuned BART (try without quantization first)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "# Try using CPU if CUDA is unstable\n",
    "# Uncomment the line below to use CPU instead\n",
    "# DEVICE = \"cpu\"  \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Use consistent compute dtype\n",
    "COMPUTE_DTYPE = torch.float32  # Using float32 to avoid dtype issues\n",
    "\n",
    "SFT_DIR = r\"D:\\kshitij-weights-folder\\bart_clinical_ft-20250422T192130Z-001\\bart_clinical_ft\"\n",
    "\n",
    "# OPTION 1: Try without quantization first to eliminate that source of errors\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_DIR,\n",
    "    torch_dtype=COMPUTE_DTYPE,\n",
    "    device_map={\"\": 0} if DEVICE == \"cuda\" else \"auto\",  # Explicitly set device mapping\n",
    "    # Reduce the model footprint\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# 2b) Attach fresh LoRA\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "# 2c) Tokenizer (ensure padding_side is explicitly set)\n",
    "tok = AutoTokenizer.from_pretrained(SFT_DIR, use_fast=False)\n",
    "tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"left\"  # Explicitly set left padding for decoder-only models\n",
    "model.config.pad_token_id = tok.eos_token_id\n",
    "model.resize_token_embeddings(len(tok))\n",
    "\n",
    "# 2d) Wrap for PPO with consistent dtype\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model, \n",
    "    peft_config=lora_cfg,\n",
    "    torch_dtype=COMPUTE_DTYPE,\n",
    ").to(DEVICE)\n",
    "\n",
    "ppo_ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model, \n",
    "    peft_config=lora_cfg,\n",
    "    torch_dtype=COMPUTE_DTYPE,\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "# Freeze reference model\n",
    "for p in ppo_ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3) Prepare your DataLoader (with references) - NO PIN MEMORY\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\")[[\"dialogue\", \"note\"]]\n",
    "\n",
    "class ClinDS(Dataset):\n",
    "    def __init__(self, df, tok, max_len=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tok = tok\n",
    "        self.L = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        conv = str(self.df.iloc[i][\"dialogue\"])\n",
    "        ref = str(self.df.iloc[i][\"note\"])\n",
    "        prompt = f\"Summarize the following conversation:\\n\\n{conv}\"\n",
    "        enc = self.tok(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.L,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc.input_ids.squeeze(),\n",
    "            \"attention_mask\": enc.attention_mask.squeeze(),\n",
    "            \"src_txt\": prompt,\n",
    "            \"ref_txt\": ref,\n",
    "        }\n",
    "\n",
    "# Disable pin_memory to avoid CUDA memory issues\n",
    "loader = DataLoader(\n",
    "    ClinDS(df.sample(50, random_state=0), tok),  # Reduce sample size to 50\n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    "    pin_memory=False,  # IMPORTANT: Disabled pin_memory\n",
    "    drop_last=True,\n",
    "    num_workers=0,  # Use single-process data loading\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4) Build PPOTrainer + optimizer\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "NUM_CANDIDATES = 2\n",
    "\n",
    "# Configure PPO with exactly matching batch sizes\n",
    "ppo_cfg = PPOConfig(\n",
    "    batch_size=loader.batch_size * NUM_CANDIDATES,  # Must match the actual number of examples\n",
    "    mini_batch_size=loader.batch_size * NUM_CANDIDATES,  # Process all at once for simplicity\n",
    "    gradient_accumulation_steps=1,\n",
    "    optimize_device_cache=True,  # Use newer parameter\n",
    "    learning_rate=2e-5,\n",
    "    log_with=None,  # Disable wandb/tensorboard logging\n",
    ")\n",
    "\n",
    "# Optimizer with explicit dtype\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, ppo_model.parameters()),\n",
    "    lr=2e-5,\n",
    "    eps=1e-5,  # Slightly larger epsilon for stability\n",
    ")\n",
    "\n",
    "# Create PPO trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_cfg,\n",
    "    model=ppo_model,\n",
    "    ref_model=ppo_ref_model,\n",
    "    tokenizer=tok,\n",
    "    optimizer=optimizer,\n",
    "    data_collator=None,  # Don't use a data collator\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5) Simplified PPO Training Loop\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def get_rewards(src, hyp, ref):\n",
    "    \"\"\"Compute rewards using UniEval and dominance scoring.\"\"\"\n",
    "    # Get scores using UniEval\n",
    "    scores = unieval_4way(src, hyp, ref).numpy()\n",
    "    \n",
    "    # Count dominance relationships\n",
    "    k = len(hyp)\n",
    "    dom_counts = np.zeros(k)\n",
    "    \n",
    "    for i in range(k):\n",
    "        for j in range(k):\n",
    "            if i == j:\n",
    "                continue\n",
    "            # Check dominance: i dominates j if all scores are >= and at least one is >\n",
    "            if np.all(scores[i] >= scores[j]) and np.any(scores[i] > scores[j]):\n",
    "                dom_counts[i] += 1\n",
    "    \n",
    "    # Convert to [-1, 1] range reward\n",
    "    max_dom = k - 1\n",
    "    if max_dom > 0:\n",
    "        rewards = 2 * (dom_counts / max_dom) - 1\n",
    "    else:\n",
    "        rewards = np.zeros(k)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# Generation parameters\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 64,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tok.eos_token_id,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "# Main training function\n",
    "def train():\n",
    "    try:\n",
    "        for epoch in range(1):\n",
    "            print(f\"Starting epoch {epoch+1}\")\n",
    "            \n",
    "            # Process one batch at a time\n",
    "            for batch_idx, batch in enumerate(loader):\n",
    "                try:\n",
    "                    print(f\"Processing batch {batch_idx}\")\n",
    "                    \n",
    "                    # Clear GPU cache before each batch\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                    # Get inputs\n",
    "                    ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                    attn_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "                    src_txt = batch[\"src_txt\"]\n",
    "                    ref_txt = batch[\"ref_txt\"]\n",
    "                    \n",
    "                    # 1. Generate candidates\n",
    "                    all_outs = []\n",
    "                    for _ in range(NUM_CANDIDATES):\n",
    "                        with torch.no_grad():\n",
    "                            try:\n",
    "                                out = ppo_model.generate(\n",
    "                                    input_ids=ids,\n",
    "                                    attention_mask=attn_mask,\n",
    "                                    **gen_kwargs\n",
    "                                )\n",
    "                                all_outs.append(out)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error during generation: {e}\")\n",
    "                                # Try to continue with what we have\n",
    "                                if not all_outs:\n",
    "                                    raise  # Re-raise if we have no outputs\n",
    "                    \n",
    "                    # Stack and decode\n",
    "                    outs = torch.stack(all_outs, dim=1)\n",
    "                    B, K, _ = outs.shape\n",
    "                    \n",
    "                    # Decode outputs for evaluation\n",
    "                    hyps = []\n",
    "                    for b in range(B):\n",
    "                        hyps_b = []\n",
    "                        for k in range(K):\n",
    "                            try:\n",
    "                                text = tok.decode(outs[b, k], skip_special_tokens=True)\n",
    "                                hyps_b.append(text)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error decoding text: {e}\")\n",
    "                                hyps_b.append(\"\")  # Add empty string as fallback\n",
    "                        hyps.append(hyps_b)\n",
    "                    \n",
    "                    # 2. Compute rewards\n",
    "                    flat_queries, flat_responses, flat_rewards = [], [], []\n",
    "                    \n",
    "                    for b in range(B):\n",
    "                        try:\n",
    "                            # Calculate rewards\n",
    "                            rewards_b = get_rewards(\n",
    "                                [src_txt[b]] * K, \n",
    "                                hyps[b],\n",
    "                                [ref_txt[b]] * K\n",
    "                            )\n",
    "                            \n",
    "                            # Flatten for PPO\n",
    "                            for k in range(K):\n",
    "                                flat_queries.append(ids[b])\n",
    "                                flat_responses.append(outs[b, k])\n",
    "                                flat_rewards.append(torch.tensor([rewards_b[k]], device=DEVICE, dtype=COMPUTE_DTYPE))\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error computing rewards: {e}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Safety check\n",
    "                    if len(flat_queries) != ppo_cfg.batch_size:\n",
    "                        print(f\"Batch size mismatch: expected {ppo_cfg.batch_size}, got {len(flat_queries)}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 3. PPO step\n",
    "                    try:\n",
    "                        # Verify shapes match\n",
    "                        print(f\"Queries: {len(flat_queries)}, Responses: {len(flat_responses)}, Rewards: {len(flat_rewards)}\")\n",
    "                        \n",
    "                        # Manual memory management\n",
    "                        torch.cuda.empty_cache()\n",
    "                        \n",
    "                        # Do PPO step\n",
    "                        stats = ppo_trainer.step(\n",
    "                            queries=flat_queries,\n",
    "                            responses=flat_responses,\n",
    "                            scores=flat_rewards\n",
    "                        )\n",
    "                        \n",
    "                        # Success! Log the output\n",
    "                        print(f\"Batch {batch_idx} PPO step successful!\")\n",
    "                        print(f\"Sample output: {hyps[0][0][:100]}...\")\n",
    "                        avg_reward = np.mean([r.item() for r in flat_rewards])\n",
    "                        print(f\"Average reward: {avg_reward:.4f}\")\n",
    "                        \n",
    "                    except RuntimeError as e:\n",
    "                        print(f\"Error in PPO step: {e}\")\n",
    "                        \n",
    "                        # If still running into CUDA errors, try moving to CPU\n",
    "                        if \"CUDA\" in str(e) and DEVICE != \"cpu\":\n",
    "                            print(\"\\nContinuing to encounter CUDA errors. Try two options:\")\n",
    "                            print(\"1. Change DEVICE = 'cpu' at the top of the script\")\n",
    "                            print(\"2. Or use the non-quantized model version\\n\")\n",
    "                            \n",
    "                        # Clear memory and continue\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Break after a few batches during testing\n",
    "                if batch_idx >= 2:  # Process 3 batches for testing\n",
    "                    print(\"Processed 3 test batches successfully, exiting test run\")\n",
    "                    break\n",
    "                \n",
    "            print(f\"✅ Epoch {epoch+1} complete\")\n",
    "        \n",
    "        print(\"🎉 PPO fine-tuning done\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Will save checkpoints to: ppo_checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at D:\\kshitij-weights-folder\\bart_clinical_ft-20250422T192130Z-001\\bart_clinical_ft and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py:254: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\generation\\utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PPO training on cuda\n",
      "Model device: cuda:0\n",
      "Starting epoch 1\n",
      "\n",
      "Processing batch 0\n",
      "Input tensor device: cuda:0\n",
      "Generating candidates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] so...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] so...\n",
      "Candidates shape: torch.Size([1, 2, 576]), device: cuda:0\n",
      "Calculating UniEval scores...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 8/8 [01:52<00:00, 14.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 8/8 [00:04<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.05010976 0.90272814 0.5243288  0.39502192]\n",
      " [0.01079702 0.90268236 0.5245611  0.04552022]]\n",
      "Calculating dominance rewards...\n",
      "Dominance rewards: [-1. -1.]\n",
      "Preparing PPO inputs...\n",
      "Queries: 2, device: cuda:0\n",
      "Responses: 2, device: cuda:0\n",
      "Rewards: 2, device: cuda:0\n",
      "Executing PPO step...\n",
      "❌ Error processing batch 0: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`\n",
      "\n",
      "❌ Training error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\AppData\\Local\\Temp\\ipykernel_17688\\3337554649.py\", line 288, in train_ppo_loop\n",
      "    stats = ppo_trainer.step(\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py\", line 706, in step\n",
      "    all_logprobs, logits_or_none, values, masks = self.batched_forward_pass(\n",
      "                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py\", line 978, in batched_forward_pass\n",
      "    logits, _, values = model(**input_kwargs)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\models\\modeling_value_head.py\", line 182, in forward\n",
      "    value = self.v_head(last_hidden_state).squeeze(-1)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\models\\modeling_value_head.py\", line 57, in forward\n",
      "    output = self.summary(output)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\AppData\\Local\\Temp\\ipykernel_17688\\3337554649.py\", line 288, in train_ppo_loop\n",
      "    stats = ppo_trainer.step(\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py\", line 706, in step\n",
      "    all_logprobs, logits_or_none, values, masks = self.batched_forward_pass(\n",
      "                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\contextlib.py\", line 81, in inner\n",
      "    return func(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py\", line 978, in batched_forward_pass\n",
      "    logits, _, values = model(**input_kwargs)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\models\\modeling_value_head.py\", line 182, in forward\n",
      "    value = self.v_head(last_hidden_state).squeeze(-1)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\models\\modeling_value_head.py\", line 57, in forward\n",
      "    output = self.summary(output)\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\AppData\\Local\\Temp\\ipykernel_17688\\3337554649.py\", line 310, in train_ppo_loop\n",
      "    torch.cuda.empty_cache()\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\cuda\\memory.py\", line 218, in empty_cache\n",
      "    torch._C._cuda_emptyCache()\n",
      "RuntimeError: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ════════════════════════════════════════════════════════════════\n",
    "# Final Working PPO Implementation (Fixed Device Tracking)\n",
    "# ════════════════════════════════════════════════════════════════\n",
    "import os, gc, torch, pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1) Device Configuration\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Check if CUDA is available\n",
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "DEVICE = \"cuda\" if CUDA_AVAILABLE else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Create save directory if it doesn't exist\n",
    "SAVE_DIR = \"ppo_checkpoints\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"Will save checkpoints to: {SAVE_DIR}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2) UniEval multi‑dim evaluator (always on CPU)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "# Initialize evaluator on CPU (always keep on CPU)\n",
    "sum_eval = get_evaluator(\"summarization\", device=\"cpu\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def unieval_4way(src, hyp, ref):\n",
    "    \"\"\"\n",
    "    src, hyp, ref: lists of strings, length B\n",
    "    returns: Tensor (B,4) with [coherence, consistency, fluency, relevance]\n",
    "    \"\"\"\n",
    "    data = convert_to_json(\n",
    "        output_list=hyp,\n",
    "        src_list=src,\n",
    "        ref_list=ref,\n",
    "    )\n",
    "    raw = sum_eval.evaluate(data)\n",
    "    scores = [\n",
    "        [d[\"coherence\"], d[\"consistency\"], d[\"fluency\"], d[\"relevance\"]]\n",
    "        for d in raw\n",
    "    ]\n",
    "    # Return scores on CPU, will move to GPU if needed\n",
    "    return torch.tensor(scores, dtype=torch.float32)  # CPU (B,4)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3) Load your SFT‑finetuned BART (no BitsAndBytes)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "# Use consistent compute dtype\n",
    "COMPUTE_DTYPE = torch.float32  # Using float32 for stability\n",
    "\n",
    "SFT_DIR = r\"D:\\kshitij-weights-folder\\bart_clinical_ft-20250422T192130Z-001\\bart_clinical_ft\"\n",
    "\n",
    "# Load model - NO BitsAndBytes\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_DIR,\n",
    "    torch_dtype=COMPUTE_DTYPE,\n",
    ")\n",
    "# Move to device after loading\n",
    "base = base.to(DEVICE)\n",
    "\n",
    "# Add LoRA \n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "model = model.to(DEVICE)  # Move model to device\n",
    "\n",
    "# Tokenizer setup\n",
    "tok = AutoTokenizer.from_pretrained(SFT_DIR, use_fast=False)\n",
    "tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"left\"  # Important for decoder-only models\n",
    "model.config.pad_token_id = tok.eos_token_id\n",
    "model.resize_token_embeddings(len(tok))\n",
    "\n",
    "# PPO models\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model, \n",
    "    peft_config=lora_cfg,\n",
    "    torch_dtype=COMPUTE_DTYPE,\n",
    ").to(DEVICE)\n",
    "\n",
    "ppo_ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model, \n",
    "    peft_config=lora_cfg,\n",
    "    torch_dtype=COMPUTE_DTYPE,\n",
    ").to(DEVICE).eval()\n",
    "\n",
    "# Freeze reference model\n",
    "for p in ppo_ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4) Prepare your DataLoader\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\")[[\"dialogue\", \"note\"]]\n",
    "\n",
    "class ClinDS(Dataset):\n",
    "    def __init__(self, df, tok, max_len=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tok = tok\n",
    "        self.L = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        conv = str(self.df.iloc[i][\"dialogue\"])\n",
    "        ref = str(self.df.iloc[i][\"note\"])\n",
    "        prompt = f\"Summarize the following conversation:\\n\\n{conv}\"\n",
    "        enc = self.tok(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.L,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc.input_ids.squeeze(),\n",
    "            \"attention_mask\": enc.attention_mask.squeeze(),\n",
    "            \"src_txt\": prompt,\n",
    "            \"ref_txt\": ref,\n",
    "        }\n",
    "\n",
    "# Small dataset size for testing\n",
    "loader = DataLoader(\n",
    "    ClinDS(df.sample(5, random_state=0), tok),  # Start with just 5 samples\n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    "    pin_memory=False,  # Disable pin_memory\n",
    "    drop_last=True,\n",
    "    num_workers=0,  # Use single-process data loading\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5) Build PPOTrainer + optimizer\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "NUM_CANDIDATES = 2\n",
    "\n",
    "# Configure PPO with matching batch sizes\n",
    "ppo_cfg = PPOConfig(\n",
    "    batch_size=loader.batch_size * NUM_CANDIDATES,\n",
    "    mini_batch_size=loader.batch_size * NUM_CANDIDATES,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-5,\n",
    "    optimize_device_cache=True if CUDA_AVAILABLE else False,\n",
    "    log_with=None,  # Disable wandb/tensorboard logging\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, ppo_model.parameters()),\n",
    "    lr=2e-5,\n",
    "    eps=1e-5,\n",
    ")\n",
    "\n",
    "# Create PPO trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_cfg,\n",
    "    model=ppo_model,\n",
    "    ref_model=ppo_ref_model,\n",
    "    tokenizer=tok,\n",
    "    optimizer=optimizer,\n",
    "    data_collator=None,\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 6) Improved PPO Training Loop with Careful Device Management\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Generation parameters\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 64,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tok.eos_token_id,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "def get_model_device(model):\n",
    "    \"\"\"Helper to get the device of a model by checking its parameters\"\"\"\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "def train_ppo_loop():\n",
    "    \"\"\"Improved PPO training loop with careful device management\"\"\"\n",
    "    print(f\"Starting PPO training on {DEVICE}\")\n",
    "    print(f\"Model device: {get_model_device(ppo_model)}\")\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(1):\n",
    "            print(f\"Starting epoch {epoch+1}\")\n",
    "            \n",
    "            for batch_idx, batch in enumerate(loader):\n",
    "                print(f\"\\nProcessing batch {batch_idx}\")\n",
    "                \n",
    "                try:\n",
    "                    # 1. Move batch to device\n",
    "                    ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                    attn_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "                    src_txt = batch[\"src_txt\"]\n",
    "                    ref_txt = batch[\"ref_txt\"]\n",
    "                    \n",
    "                    print(f\"Input tensor device: {ids.device}\")\n",
    "                    \n",
    "                    # 2. Generate candidates\n",
    "                    print(\"Generating candidates...\")\n",
    "                    all_candidates = []\n",
    "                    decoded_candidates = []\n",
    "                    \n",
    "                    for c_idx in range(NUM_CANDIDATES):\n",
    "                        with torch.no_grad():\n",
    "                            # Generate\n",
    "                            output = ppo_model.generate(\n",
    "                                input_ids=ids,\n",
    "                                attention_mask=attn_mask,\n",
    "                                **gen_kwargs\n",
    "                            )\n",
    "                            all_candidates.append(output)\n",
    "                            \n",
    "                            # Decode for evaluation\n",
    "                            decoded = tok.decode(output[0], skip_special_tokens=True)\n",
    "                            decoded_candidates.append(decoded)\n",
    "                            print(f\"Candidate {c_idx+1}: {decoded[:50]}...\")\n",
    "                    \n",
    "                    # Stack candidates\n",
    "                    candidates = torch.stack(all_candidates, dim=1)\n",
    "                    B, K, _ = candidates.shape\n",
    "                    print(f\"Candidates shape: {candidates.shape}, device: {candidates.device}\")\n",
    "                    \n",
    "                    # 3. Get scores using UniEval\n",
    "                    print(\"Calculating UniEval scores...\")\n",
    "                    scores = unieval_4way(\n",
    "                        [src_txt[0]] * K,\n",
    "                        decoded_candidates,\n",
    "                        [ref_txt[0]] * K\n",
    "                    ).numpy()\n",
    "                    print(f\"UniEval scores: {scores}\")\n",
    "                    \n",
    "                    # 4. Calculate dominance rewards\n",
    "                    print(\"Calculating dominance rewards...\")\n",
    "                    dom_counts = np.zeros(K)\n",
    "                    for i in range(K):\n",
    "                        for j in range(K):\n",
    "                            if i == j:\n",
    "                                continue\n",
    "                            # Check if i dominates j\n",
    "                            if np.all(scores[i] >= scores[j]) and np.any(scores[i] > scores[j]):\n",
    "                                dom_counts[i] += 1\n",
    "                    \n",
    "                    max_dom = K - 1\n",
    "                    rewards = 2 * (dom_counts / max_dom) - 1 if max_dom > 0 else np.zeros(K)\n",
    "                    print(f\"Dominance rewards: {rewards}\")\n",
    "                    \n",
    "                    # 5. Prepare inputs for PPO step\n",
    "                    print(\"Preparing PPO inputs...\")\n",
    "                    flat_queries = []\n",
    "                    flat_responses = []\n",
    "                    flat_rewards = []\n",
    "                    \n",
    "                    for b in range(B):\n",
    "                        for k in range(K):\n",
    "                            flat_queries.append(ids[b])\n",
    "                            flat_responses.append(candidates[b, k])\n",
    "                            # Create reward tensor on correct device\n",
    "                            reward_tensor = torch.tensor([rewards[k]], dtype=COMPUTE_DTYPE, device=DEVICE)\n",
    "                            flat_rewards.append(reward_tensor)\n",
    "                    \n",
    "                    # Verify shapes for PPO\n",
    "                    print(f\"Queries: {len(flat_queries)}, device: {flat_queries[0].device}\")\n",
    "                    print(f\"Responses: {len(flat_responses)}, device: {flat_responses[0].device}\")\n",
    "                    print(f\"Rewards: {len(flat_rewards)}, device: {flat_rewards[0].device}\")\n",
    "                    \n",
    "                    # 6. Run PPO step\n",
    "                    print(\"Executing PPO step...\")\n",
    "                    stats = ppo_trainer.step(\n",
    "                        queries=flat_queries,\n",
    "                        responses=flat_responses,\n",
    "                        scores=flat_rewards\n",
    "                    )\n",
    "                    \n",
    "                    # 7. Log success\n",
    "                    print(f\"✅ PPO step successful for batch {batch_idx}!\")\n",
    "                    print(f\"Stats: {stats}\")\n",
    "                    \n",
    "                    # Clean up memory\n",
    "                    if CUDA_AVAILABLE:\n",
    "                        torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error processing batch {batch_idx}: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    \n",
    "                    # Try to recover and continue with next batch\n",
    "                    if CUDA_AVAILABLE:\n",
    "                        torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "                \n",
    "                # For testing purposes, just do a few batches\n",
    "                if batch_idx >= 2:\n",
    "                    print(\"\\nTest run completed with 3 batches\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"\\n✅ Epoch {epoch+1} complete\")\n",
    "            \n",
    "            # Save the model after each epoch\n",
    "            print(\"Saving model checkpoint...\")\n",
    "            save_path = os.path.join(SAVE_DIR, f\"epoch_{epoch+1}\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            ppo_model.save_pretrained(save_path)\n",
    "            \n",
    "            # Save the tokenizer\n",
    "            tok.save_pretrained(save_path)\n",
    "        \n",
    "        print(\"\\n🎉 PPO fine-tuning complete\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Training error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run training\n",
    "train_ppo_loop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAR FROM HEREEEEE\n"
     ]
    }
   ],
   "source": [
    "print(\"STAR FROM HEREEEEE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU for all operations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not load bitsandbytes native library: list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\bitsandbytes\\cextension.py\", line 85, in <module>\n",
      "    lib = get_native_library()\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\bitsandbytes\\cextension.py\", line 64, in get_native_library\n",
      "    cuda_specs = get_cuda_specs()\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\bitsandbytes\\cuda_specs.py\", line 38, in get_cuda_specs\n",
      "    highest_compute_capability=(get_compute_capabilities()[-1]),\n",
      "                                ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^\n",
      "IndexError: list index out of range\n",
      "\n",
      "CUDA Setup failed despite CUDA being available. Please run the following command to get more information:\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n",
      "to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n",
      "and open an issue at: https://github.com/bitsandbytes-foundation/bitsandbytes/issues\n",
      "\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting reward-based training (no TRL)\n",
      "\n",
      "==============================\n",
      "Epoch 1/3\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                                                        | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] he...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] he...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.05s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.74s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.35s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.03s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.8156485  0.51292425 0.62240136 0.19431536]\n",
      " [0.67899764 0.66789716 0.5546173  0.18404898]]\n",
      "Best candidate: 1 with score 2.1453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  10%|██████▍                                                         | 1/10 [00:17<02:38, 17.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.2263, Weighted Loss: 1.0258\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.07s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.62s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.06s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.12032465 0.09880885 0.17077254 0.35931796]\n",
      " [0.23346081 0.2542215  0.1789969  0.32776153]]\n",
      "Best candidate: 2 with score 0.9944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  20%|████████████▊                                                   | 2/10 [00:32<02:09, 16.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.2906, Weighted Loss: 1.6499\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] ok...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] ok...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.20s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.28s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.59s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.19s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.87280875 0.89389765 0.5762318  0.78019637]\n",
      " [0.7488114  0.73546976 0.73058736 0.58123803]]\n",
      "Best candidate: 1 with score 3.1231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  30%|███████████████████▏                                            | 3/10 [00:48<01:51, 15.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.1939, Weighted Loss: 0.7746\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.02s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.70s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.10s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.07s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.75040156 0.7281368  0.7843533  0.72957665]\n",
      " [0.36572725 0.54545105 0.7890035  0.49600145]]\n",
      "Best candidate: 1 with score 2.9925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  30%|███████████████████▏                                            | 3/10 [01:11<02:47, 23.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.4739, Weighted Loss: 0.8701\n",
      "Processed 4 batches in first epoch for demonstration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete. Average score: 2.3138\n",
      "\n",
      "==============================\n",
      "Epoch 2/3\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|                                                                        | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.10s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.10s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.48s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.98s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.9460837  0.93651366 0.79167867 0.6155328 ]\n",
      " [0.9339346  0.94431704 0.7979428  0.62661076]]\n",
      "Best candidate: 2 with score 3.3028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  10%|██████▍                                                         | 1/10 [00:15<02:16, 15.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7943, Weighted Loss: 0.6494\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] ka...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] ka...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.14s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|██████▋                                                                  | 1/11 [00:14<02:23, 14.31s/it]\u001b[A\n",
      " 18%|█████████████▎                                                           | 2/11 [00:28<02:08, 14.29s/it]\u001b[A\n",
      " 27%|███████████████████▉                                                     | 3/11 [00:43<01:56, 14.57s/it]\u001b[A\n",
      " 36%|██████████████████████████▌                                              | 4/11 [00:57<01:40, 14.34s/it]\u001b[A\n",
      " 45%|█████████████████████████████████▏                                       | 5/11 [01:12<01:27, 14.61s/it]\u001b[A\n",
      " 55%|███████████████████████████████████████▊                                 | 6/11 [01:26<01:11, 14.37s/it]\u001b[A\n",
      " 64%|██████████████████████████████████████████████▍                          | 7/11 [01:41<00:58, 14.57s/it]\u001b[A\n",
      " 73%|█████████████████████████████████████████████████████                    | 8/11 [01:55<00:42, 14.33s/it]\u001b[A\n",
      " 82%|███████████████████████████████████████████████████████████▋             | 9/11 [02:10<00:29, 14.53s/it]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████▍      | 10/11 [02:24<00:14, 14.31s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11/11 [02:30<00:00, 13.68s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|██████▋                                                                  | 1/11 [00:00<00:07,  1.36it/s]\u001b[A\n",
      " 18%|█████████████▎                                                           | 2/11 [00:01<00:04,  2.01it/s]\u001b[A\n",
      " 27%|███████████████████▉                                                     | 3/11 [00:01<00:03,  2.34it/s]\u001b[A\n",
      " 36%|██████████████████████████▌                                              | 4/11 [00:01<00:02,  2.35it/s]\u001b[A\n",
      " 45%|█████████████████████████████████▏                                       | 5/11 [00:02<00:02,  2.45it/s]\u001b[A\n",
      " 55%|███████████████████████████████████████▊                                 | 6/11 [00:02<00:02,  2.19it/s]\u001b[A\n",
      " 64%|██████████████████████████████████████████████▍                          | 7/11 [00:03<00:01,  2.46it/s]\u001b[A\n",
      " 73%|█████████████████████████████████████████████████████                    | 8/11 [00:03<00:01,  2.64it/s]\u001b[A\n",
      " 82%|███████████████████████████████████████████████████████████▋             | 9/11 [00:03<00:00,  2.62it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████▍      | 10/11 [00:04<00:00,  2.71it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.51it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.9370204 0.8676779 0.6193464 0.9311132]\n",
      " [0.9311306 0.8629612 0.6395685 0.9278866]]\n",
      "Best candidate: 2 with score 3.3615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  20%|████████████▌                                                  | 2/10 [03:00<13:48, 103.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.1011, Weighted Loss: 0.7110\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] th...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] th...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.01s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|██████████▌                                                               | 1/7 [00:15<01:35, 15.96s/it]\u001b[A\n",
      " 29%|█████████████████████▏                                                    | 2/7 [00:29<01:13, 14.72s/it]\u001b[A\n",
      " 43%|███████████████████████████████▋                                          | 3/7 [00:45<01:01, 15.26s/it]\u001b[A\n",
      " 57%|██████████████████████████████████████████▎                               | 4/7 [00:59<00:44, 14.67s/it]\u001b[A\n",
      " 71%|████████████████████████████████████████████████████▊                     | 5/7 [01:15<00:30, 15.13s/it]\u001b[A\n",
      " 86%|███████████████████████████████████████████████████████████████▍          | 6/7 [01:29<00:14, 14.69s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 7/7 [01:37<00:00, 13.91s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|██████████▌                                                               | 1/7 [00:00<00:03,  1.59it/s]\u001b[A\n",
      " 29%|█████████████████████▏                                                    | 2/7 [00:01<00:02,  1.71it/s]\u001b[A\n",
      " 43%|███████████████████████████████▋                                          | 3/7 [00:01<00:02,  1.61it/s]\u001b[A\n",
      " 57%|██████████████████████████████████████████▎                               | 4/7 [00:02<00:01,  1.85it/s]\u001b[A\n",
      " 71%|████████████████████████████████████████████████████▊                     | 5/7 [00:02<00:01,  1.88it/s]\u001b[A\n",
      " 86%|███████████████████████████████████████████████████████████████▍          | 6/7 [00:03<00:00,  1.73it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 7/7 [00:03<00:00,  1.79it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.54006886 0.8651444  0.4970894  0.5230327 ]\n",
      " [0.6105341  0.8697496  0.45273086 0.6482121 ]]\n",
      "Best candidate: 2 with score 2.5812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  30%|██████████████████▉                                            | 3/10 [04:52<12:31, 107.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.8384, Weighted Loss: 1.0718\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] pa...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] pa...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.03s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|██████▋                                                                  | 1/11 [00:14<02:21, 14.18s/it]\u001b[A\n",
      " 18%|█████████████▎                                                           | 2/11 [00:27<02:05, 13.94s/it]\u001b[A\n",
      " 27%|███████████████████▉                                                     | 3/11 [00:43<01:58, 14.79s/it]\u001b[A\n",
      " 36%|██████████████████████████▌                                              | 4/11 [00:58<01:42, 14.69s/it]\u001b[A\n",
      " 45%|█████████████████████████████████▏                                       | 5/11 [01:15<01:32, 15.49s/it]\u001b[A\n",
      " 55%|███████████████████████████████████████▊                                 | 6/11 [01:29<01:14, 14.92s/it]\u001b[A\n",
      " 64%|██████████████████████████████████████████████▍                          | 7/11 [01:44<01:00, 15.15s/it]\u001b[A\n",
      " 73%|█████████████████████████████████████████████████████                    | 8/11 [01:58<00:44, 14.74s/it]\u001b[A\n",
      " 82%|███████████████████████████████████████████████████████████▋             | 9/11 [02:14<00:30, 15.04s/it]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████▍      | 10/11 [02:28<00:14, 14.70s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11/11 [02:32<00:00, 13.89s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|██████▋                                                                  | 1/11 [00:00<00:05,  1.95it/s]\u001b[A\n",
      " 18%|█████████████▎                                                           | 2/11 [00:00<00:03,  2.37it/s]\u001b[A\n",
      " 27%|███████████████████▉                                                     | 3/11 [00:01<00:03,  2.51it/s]\u001b[A\n",
      " 36%|██████████████████████████▌                                              | 4/11 [00:01<00:02,  2.50it/s]\u001b[A\n",
      " 45%|█████████████████████████████████▏                                       | 5/11 [00:02<00:02,  2.32it/s]\u001b[A\n",
      " 55%|███████████████████████████████████████▊                                 | 6/11 [00:02<00:01,  2.51it/s]\u001b[A\n",
      " 64%|██████████████████████████████████████████████▍                          | 7/11 [00:02<00:01,  2.51it/s]\u001b[A\n",
      " 73%|█████████████████████████████████████████████████████                    | 8/11 [00:03<00:01,  2.63it/s]\u001b[A\n",
      " 82%|███████████████████████████████████████████████████████████▋             | 9/11 [00:03<00:00,  2.74it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████▍      | 10/11 [00:04<00:00,  2.50it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.52it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.963919   0.91738725 0.80172455 0.9281089 ]\n",
      " [0.95988935 0.9361755  0.7940443  0.9193229 ]]\n",
      "Best candidate: 1 with score 3.6111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  40%|█████████████████████████▏                                     | 4/10 [07:40<13:06, 131.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.4791, Weighted Loss: 0.5376\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] he...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] he...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.95s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.00s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.44s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.94s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.93311125 0.93273103 0.62301916 0.5334728 ]\n",
      " [0.9153697  0.916284   0.6688446  0.5619631 ]]\n",
      "Best candidate: 2 with score 3.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  50%|████████████████████████████████                                | 5/10 [07:54<07:25, 89.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7213, Weighted Loss: 0.6699\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] he...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] he...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.53s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.98s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.5331384  0.5413694  0.6151794  0.21584497]\n",
      " [0.8403657  0.8543995  0.63494474 0.19231147]]\n",
      "Best candidate: 2 with score 2.5220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  60%|██████████████████████████████████████▍                         | 6/10 [08:09<04:15, 63.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.1836, Weighted Loss: 0.9039\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.08s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.45s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.13086264 0.17930661 0.1790385  0.38674003]\n",
      " [0.3100764  0.73759663 0.1800412  0.3527536 ]]\n",
      "Best candidate: 2 with score 1.5805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  70%|████████████████████████████████████████████▊                   | 7/10 [08:24<02:23, 47.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.2222, Weighted Loss: 1.2487\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.06s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.23s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.27s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.08s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.6837006  0.73258626 0.8014618  0.73102564]\n",
      " [0.35653263 0.4764218  0.54118085 0.5983527 ]]\n",
      "Best candidate: 1 with score 2.9488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  80%|███████████████████████████████████████████████████▏            | 8/10 [08:44<01:17, 39.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.3408, Weighted Loss: 0.8460\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.15s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|███████▎                                                                 | 1/10 [00:16<02:30, 16.74s/it]\u001b[A\n",
      " 20%|██████████████▌                                                          | 2/10 [00:30<02:00, 15.01s/it]\u001b[A\n",
      " 30%|█████████████████████▉                                                   | 3/10 [00:46<01:48, 15.48s/it]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 4/10 [01:00<01:28, 14.83s/it]\u001b[A\n",
      " 50%|████████████████████████████████████▌                                    | 5/10 [01:16<01:16, 15.22s/it]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 6/10 [01:30<00:59, 14.79s/it]\u001b[A\n",
      " 70%|███████████████████████████████████████████████████                      | 7/10 [01:46<00:45, 15.22s/it]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 8/10 [02:00<00:29, 14.78s/it]\u001b[A\n",
      " 90%|█████████████████████████████████████████████████████████████████▋       | 9/10 [02:16<00:15, 15.15s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 10/10 [02:19<00:00, 13.93s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|███████▎                                                                 | 1/10 [00:00<00:04,  2.21it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 2/10 [00:00<00:03,  2.01it/s]\u001b[A\n",
      " 30%|█████████████████████▉                                                   | 3/10 [00:01<00:03,  2.29it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 4/10 [00:01<00:02,  2.30it/s]\u001b[A\n",
      " 50%|████████████████████████████████████▌                                    | 5/10 [00:02<00:02,  2.31it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 6/10 [00:02<00:01,  2.29it/s]\u001b[A\n",
      " 70%|███████████████████████████████████████████████████                      | 7/10 [00:03<00:01,  2.32it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 8/10 [00:03<00:00,  2.29it/s]\u001b[A\n",
      " 90%|█████████████████████████████████████████████████████████████████▋       | 9/10 [00:03<00:00,  2.30it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 10/10 [00:04<00:00,  2.44it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.98s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.87769604 0.9130933  0.6716667  0.8509167 ]\n",
      " [0.94064003 0.9007478  0.6831777  0.89207697]]\n",
      "Best candidate: 2 with score 3.4166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  90%|█████████████████████████████████████████████████████████▌      | 9/10 [11:18<01:14, 74.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7297, Weighted Loss: 0.6180\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] ok...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] ok...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.05s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.99s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.47s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.867584   0.8780972  0.6831967  0.76737076]\n",
      " [0.908229   0.9096109  0.59641767 0.7629183 ]]\n",
      "Best candidate: 1 with score 3.1962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████| 10/10 [11:33<00:00, 69.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.1470, Weighted Loss: 0.7499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete. Average score: 2.9583\n",
      "\n",
      "==============================\n",
      "Epoch 3/3\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|                                                                        | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] ka...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] ka...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|██████▋                                                                  | 1/11 [00:15<02:39, 15.95s/it]\u001b[A\n",
      " 18%|█████████████▎                                                           | 2/11 [00:29<02:11, 14.66s/it]\u001b[A\n",
      " 27%|███████████████████▉                                                     | 3/11 [00:45<02:02, 15.25s/it]\u001b[A\n",
      " 36%|██████████████████████████▌                                              | 4/11 [00:59<01:43, 14.83s/it]\u001b[A\n",
      " 45%|█████████████████████████████████▏                                       | 5/11 [01:15<01:31, 15.25s/it]\u001b[A\n",
      " 55%|███████████████████████████████████████▊                                 | 6/11 [01:29<01:13, 14.75s/it]\u001b[A\n",
      " 64%|██████████████████████████████████████████████▍                          | 7/11 [01:45<01:00, 15.10s/it]\u001b[A\n",
      " 73%|█████████████████████████████████████████████████████                    | 8/11 [01:59<00:44, 14.69s/it]\u001b[A\n",
      " 82%|███████████████████████████████████████████████████████████▋             | 9/11 [02:15<00:30, 15.08s/it]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████▍      | 10/11 [02:28<00:14, 14.67s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11/11 [02:35<00:00, 14.16s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|██████▋                                                                  | 1/11 [00:00<00:07,  1.42it/s]\u001b[A\n",
      " 18%|█████████████▎                                                           | 2/11 [00:01<00:04,  1.97it/s]\u001b[A\n",
      " 27%|███████████████████▉                                                     | 3/11 [00:01<00:03,  2.30it/s]\u001b[A\n",
      " 36%|██████████████████████████▌                                              | 4/11 [00:01<00:02,  2.36it/s]\u001b[A\n",
      " 45%|█████████████████████████████████▏                                       | 5/11 [00:02<00:02,  2.62it/s]\u001b[A\n",
      " 55%|███████████████████████████████████████▊                                 | 6/11 [00:02<00:02,  2.34it/s]\u001b[A\n",
      " 64%|██████████████████████████████████████████████▍                          | 7/11 [00:02<00:01,  2.56it/s]\u001b[A\n",
      " 73%|█████████████████████████████████████████████████████                    | 8/11 [00:03<00:01,  2.72it/s]\u001b[A\n",
      " 82%|███████████████████████████████████████████████████████████▋             | 9/11 [00:03<00:00,  2.60it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████▍      | 10/11 [00:04<00:00,  2.58it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.52it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.94s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.9176087  0.85493135 0.68302125 0.9105813 ]\n",
      " [0.93535894 0.9076342  0.6554592  0.9227168 ]]\n",
      "Best candidate: 2 with score 3.4212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  10%|██████▎                                                        | 1/10 [02:50<25:34, 170.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.0959, Weighted Loss: 0.7002\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] th...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] th...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.10s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|██████████▌                                                               | 1/7 [00:14<01:27, 14.62s/it]\u001b[A\n",
      " 29%|█████████████████████▏                                                    | 2/7 [00:28<01:10, 14.15s/it]\u001b[A\n",
      " 43%|███████████████████████████████▋                                          | 3/7 [00:44<00:59, 14.88s/it]\u001b[A\n",
      " 57%|██████████████████████████████████████████▎                               | 4/7 [00:57<00:43, 14.45s/it]\u001b[A\n",
      " 71%|████████████████████████████████████████████████████▊                     | 5/7 [01:13<00:29, 14.86s/it]\u001b[A\n",
      " 86%|███████████████████████████████████████████████████████████████▍          | 6/7 [01:27<00:14, 14.49s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 7/7 [01:38<00:00, 14.07s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|██████████▌                                                               | 1/7 [00:00<00:03,  1.59it/s]\u001b[A\n",
      " 29%|█████████████████████▏                                                    | 2/7 [00:01<00:02,  1.75it/s]\u001b[A\n",
      " 43%|███████████████████████████████▋                                          | 3/7 [00:01<00:02,  1.64it/s]\u001b[A\n",
      " 57%|██████████████████████████████████████████▎                               | 4/7 [00:02<00:01,  1.88it/s]\u001b[A\n",
      " 71%|████████████████████████████████████████████████████▊                     | 5/7 [00:02<00:01,  1.87it/s]\u001b[A\n",
      " 86%|███████████████████████████████████████████████████████████████▍          | 6/7 [00:03<00:00,  1.72it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 7/7 [00:03<00:00,  1.84it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.80291355 0.8594468  0.48933992 0.82005095]\n",
      " [0.78175014 0.87534386 0.48537263 0.85947704]]\n",
      "Best candidate: 2 with score 3.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  20%|████████████▌                                                  | 2/10 [04:43<18:12, 136.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.7720, Weighted Loss: 0.9426\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.03s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.48s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.08s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.09042832 0.06878943 0.21315213 0.37108418]\n",
      " [0.1441121  0.18272017 0.17061749 0.3931847 ]]\n",
      "Best candidate: 2 with score 0.8906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  30%|███████████████████▏                                            | 3/10 [04:58<09:27, 81.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.2289, Weighted Loss: 1.7079\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] he...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] he...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.98s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.22s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.59s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.95s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.87785083 0.87469226 0.6273687  0.1789501 ]\n",
      " [0.81112665 0.8312623  0.5284885  0.18352531]]\n",
      "Best candidate: 1 with score 2.5589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  40%|█████████████████████████▌                                      | 4/10 [05:13<05:29, 54.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.0553, Weighted Loss: 0.8585\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] ok...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] ok...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.01s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.44s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.8433821  0.8518076  0.60936636 0.73403615]\n",
      " [0.8929249  0.9073397  0.6951779  0.75595355]]\n",
      "Best candidate: 2 with score 3.2514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  50%|████████████████████████████████                                | 5/10 [05:28<03:22, 40.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.0752, Weighted Loss: 0.7233\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|███████▎                                                                 | 1/10 [00:14<02:10, 14.54s/it]\u001b[A\n",
      " 20%|██████████████▌                                                          | 2/10 [00:28<01:53, 14.13s/it]\u001b[A\n",
      " 30%|█████████████████████▉                                                   | 3/10 [00:44<01:44, 14.88s/it]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 4/10 [00:57<01:26, 14.44s/it]\u001b[A\n",
      " 50%|████████████████████████████████████▌                                    | 5/10 [01:13<01:14, 14.88s/it]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 6/10 [01:27<00:58, 14.67s/it]\u001b[A\n",
      " 70%|███████████████████████████████████████████████████                      | 7/10 [01:43<00:44, 14.95s/it]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 8/10 [01:57<00:29, 14.56s/it]\u001b[A\n",
      " 90%|█████████████████████████████████████████████████████████████████▋       | 9/10 [02:12<00:14, 14.88s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 10/10 [02:24<00:00, 14.48s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|███████▎                                                                 | 1/10 [00:00<00:04,  1.91it/s]\u001b[A\n",
      " 20%|██████████████▌                                                          | 2/10 [00:01<00:04,  1.73it/s]\u001b[A\n",
      " 30%|█████████████████████▉                                                   | 3/10 [00:01<00:03,  2.10it/s]\u001b[A\n",
      " 40%|█████████████████████████████▏                                           | 4/10 [00:01<00:02,  2.07it/s]\u001b[A\n",
      " 50%|████████████████████████████████████▌                                    | 5/10 [00:02<00:02,  2.05it/s]\u001b[A\n",
      " 60%|███████████████████████████████████████████▊                             | 6/10 [00:02<00:01,  2.05it/s]\u001b[A\n",
      " 70%|███████████████████████████████████████████████████                      | 7/10 [00:03<00:01,  2.09it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████▍              | 8/10 [00:03<00:00,  2.06it/s]\u001b[A\n",
      " 90%|█████████████████████████████████████████████████████████████████▋       | 9/10 [00:04<00:00,  2.06it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 10/10 [00:04<00:00,  2.14it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.10s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.74687743 0.8746346  0.6658515  0.6630001 ]\n",
      " [0.9751532  0.9156578  0.6380515  0.92391837]]\n",
      "Best candidate: 2 with score 3.4528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  60%|██████████████████████████████████████▍                         | 6/10 [08:08<05:24, 81.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.6165, Weighted Loss: 0.5876\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] he...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] he...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.17s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.69s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.34s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.17s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.8441484  0.45822147 0.49691284 0.6073074 ]\n",
      " [0.8685372  0.8697154  0.6761829  0.5553409 ]]\n",
      "Best candidate: 2 with score 2.9698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  70%|████████████████████████████████████████████▊                   | 7/10 [08:25<03:01, 60.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.6237, Weighted Loss: 0.6609\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] pa...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] pa...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.09s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|██████▋                                                                  | 1/11 [00:16<02:41, 16.12s/it]\u001b[A\n",
      " 18%|█████████████▎                                                           | 2/11 [00:30<02:14, 14.95s/it]\u001b[A\n",
      " 27%|███████████████████▉                                                     | 3/11 [00:46<02:04, 15.53s/it]\u001b[A\n",
      " 36%|██████████████████████████▌                                              | 4/11 [01:00<01:45, 15.02s/it]\u001b[A\n",
      " 45%|█████████████████████████████████▏                                       | 5/11 [01:16<01:32, 15.43s/it]\u001b[A\n",
      " 55%|███████████████████████████████████████▊                                 | 6/11 [01:30<01:14, 14.96s/it]\u001b[A\n",
      " 64%|██████████████████████████████████████████████▍                          | 7/11 [01:47<01:01, 15.35s/it]\u001b[A\n",
      " 73%|█████████████████████████████████████████████████████                    | 8/11 [02:00<00:44, 14.86s/it]\u001b[A\n",
      " 82%|███████████████████████████████████████████████████████████▋             | 9/11 [02:17<00:30, 15.36s/it]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████▍      | 10/11 [02:31<00:14, 14.89s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11/11 [02:42<00:00, 14.81s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|██████▋                                                                  | 1/11 [00:00<00:04,  2.33it/s]\u001b[A\n",
      " 18%|█████████████▎                                                           | 2/11 [00:00<00:04,  2.18it/s]\u001b[A\n",
      " 27%|███████████████████▉                                                     | 3/11 [00:01<00:03,  2.39it/s]\u001b[A\n",
      " 36%|██████████████████████████▌                                              | 4/11 [00:01<00:02,  2.44it/s]\u001b[A\n",
      " 45%|█████████████████████████████████▏                                       | 5/11 [00:02<00:02,  2.37it/s]\u001b[A\n",
      " 55%|███████████████████████████████████████▊                                 | 6/11 [00:02<00:02,  2.49it/s]\u001b[A\n",
      " 64%|██████████████████████████████████████████████▍                          | 7/11 [00:02<00:01,  2.71it/s]\u001b[A\n",
      " 73%|█████████████████████████████████████████████████████                    | 8/11 [00:03<00:01,  2.76it/s]\u001b[A\n",
      " 82%|███████████████████████████████████████████████████████████▋             | 9/11 [00:03<00:00,  2.85it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████▍      | 10/11 [00:03<00:00,  2.70it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 11/11 [00:04<00:00,  2.60it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.97s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.9614441  0.91740984 0.80754805 0.927141  ]\n",
      " [0.96298325 0.91512823 0.79483646 0.92092985]]\n",
      "Best candidate: 1 with score 3.6135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  80%|███████████████████████████████████████████████████▏            | 8/10 [11:23<03:15, 97.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.4185, Weighted Loss: 0.5242\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.17s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.57s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.51s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.21s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.9212868  0.9364356  0.84396064 0.6389437 ]\n",
      " [0.96445614 0.80126816 0.40919822 0.65483016]]\n",
      "Best candidate: 1 with score 3.3406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  90%|█████████████████████████████████████████████████████████▌      | 9/10 [11:41<01:12, 72.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.7187, Weighted Loss: 0.6263\n",
      "Candidate 1: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Candidate 2: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi...\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.01s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.03s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.60s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                  | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.94s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniEval scores: [[0.43471366 0.44009072 0.6994359  0.631724  ]\n",
      " [0.80084926 0.7911317  0.7820975  0.76566577]]\n",
      "Best candidate: 2 with score 3.1397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████| 10/10 [11:56<00:00, 71.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.4237, Weighted Loss: 0.8270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete. Average score: 2.9640\n",
      "\n",
      "🎉 Reward-based training complete!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 250\u001b[39m\n\u001b[32m    245\u001b[39m metrics = train_with_rewards()\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# ────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# 5) Visualize training progress\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# ────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m    252\u001b[39m metrics_df = pd.DataFrame(metrics)\n\u001b[32m    254\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# ════════════════════════════════════════════════════════════════\n",
    "# Custom Reward-Based Training (Without TRL)\n",
    "# ════════════════════════════════════════════════════════════════\n",
    "import os, gc, torch, pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Disable CUDA to avoid device conflicts\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "DEVICE = \"cpu\"\n",
    "print(f\"Using CPU for all operations\")\n",
    "\n",
    "# Create save directory\n",
    "SAVE_DIR = \"reward_checkpoints\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1) UniEval scorer\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "sum_eval = get_evaluator(\"summarization\", device=\"cpu\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_unieval_scores(src, hyp, ref):\n",
    "    \"\"\"Get UniEval scores for generated summaries\"\"\"\n",
    "    data = convert_to_json(\n",
    "        output_list=hyp,\n",
    "        src_list=src,\n",
    "        ref_list=ref,\n",
    "    )\n",
    "    raw = sum_eval.evaluate(data)\n",
    "    scores = [\n",
    "        [d[\"coherence\"], d[\"consistency\"], d[\"fluency\"], d[\"relevance\"]]\n",
    "        for d in raw\n",
    "    ]\n",
    "    return torch.tensor(scores, dtype=torch.float32)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2) Load GPT-2 Model and tokenizer\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "BASE_MODEL = \"gpt2\"\n",
    "base = GPT2LMHeadModel.from_pretrained(BASE_MODEL)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# Set padding\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    base.config.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Add LoRA for efficient fine-tuning\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3) Dataset with sampling\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\")[[\"dialogue\", \"note\"]]\n",
    "\n",
    "class ClinicalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dialogue = str(self.df.iloc[idx][\"dialogue\"])\n",
    "        reference = str(self.df.iloc[idx][\"note\"])\n",
    "        prompt = f\"Summarize the following conversation:\\n\\n{dialogue}\"\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompt, \n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs.input_ids.squeeze(),\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(),\n",
    "            \"prompt\": prompt,\n",
    "            \"reference\": reference,\n",
    "            \"dialogue\": dialogue\n",
    "        }\n",
    "\n",
    "# Create a small dataset to demonstrate the concept\n",
    "dataset = ClinicalDataset(df.sample(10, random_state=42), tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4) Custom reward-weighted training loop\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "NUM_EPOCHS = 3\n",
    "NUM_CANDIDATES = 2\n",
    "LEARNING_RATE = 5e-5  # Higher learning rate for more impact\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# We'll keep track of performance\n",
    "all_metrics = []\n",
    "\n",
    "def train_with_rewards():\n",
    "    print(\"Starting reward-based training (no TRL)\")\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\n{'='*30}\\nEpoch {epoch+1}/{NUM_EPOCHS}\\n{'='*30}\")\n",
    "        epoch_metrics = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(loader, desc=f\"Epoch {epoch+1}\")):\n",
    "            try:\n",
    "                # Each batch contains a single example\n",
    "                input_ids = batch[\"input_ids\"]\n",
    "                attention_mask = batch[\"attention_mask\"]\n",
    "                dialogue = batch[\"dialogue\"][0]  # String\n",
    "                reference = batch[\"reference\"][0]  # String\n",
    "                \n",
    "                # 1. Generate multiple candidates\n",
    "                candidates = []\n",
    "                candidate_texts = []\n",
    "                \n",
    "                # Generate candidates\n",
    "                for _ in range(NUM_CANDIDATES):\n",
    "                    # Forward pass with model to get generation loss\n",
    "                    output = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=input_ids  # Using input as target for causal LM\n",
    "                    )\n",
    "                    \n",
    "                    # Generate text\n",
    "                    with torch.no_grad():\n",
    "                        generated = model.generate(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            do_sample=True,\n",
    "                            temperature=0.7,\n",
    "                            max_new_tokens=64,\n",
    "                            num_return_sequences=1,\n",
    "                            pad_token_id=tokenizer.eos_token_id\n",
    "                        )\n",
    "                    \n",
    "                    # Save generation and decode\n",
    "                    candidates.append(generated)\n",
    "                    text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                    candidate_texts.append(text)\n",
    "                    print(f\"Candidate {len(candidate_texts)}: {text[:50]}...\")\n",
    "                \n",
    "                # 2. Score candidates with UniEval\n",
    "                scores = get_unieval_scores(\n",
    "                    src=[dialogue] * NUM_CANDIDATES,\n",
    "                    hyp=candidate_texts,\n",
    "                    ref=[reference] * NUM_CANDIDATES\n",
    "                ).numpy()\n",
    "                \n",
    "                print(f\"UniEval scores: {scores}\")\n",
    "                \n",
    "                # 3. Identify the best candidate\n",
    "                total_scores = scores.sum(axis=1)\n",
    "                best_idx = np.argmax(total_scores)\n",
    "                best_score = total_scores[best_idx]\n",
    "                \n",
    "                print(f\"Best candidate: {best_idx+1} with score {best_score:.4f}\")\n",
    "                \n",
    "                # 4. Retrain model to make best candidate more likely\n",
    "                # Get the model output for the best candidate\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Run model on best candidate\n",
    "                best_candidate = candidates[best_idx]\n",
    "                \n",
    "                # Compute language modeling loss for the best candidate\n",
    "                outputs = model(\n",
    "                    input_ids=best_candidate,\n",
    "                    labels=best_candidate\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Weight the loss by the score - better generations = stronger signal\n",
    "                weighted_loss = loss * (1.0 / (best_score + 1.0))  # +1 to prevent division by zero\n",
    "                \n",
    "                # Backpropagate and update\n",
    "                weighted_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Log metrics\n",
    "                metric_entry = {\n",
    "                    \"batch\": batch_idx,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"best_score\": best_score.item(),\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"weighted_loss\": weighted_loss.item()\n",
    "                }\n",
    "                epoch_metrics.append(metric_entry)\n",
    "                \n",
    "                print(f\"Loss: {loss.item():.4f}, Weighted Loss: {weighted_loss.item():.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {batch_idx}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "                \n",
    "            # For demonstration, process a few batches\n",
    "            if batch_idx >= 3 and epoch == 0:\n",
    "                print(\"Processed 4 batches in first epoch for demonstration\")\n",
    "                break\n",
    "                \n",
    "        # Save model after each epoch\n",
    "        save_path = os.path.join(SAVE_DIR, f\"epoch_{epoch+1}\")\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        model.save_pretrained(save_path)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "        # Save metrics\n",
    "        all_metrics.extend(epoch_metrics)\n",
    "        metrics_df = pd.DataFrame(all_metrics)\n",
    "        metrics_df.to_csv(os.path.join(SAVE_DIR, \"training_metrics.csv\"), index=False)\n",
    "        \n",
    "        epoch_avg_score = np.mean([m[\"best_score\"] for m in epoch_metrics])\n",
    "        print(f\"Epoch {epoch+1} complete. Average score: {epoch_avg_score:.4f}\")\n",
    "    \n",
    "    print(\"\\n🎉 Reward-based training complete!\")\n",
    "    return all_metrics\n",
    "\n",
    "# Run the training\n",
    "metrics = train_with_rewards()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5) Visualize training progress\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(metrics_df[\"batch\"], metrics_df[\"best_score\"], marker='o')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Best Candidate Score')\n",
    "plt.title('Training Progress: Best Candidate Score Over Time')\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"training_progress.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(metrics_df[\"batch\"], metrics_df[\"loss\"], label='Original Loss')\n",
    "plt.plot(metrics_df[\"batch\"], metrics_df[\"weighted_loss\"], label='Weighted Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')l\n",
    "plt.title('Training Losses Over Time')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"training_losses.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda for inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA weights from reward_checkpoints/epoch_3\n",
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                  | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 20%|██████████████▊                                                           | 1/5 [00:02<00:11,  2.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 40%|█████████████████████████████▌                                            | 2/5 [00:05<00:08,  2.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 60%|████████████████████████████████████████████▍                             | 3/5 [00:08<00:05,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 80%|███████████████████████████████████████████████████████████▏              | 4/5 [00:11<00:02,  2.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.71s/it]\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated 20 summaries.\n",
      "Creating evaluation data...\n",
      "Running UniEval...\n",
      "Evaluating coherence of 20 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 3/3 [00:34<00:00, 11.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 20 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 43/43 [10:06<00:00, 14.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 20 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 43/43 [01:09<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 20 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 3/3 [00:35<00:00, 11.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation scores are shown below:\n",
      "+-------------+----------+\n",
      "|  Dimensions |  Score   |\n",
      "+-------------+----------+\n",
      "|  coherence  | 0.776413 |\n",
      "| consistency | 0.710692 |\n",
      "|   fluency   | 0.680258 |\n",
      "|  relevance  | 0.648008 |\n",
      "|   overall   | 0.703843 |\n",
      "+-------------+----------+\n",
      "\n",
      "=== Summary Evaluation Results ===\n",
      "Average Coherence: 0.7764\n",
      "Average Consistency: 0.7107\n",
      "Average Fluency: 0.6803\n",
      "Average Relevance: 0.6480\n",
      "Overall Average: 0.7038\n",
      "\n",
      "=== Sample Output Comparison ===\n",
      "\n",
      "Example 1:\n",
      "Dialogue: [doctor] hi bruce , how are you ?\n",
      "[patient] hey , good to see you .\n",
      "[doctor] good to see you as well...\n",
      "Reference: CHIEF COMPLAINT\n",
      "\n",
      "Follow up of chronic problems.\n",
      "\n",
      "HISTORY OF PRESENT ILLNESS\n",
      "\n",
      "Bruce Howard is a 60-ye...\n",
      "Generated: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi bruce, how are you?\n",
      "[patient] hey, good to see yo...\n",
      "Scores: Coherence=0.95, Consistency=0.90, Fluency=0.71, Relevance=0.91\n",
      "\n",
      "Example 2:\n",
      "Dialogue: [doctor] okay michael so i see in here that you're here because you're experiencing some symptoms th...\n",
      "Reference: CHIEF COMPLAINT\n",
      "\n",
      "Back pain.\n",
      "\n",
      "MEDICAL HISTORY\n",
      "\n",
      "Patient reports that he has a frequent history of stre...\n",
      "Generated: Summarize the following conversation:\n",
      "\n",
      "[doctor] okay michael so i see in here that you're here becau...\n",
      "Scores: Coherence=0.76, Consistency=0.84, Fluency=0.82, Relevance=0.78\n",
      "\n",
      "Example 3:\n",
      "Dialogue: [doctor] hi frank how are you i heard the medical assistant told me that you're having some shortnes...\n",
      "Reference: CHIEF COMPLAINT\n",
      "\n",
      "Shortness of breath.\n",
      "\n",
      "MEDICAL HISTORY\n",
      "\n",
      "Patient reports a history of coronary diseas...\n",
      "Generated: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi frank how are you i heard the medical assistant t...\n",
      "Scores: Coherence=0.92, Consistency=0.61, Fluency=0.87, Relevance=0.68\n",
      "\n",
      "Results saved to ppo_evaluation_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ════════════════════════════════════════════════════════════════\n",
    "# Testing Script for PPO-Trained LoRA Weights\n",
    "# ════════════════════════════════════════════════════════════════\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Check if CUDA is available for inference\n",
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "DEVICE = \"cuda\" if CUDA_AVAILABLE else \"cpu\"\n",
    "print(f\"Using device: {DEVICE} for inference\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1) Load the trained model weights\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Path to the saved LoRA weights\n",
    "LORA_PATH = \"reward_checkpoints/epoch_3\"  # Updated to use epoch 3 weights\n",
    "\n",
    "# First load the base model\n",
    "base_model = \"gpt2\"\n",
    "base = GPT2LMHeadModel.from_pretrained(base_model)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(LORA_PATH)  # Load from checkpoint\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    base.config.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"  # Important for decoder-only models\n",
    "\n",
    "# Load the LoRA weights\n",
    "print(f\"Loading LoRA weights from {LORA_PATH}\")\n",
    "model = PeftModel.from_pretrained(base, LORA_PATH)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2) Prepare test dataset\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Load test data\n",
    "df = pd.read_csv(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\")\n",
    "test_df = df.sample(20, random_state=42)  # Using 20 samples for testing\n",
    "\n",
    "# Prepare test dialogues and references\n",
    "dialogues = test_df[\"dialogue\"].tolist()\n",
    "references = test_df[\"note\"].tolist()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3) Generate summaries using trained model\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "print(\"Generating summaries...\")\n",
    "batch_size = 4\n",
    "num_samples = len(dialogues)\n",
    "num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    start, end = i*batch_size, min((i+1)*batch_size, num_samples)\n",
    "    convs = dialogues[start:end]\n",
    "    prompts = [\n",
    "        f\"Summarize the following conversation:\\n\\n{c}\"\n",
    "        for c in convs if len(str(c).strip()) > 10\n",
    "    ]\n",
    "    if not prompts:\n",
    "        continue\n",
    "    \n",
    "    # Tokenize\n",
    "    enc = tokenizer(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            input_ids=enc.input_ids,\n",
    "            attention_mask=enc.attention_mask,\n",
    "            max_new_tokens=120,\n",
    "            do_sample=False,\n",
    "            num_beams=3,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    dec = tokenizer.batch_decode(out_ids, skip_special_tokens=True)\n",
    "    predictions.extend(dec)\n",
    "\n",
    "print(f\"✅ Generated {len(predictions)} summaries.\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4) Evaluate with UniEval\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "# Keep UniEval on CPU (more stable)\n",
    "sum_eval = get_evaluator(\"summarization\", device=\"cpu\")\n",
    "\n",
    "# Trim predictions and references to the same length\n",
    "min_len = min(len(predictions), len(references), len(dialogues))\n",
    "predictions = predictions[:min_len]\n",
    "references = references[:min_len]\n",
    "dialogues = dialogues[:min_len]\n",
    "\n",
    "# Create JSON data for UniEval\n",
    "print(\"Creating evaluation data...\")\n",
    "data = convert_to_json(\n",
    "    src_list=[str(d) for d in dialogues],\n",
    "    ref_list=[str(r) for r in references],\n",
    "    output_list=[str(p) for p in predictions]\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running UniEval...\")\n",
    "scores = sum_eval.evaluate(data, print_result=True)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5) Display results\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Extract scores for each metric\n",
    "coherence_scores = [item[\"coherence\"] for item in scores]\n",
    "consistency_scores = [item[\"consistency\"] for item in scores]\n",
    "fluency_scores = [item[\"fluency\"] for item in scores]\n",
    "relevance_scores = [item[\"relevance\"] for item in scores]\n",
    "\n",
    "# Calculate averages\n",
    "avg_coherence = sum(coherence_scores) / len(coherence_scores)\n",
    "avg_consistency = sum(consistency_scores) / len(consistency_scores)\n",
    "avg_fluency = sum(fluency_scores) / len(fluency_scores)\n",
    "avg_relevance = sum(relevance_scores) / len(relevance_scores)\n",
    "\n",
    "print(\"\\n=== Summary Evaluation Results ===\")\n",
    "print(f\"Average Coherence: {avg_coherence:.4f}\")\n",
    "print(f\"Average Consistency: {avg_consistency:.4f}\")\n",
    "print(f\"Average Fluency: {avg_fluency:.4f}\")\n",
    "print(f\"Average Relevance: {avg_relevance:.4f}\")\n",
    "print(f\"Overall Average: {(avg_coherence + avg_consistency + avg_fluency + avg_relevance) / 4:.4f}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 6) Sample output comparison\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "print(\"\\n=== Sample Output Comparison ===\")\n",
    "for i in range(min(3, min_len)):  # Show up to 3 examples\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Dialogue: {dialogues[i][:100]}...\")\n",
    "    print(f\"Reference: {references[i][:100]}...\")\n",
    "    print(f\"Generated: {predictions[i][:100]}...\")\n",
    "    print(f\"Scores: Coherence={coherence_scores[i]:.2f}, Consistency={consistency_scores[i]:.2f}, \"\n",
    "          f\"Fluency={fluency_scores[i]:.2f}, Relevance={relevance_scores[i]:.2f}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 7) Save results\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Create a results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'dialogue': dialogues[:min_len],\n",
    "    'reference': references[:min_len],\n",
    "    'prediction': predictions[:min_len],\n",
    "    'coherence': coherence_scores,\n",
    "    'consistency': consistency_scores,\n",
    "    'fluency': fluency_scores,\n",
    "    'relevance': relevance_scores\n",
    "})\n",
    "\n",
    "# Save to CSVl\n",
    "results_file = \"ppo_evaluation_results.csv\"\n",
    "results_df.to_csv(results_file, index=False)\n",
    "print(f\"\\nResults saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu for inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded baseline GPT-2 model (without LoRA weights)\n",
      "Generating summaries with baseline GPT-2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                  | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 20%|██████████████▊                                                           | 1/5 [00:14<00:56, 14.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 40%|█████████████████████████████▌                                            | 2/5 [00:27<00:41, 13.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 60%|████████████████████████████████████████████▍                             | 3/5 [00:41<00:27, 13.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 80%|███████████████████████████████████████████████████████████▏              | 4/5 [00:55<00:13, 13.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 5/5 [01:09<00:00, 13.81s/it]\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Generated 20 baseline summaries.\n",
      "Creating evaluation data...\n",
      "Running UniEval on baseline model outputs...\n",
      "Evaluating coherence of 20 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 3/3 [00:38<00:00, 12.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 20 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 45/45 [11:40<00:00, 15.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 20 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 45/45 [01:05<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 20 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 3/3 [00:37<00:00, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation scores are shown below:\n",
      "+-------------+----------+\n",
      "|  Dimensions |  Score   |\n",
      "+-------------+----------+\n",
      "|  coherence  | 0.777173 |\n",
      "| consistency | 0.623779 |\n",
      "|   fluency   | 0.672602 |\n",
      "|  relevance  | 0.653257 |\n",
      "|   overall   | 0.681703 |\n",
      "+-------------+----------+\n",
      "\n",
      "=== Baseline GPT-2 Summary Evaluation Results ===\n",
      "Average Coherence: 0.7772\n",
      "Average Consistency: 0.6238\n",
      "Average Fluency: 0.6726\n",
      "Average Relevance: 0.6533\n",
      "Overall Average: 0.6817\n",
      "\n",
      "=== Sample Baseline Outputs ===\n",
      "\n",
      "Example 1:\n",
      "Dialogue: [doctor] hi bruce , how are you ?\n",
      "[patient] hey , good to see you .\n",
      "[doctor] good to see you as well...\n",
      "Reference: CHIEF COMPLAINT\n",
      "\n",
      "Follow up of chronic problems.\n",
      "\n",
      "HISTORY OF PRESENT ILLNESS\n",
      "\n",
      "Bruce Howard is a 60-ye...\n",
      "Generated: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi bruce, how are you?\n",
      "[patient] hey, good to see yo...\n",
      "Scores: Coherence=0.97, Consistency=0.88, Fluency=0.71, Relevance=0.93\n",
      "\n",
      "Example 2:\n",
      "Dialogue: [doctor] okay michael so i see in here that you're here because you're experiencing some symptoms th...\n",
      "Reference: CHIEF COMPLAINT\n",
      "\n",
      "Back pain.\n",
      "\n",
      "MEDICAL HISTORY\n",
      "\n",
      "Patient reports that he has a frequent history of stre...\n",
      "Generated: Summarize the following conversation:\n",
      "\n",
      "[doctor] okay michael so i see in here that you're here becau...\n",
      "Scores: Coherence=0.78, Consistency=0.48, Fluency=0.86, Relevance=0.78\n",
      "\n",
      "Example 3:\n",
      "Dialogue: [doctor] hi frank how are you i heard the medical assistant told me that you're having some shortnes...\n",
      "Reference: CHIEF COMPLAINT\n",
      "\n",
      "Shortness of breath.\n",
      "\n",
      "MEDICAL HISTORY\n",
      "\n",
      "Patient reports a history of coronary diseas...\n",
      "Generated: Summarize the following conversation:\n",
      "\n",
      "[doctor] hi frank how are you i heard the medical assistant t...\n",
      "Scores: Coherence=0.90, Consistency=0.47, Fluency=0.58, Relevance=0.64\n",
      "\n",
      "Baseline results saved to baseline_gpt2_evaluation_results.csv\n",
      "\n",
      "=== Comparison: Baseline vs. LoRA PPO ===\n",
      "Metric       Baseline   LoRA PPO   Difference\n",
      "------------------------------------------\n",
      "Coherence    0.7772     0.7772     +0.0000\n",
      "Consistency  0.6238     0.6238     +0.0000\n",
      "Fluency      0.6726     0.6726     +0.0000\n",
      "Relevance    0.6533     0.6533     +0.0000\n",
      "------------------------------------------\n",
      "OVERALL      0.6817     0.6817     +0.0000\n",
      "\n",
      "Comparison results saved to comparison_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ════════════════════════════════════════════════════════════════\n",
    "# Baseline GPT-2 Testing Script (For Comparison)\n",
    "# ════════════════════════════════════════════════════════════════\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Check if CUDA is available for inference\n",
    "CUDA_AVAILABLE = torch.cuda.is_available()\n",
    "DEVICE = \"cuda\" if CUDA_AVAILABLE else \"cpu\"\n",
    "print(f\"Using device: {DEVICE} for inference\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1) Load the base GPT-2 model (without LoRA)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load base model directly\n",
    "base_model = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(base_model)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\"  # Important for decoder-only models\n",
    "\n",
    "# Move model to appropriate device\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Loaded baseline GPT-2 model (without LoRA weights)\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2) Prepare the same test dataset as before\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Load test data - ensure we use the same samples as before\n",
    "df = pd.read_csv(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\")\n",
    "test_df = df.sample(20, random_state=42)  # Using same random seed as LoRA test\n",
    "\n",
    "# Prepare test dialogues and references\n",
    "dialogues = test_df[\"dialogue\"].tolist()\n",
    "references = test_df[\"note\"].tolist()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3) Generate summaries using baseline model\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "print(\"Generating summaries with baseline GPT-2...\")\n",
    "batch_size = 4\n",
    "num_samples = len(dialogues)\n",
    "num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "baseline_predictions = []\n",
    "\n",
    "for i in tqdm(range(num_batches)):\n",
    "    start, end = i*batch_size, min((i+1)*batch_size, num_samples)\n",
    "    convs = dialogues[start:end]\n",
    "    prompts = [\n",
    "        f\"Summarize the following conversation:\\n\\n{c}\"\n",
    "        for c in convs if len(str(c).strip()) > 10\n",
    "    ]\n",
    "    if not prompts:\n",
    "        continue\n",
    "    \n",
    "    # Tokenize\n",
    "    enc = tokenizer(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Generate - using same parameters as LoRA model for fair comparison\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            input_ids=enc.input_ids,\n",
    "            attention_mask=enc.attention_mask,\n",
    "            max_new_tokens=120,\n",
    "            do_sample=False,\n",
    "            num_beams=3,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    dec = tokenizer.batch_decode(out_ids, skip_special_tokens=True)\n",
    "    baseline_predictions.extend(dec)\n",
    "\n",
    "print(f\"✅ Generated {len(baseline_predictions)} baseline summaries.\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4) Evaluate with UniEval\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "# Keep UniEval on CPU\n",
    "sum_eval = get_evaluator(\"summarization\", device=\"cpu\")\n",
    "\n",
    "# Trim predictions and references to the same length\n",
    "min_len = min(len(baseline_predictions), len(references), len(dialogues))\n",
    "baseline_predictions = baseline_predictions[:min_len]\n",
    "references = references[:min_len]\n",
    "dialogues = dialogues[:min_len]\n",
    "\n",
    "# Create JSON data for UniEval\n",
    "print(\"Creating evaluation data...\")\n",
    "data = convert_to_json(\n",
    "    src_list=[str(d) for d in dialogues],\n",
    "    ref_list=[str(r) for r in references],\n",
    "    output_list=[str(p) for p in baseline_predictions]\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running UniEval on baseline model outputs...\")\n",
    "baseline_scores = sum_eval.evaluate(data, print_result=True)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5) Display baseline results\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Extract scores for each metric\n",
    "baseline_coherence = [item[\"coherence\"] for item in baseline_scores]\n",
    "baseline_consistency = [item[\"consistency\"] for item in baseline_scores]\n",
    "baseline_fluency = [item[\"fluency\"] for item in baseline_scores]\n",
    "baseline_relevance = [item[\"relevance\"] for item in baseline_scores]\n",
    "\n",
    "# Calculate averages\n",
    "avg_coherence = sum(baseline_coherence) / len(baseline_coherence)\n",
    "avg_consistency = sum(baseline_consistency) / len(baseline_consistency)\n",
    "avg_fluency = sum(baseline_fluency) / len(baseline_fluency)\n",
    "avg_relevance = sum(baseline_relevance) / len(baseline_relevance)\n",
    "\n",
    "print(\"\\n=== Baseline GPT-2 Summary Evaluation Results ===\")\n",
    "print(f\"Average Coherence: {avg_coherence:.4f}\")\n",
    "print(f\"Average Consistency: {avg_consistency:.4f}\")\n",
    "print(f\"Average Fluency: {avg_fluency:.4f}\")\n",
    "print(f\"Average Relevance: {avg_relevance:.4f}\")\n",
    "print(f\"Overall Average: {(avg_coherence + avg_consistency + avg_fluency + avg_relevance) / 4:.4f}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 6) Sample output comparison\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "print(\"\\n=== Sample Baseline Outputs ===\")\n",
    "for i in range(min(3, min_len)):  # Show up to 3 examples\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Dialogue: {dialogues[i][:100]}...\")\n",
    "    print(f\"Reference: {references[i][:100]}...\")\n",
    "    print(f\"Generated: {baseline_predictions[i][:100]}...\")\n",
    "    print(f\"Scores: Coherence={baseline_coherence[i]:.2f}, Consistency={baseline_consistency[i]:.2f}, \"\n",
    "          f\"Fluency={baseline_fluency[i]:.2f}, Relevance={baseline_relevance[i]:.2f}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 7) Save results\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# Create a results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'dialogue': dialogues[:min_len],\n",
    "    'reference': references[:min_len],\n",
    "    'baseline_prediction': baseline_predictions[:min_len],\n",
    "    'coherence': baseline_coherence,\n",
    "    'consistency': baseline_consistency,\n",
    "    'fluency': baseline_fluency,\n",
    "    'relevance': baseline_relevance\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "results_file = \"baseline_gpt2_evaluation_results.csv\"\n",
    "results_df.to_csv(results_file, index=False)\n",
    "print(f\"\\nBaseline results saved to {results_file}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 8) Try to load LoRA results for comparison (if available)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "try:\n",
    "    lora_results = pd.read_csv(\"ppo_evaluation_results.csv\")\n",
    "    \n",
    "    # Check if we have matching entries\n",
    "    if len(lora_results) == len(results_df):\n",
    "        print(\"\\n=== Comparison: Baseline vs. LoRA PPO ===\")\n",
    "        \n",
    "        # Calculate average score improvements\n",
    "        lora_coherence_avg = lora_results['coherence'].mean()\n",
    "        lora_consistency_avg = lora_results['consistency'].mean()\n",
    "        lora_fluency_avg = lora_results['fluency'].mean()\n",
    "        lora_relevance_avg = lora_results['relevance'].mean()\n",
    "        lora_overall = (lora_coherence_avg + lora_consistency_avg + lora_fluency_avg + lora_relevance_avg) / 4\n",
    "        \n",
    "        baseline_overall = (avg_coherence + avg_consistency + avg_fluency + avg_relevance) / 4\n",
    "        \n",
    "        # Print comparison\n",
    "        print(f\"{'Metric':<12} {'Baseline':<10} {'LoRA PPO':<10} {'Difference':<10}\")\n",
    "        print(f\"{'-'*42}\")\n",
    "        print(f\"{'Coherence':<12} {avg_coherence:.4f}     {lora_coherence_avg:.4f}     {lora_coherence_avg-avg_coherence:+.4f}\")\n",
    "        print(f\"{'Consistency':<12} {avg_consistency:.4f}     {lora_consistency_avg:.4f}     {lora_consistency_avg-avg_consistency:+.4f}\")\n",
    "        print(f\"{'Fluency':<12} {avg_fluency:.4f}     {lora_fluency_avg:.4f}     {lora_fluency_avg-avg_fluency:+.4f}\")\n",
    "        print(f\"{'Relevance':<12} {avg_relevance:.4f}     {lora_relevance_avg:.4f}     {lora_relevance_avg-avg_relevance:+.4f}\")\n",
    "        print(f\"{'-'*42}\")\n",
    "        print(f\"{'OVERALL':<12} {baseline_overall:.4f}     {lora_overall:.4f}     {lora_overall-baseline_overall:+.4f}\")\n",
    "        \n",
    "        # Create a combined CSV for easy comparison\n",
    "        combined_df = results_df.copy()\n",
    "        combined_df['lora_prediction'] = lora_results['prediction']\n",
    "        combined_df['lora_coherence'] = lora_results['coherence']\n",
    "        combined_df['lora_consistency'] = lora_results['consistency']\n",
    "        combined_df['lora_fluency'] = lora_results['fluency']\n",
    "        combined_df['lora_relevance'] = lora_results['relevance']\n",
    "        \n",
    "        # Calculate per-example improvement\n",
    "        combined_df['coherence_diff'] = combined_df['lora_coherence'] - combined_df['coherence']\n",
    "        combined_df['consistency_diff'] = combined_df['lora_consistency'] - combined_df['consistency']\n",
    "        combined_df['fluency_diff'] = combined_df['lora_fluency'] - combined_df['fluency']\n",
    "        combined_df['relevance_diff'] = combined_df['lora_relevance'] - combined_df['relevance']\n",
    "        \n",
    "        # Save combined results\n",
    "        combined_file = \"comparison_results.csv\"\n",
    "        combined_df.to_csv(combined_file, index=False)\n",
    "        print(f\"\\nComparison results saved to {combined_file}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"\\nNo LoRA results file found for comparison. Run the LoRA testing script first.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError comparing results: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"D:\\hf-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1) UniEval multi‑dim evaluator (CPU only, load once)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")  # if needed to make sure your Python can import from the UniEval folder\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "import torch\n",
    "\n",
    "sum_eval = get_evaluator(\"summarization\", device=\"cpu\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def unieval_4way(src, hyp, ref):\n",
    "    \"\"\"\n",
    "    src, hyp, ref: lists of strings, length B\n",
    "    returns: Tensor (B,4) with [coherence, consistency, fluency, relevance]\n",
    "    \"\"\"\n",
    "    data = convert_to_json(\n",
    "        output_list=hyp,\n",
    "        src_list=src,\n",
    "        ref_list=ref,\n",
    "    )\n",
    "    raw = sum_eval.evaluate(data, print_result=True)\n",
    "    scores = [\n",
    "        [d[\"coherence\"], d[\"consistency\"], d[\"fluency\"], d[\"relevance\"]]\n",
    "        for d in raw\n",
    "    ]\n",
    "    return torch.tensor(scores, dtype=torch.float32)  # CPU (B,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WaCjWv2VBLA"
   },
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════\n",
    "# Requirements:\n",
    "#   pip install trl==0.7.4 transformers==4.38.2 peft==0.10.0 \\\n",
    "#               accelerate==0.28.0 bitsandbytes datasets evaluate pandas\n",
    "# ════════════════════════════════════════════════════════════════\n",
    "import os, gc, torch, pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2) Load your SFT‑finetuned BART in 4‑bit + LoRA\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForSeq2SeqLMWithValueHead\n",
    "\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = \"cpu\"\n",
    "SFT_DIR = r\"D:\\kshitij-weights-folder\\bart_clinical_ft-20250422T192130Z-001\\bart_clinical_ft\"\n",
    "\n",
    "# 2a) Quantize & prepare\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    SFT_DIR,\n",
    "    quantization_config=bnb,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base = prepare_model_for_kbit_training(base)\n",
    "base.gradient_checkpointing_enable()\n",
    "base.config.use_cache = False\n",
    "\n",
    "# 2b) Attach fresh LoRA\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg).to(DEVICE)\n",
    "\n",
    "# 2c) Tokenizer (decoder‑only → left‑pad)\n",
    "tok = AutoTokenizer.from_pretrained(SFT_DIR, use_fast=False)\n",
    "tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"right\"\n",
    "model.config.pad_token_id = tok.eos_token_id\n",
    "model.resize_token_embeddings(len(tok))\n",
    "\n",
    "# 2d) Wrap for PPO\n",
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\n",
    "    model, peft_config=lora_cfg\n",
    ").to(DEVICE)\n",
    "ppo_ref_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\n",
    "    model, peft_config=lora_cfg\n",
    ").to(DEVICE).eval()\n",
    "for p in ppo_ref_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3) Prepare your DataLoader (with references)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\")[[\"dialogue\", \"note\"]]\n",
    "\n",
    "class ClinDS(Dataset):\n",
    "    def __init__(self, df, tok, max_len=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tok = tok\n",
    "        self.L = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        conv = str(self.df.iloc[i][\"dialogue\"])\n",
    "        ref = str(self.df.iloc[i][\"note\"])\n",
    "        prompt = f\"Summarize the following conversation:\\n\\n{conv}\"\n",
    "        enc = self.tok(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.L,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc.input_ids.squeeze(),\n",
    "            \"attention_mask\": enc.attention_mask.squeeze(),\n",
    "            \"src_txt\": prompt,\n",
    "            \"ref_txt\": ref,\n",
    "        }\n",
    "\n",
    "loader = DataLoader(\n",
    "    ClinDS(df.sample(200, random_state=0), tok),\n",
    "    batch_size=1, shuffle=True, pin_memory=True, drop_last=True\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4) Build PPOTrainer + optimizer\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "ppo_cfg = PPOConfig(\n",
    "    batch_size=1,\n",
    "    mini_batch_size=1,\n",
    "    log_with=\"tensorboard\",\n",
    "    project_kwargs={\"logging_dir\": \"./logs\"},\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, ppo_model.parameters()),\n",
    "    lr=2e-5\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_cfg,\n",
    "    model=ppo_model,\n",
    "    ref_model=ppo_ref_model,\n",
    "    tokenizer=tok,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5) Training loop with candidate generation and dominance rewards\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": 64,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tok.eos_token_id,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "for epoch in range(1):\n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        # Prepare inputs\n",
    "        ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attn_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        src_txt = batch[\"src_txt\"]  # list[str]\n",
    "        ref_txt = batch[\"ref_txt\"]  # list[str]\n",
    "\n",
    "        # Generate multiple candidates per prompt\n",
    "        NUM_CANDIDATES = 1\n",
    "        all_outs = []\n",
    "        for _ in range(NUM_CANDIDATES):\n",
    "            with torch.no_grad():\n",
    "                out = ppo_model.generate(\n",
    "                    input_ids=ids,\n",
    "                    attention_mask=attn_mask,\n",
    "                    **gen_kwargs\n",
    "                )\n",
    "            all_outs.append(out)\n",
    "\n",
    "        # Stack outputs (B, K, L)\n",
    "        outs = torch.stack(all_outs, dim=1)\n",
    "\n",
    "        # Decode all candidates\n",
    "        hyps = [\n",
    "            [tok.decode(outs[b, k], skip_special_tokens=True)\n",
    "            for k in range(NUM_CANDIDATES)]\n",
    "            for b in range(outs.size(0))\n",
    "        ]\n",
    "\n",
    "        # Compute rewards using UniEval and dominance scoring\n",
    "        rewards = []\n",
    "        for b in range(len(src_txt)):\n",
    "            # Get scores for all candidates (K, 4)\n",
    "            scores = unieval_4way(\n",
    "                [src_txt[b]] * NUM_CANDIDATES,\n",
    "                hyps[b],\n",
    "                [ref_txt[b]] * NUM_CANDIDATES\n",
    "            ).numpy()\n",
    "\n",
    "            # Compute dominance counts\n",
    "            dom_counts = np.zeros(NUM_CANDIDATES)\n",
    "            for i in range(NUM_CANDIDATES):\n",
    "                for j in range(NUM_CANDIDATES):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    # Check if i dominates j\n",
    "                    if np.all(scores[i] >= scores[j]) and np.any(scores[i] > scores[j]):\n",
    "                        dom_counts[i] += 1\n",
    "\n",
    "            # Normalize to [-1, 1]\n",
    "            max_dom = NUM_CANDIDATES - 1\n",
    "            scalar_rewards = 2 * (dom_counts / max_dom) - 1\n",
    "            rewards.append(scalar_rewards)\n",
    "\n",
    "        # Flatten for PPO\n",
    "        flat_queries = []\n",
    "        flat_responses = []\n",
    "        flat_rewards = []\n",
    "\n",
    "        for b in range(len(src_txt)):\n",
    "            for k in range(NUM_CANDIDATES):\n",
    "                flat_queries.append(ids[b])\n",
    "                flat_responses.append(outs[b, k])\n",
    "                flat_rewards.append(torch.tensor([rewards[b][k]], device=DEVICE))\n",
    "\n",
    "        # PPO step\n",
    "        stats = ppo_trainer.step(\n",
    "            queries=flat_queries,\n",
    "            responses=flat_responses,\n",
    "            scores=flat_rewards\n",
    "        )\n",
    "\n",
    "        # Logging\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}\")\n",
    "            print(f\"Sample output: {hyps[0][0][:100]}...\")\n",
    "            print(f\"Average reward: {np.mean([r.item() for r in flat_rewards]):.4f}\")\n",
    "\n",
    "    print(f\"✅ Epoch {epoch+1}/3 complete\")\n",
    "\n",
    "print(\"🎉 PPO fine-tuning done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnJrWir2VBIq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"D:\\hf-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdXo7nrAVBGJ"
   },
   "outputs": [],
   "source": [
    "# # ════════════════════════════════════════════════════════════════\n",
    "# # 0) Force CPU Execution\n",
    "# # ════════════════════════════════════════════════════════════════\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # For CPU BLAS\n",
    "# import torch\n",
    "# torch.set_default_tensor_type(torch.FloatTensor)\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# import gc, pandas as pd\n",
    "# import numpy as np\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# # ════════════════════════════════════════════════════════════════\n",
    "# # 1) UniEval Setup (CPU only)\n",
    "# # ════════════════════════════════════════════════════════════════\n",
    "# import sys\n",
    "# sys.path.append(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")\n",
    "# from utils import convert_to_json\n",
    "# from metric.evaluator import get_evaluator\n",
    "\n",
    "# sum_eval = get_evaluator(\"summarization\", device=\"cpu\")\n",
    "\n",
    "# @torch.inference_mode()\n",
    "# def unieval_4way(src, hyp, ref):\n",
    "#     data = convert_to_json(output_list=hyp, src_list=src, ref_list=ref)\n",
    "#     raw = sum_eval.evaluate(data)\n",
    "#     return torch.tensor([\n",
    "#         [d[\"coherence\"], d[\"consistency\"], d[\"fluency\"], d[\"relevance\"]]\n",
    "#         for d in raw\n",
    "#     ], dtype=torch.float32)\n",
    "\n",
    "# # ════════════════════════════════════════════════════════════════\n",
    "# # 2) Causal LM Setup (BART as decoder-only)\n",
    "# # ════════════════════════════════════════════════════════════════\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "# from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "# DEVICE = \"cpu\"\n",
    "# SFT_DIR = r\"D:\\kshitij-weights-folder\\bart_clinical_ft-20250422T192130Z-001\\bart_clinical_ft\"\n",
    "\n",
    "# # 2a) Load model without quantization\n",
    "# base = AutoModelForCausalLM.from_pretrained(\n",
    "#     SFT_DIR,\n",
    "#     trust_remote_code=True\n",
    "# ).float().cpu()\n",
    "\n",
    "# # 2b) Causal LM LoRA config\n",
    "# lora_cfg = LoraConfig(\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     r=4,  # Reduced for CPU\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],  # Simplified\n",
    "#     lora_dropout=0.05\n",
    "# )\n",
    "# model = get_peft_model(base, lora_cfg).cpu()\n",
    "\n",
    "# # 2c) Tokenizer config\n",
    "# tok = AutoTokenizer.from_pretrained(SFT_DIR)\n",
    "# tok.pad_token = tok.eos_token\n",
    "# tok.padding_side = \"left\"  # Causal LM standard\n",
    "\n",
    "# # 2d) PPO models\n",
    "# ppo_model = AutoModelForCausalLMWithValueHead(model).cpu()\n",
    "# ppo_ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(SFT_DIR).cpu().eval()\n",
    "# for p in ppo_ref_model.parameters():\n",
    "#     p.requires_grad = False\n",
    "\n",
    "# # ════════════════════════════════════════════════════════════════\n",
    "# # 3) Data Loading with Dominance Prep\n",
    "# # ════════════════════════════════════════════════════════════════\n",
    "# # class ClinDS(Dataset):\n",
    "# #     def __init__(self, df, tok, max_len=256):  # Reduced length\n",
    "# #         self.df = df.reset_index(drop=True)\n",
    "# #         self.tok = tok\n",
    "# #         self.L = max_len\n",
    "\n",
    "# #     def __getitem__(self, i):\n",
    "# #         conv = str(self.df.iloc[i][\"dialogue\"])\n",
    "# #         ref = str(self.df.iloc[i][\"note\"])\n",
    "# #         prompt = f\"Summarize:\\n\\n{conv}\\n\\nSummary:\"\n",
    "# #         enc = self.tok(\n",
    "# #             prompt,\n",
    "# #             truncation=True,\n",
    "# #             padding=\"max_length\",\n",
    "# #             max_length=self.L,\n",
    "# #             return_tensors=\"pt\",\n",
    "# #         )\n",
    "# #         return {\n",
    "# #             \"input_ids\": enc.input_ids.squeeze(),\n",
    "# #             \"attention_mask\": enc.attention_mask.squeeze(),\n",
    "# #             \"src_txt\": conv,\n",
    "# #             \"ref_txt\": ref,\n",
    "# #         }\n",
    "\n",
    "# # df = pd.read_csv(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\")\n",
    "# # loader = DataLoader(\n",
    "# #     ClinDS(df.sample(50, random_state=0), tok),  # Smaller sample\n",
    "# #     batch_size=1,\n",
    "# #     shuffle=True,\n",
    "# #     pin_memory=False\n",
    "# # )\n",
    "# df = pd.read_csv(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\")[[\"dialogue\", \"note\"]]\n",
    "\n",
    "# class ClinDS(Dataset):\n",
    "#     def __init__(self, df, tok, max_len=512):\n",
    "#         self.df = df.reset_index(drop=True)\n",
    "#         self.tok = tok\n",
    "#         self.L = max_len\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         conv = str(self.df.iloc[i][\"dialogue\"])\n",
    "#         ref = str(self.df.iloc[i][\"note\"])\n",
    "#         prompt = f\"Summarize the following conversation:\\n\\n{conv}\"\n",
    "#         enc = self.tok(\n",
    "#             prompt,\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\",\n",
    "#             max_length=self.L,\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "#         return {\n",
    "#             \"input_ids\": enc.input_ids.squeeze(),\n",
    "#             \"attention_mask\": enc.attention_mask.squeeze(),\n",
    "#             \"src_txt\": prompt,\n",
    "#             \"ref_txt\": ref,\n",
    "#         }\n",
    "\n",
    "# loader = DataLoader(\n",
    "#     ClinDS(df.sample(200, random_state=0), tok),\n",
    "#     batch_size=1, shuffle=True, pin_memory=True, drop_last=True\n",
    "# )\n",
    "# # ════════════════════════════════════════════════════════════════\n",
    "# # 4) Training Loop with Dominance Scoring\n",
    "# # ════════════════════════════════════════════════════════════════\n",
    "# gen_kwargs = {\n",
    "#     \"max_new_tokens\": 48,\n",
    "#     \"do_sample\": True,\n",
    "#     \"temperature\": 0.7,\n",
    "#     \"top_p\": 0.9,\n",
    "#     \"pad_token_id\": tok.eos_token_id,\n",
    "#     # \"no_cuda\": True\n",
    "# }\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     for batch_idx, batch in enumerate(loader):\n",
    "#         # Explicit CPU handling\n",
    "#         ids = batch[\"input_ids\"].clone().detach().cpu()\n",
    "#         attn_mask = batch[\"attention_mask\"].clone().detach().cpu()\n",
    "        \n",
    "#         # Generate candidates\n",
    "#         NUM_CANDIDATES = 3  # Reduced for CPU\n",
    "#         all_outs = []\n",
    "#         for _ in range(NUM_CANDIDATES):\n",
    "#             with torch.no_grad():\n",
    "#                 out = ppo_model.generate(\n",
    "#                     input_ids=ids,\n",
    "#                     attention_mask=attn_mask,\n",
    "#                     **gen_kwargs\n",
    "#                 ).cpu()\n",
    "#             all_outs.append(out)\n",
    "        \n",
    "#         # Process outputs\n",
    "#         outs = torch.stack(all_outs, dim=1)\n",
    "#         hyps = [\n",
    "#             [tok.decode(outs[b, k], skip_special_tokens=True)\n",
    "#             for k in range(NUM_CANDIDATES)]\n",
    "#             for b in range(outs.size(0))\n",
    "#         ]\n",
    "        \n",
    "#         # Dominance scoring\n",
    "#         rewards = []\n",
    "#         for b in range(len(batch[\"src_txt\"])):\n",
    "#             scores = unieval_4way(\n",
    "#                 [batch[\"src_txt\"][b]] * NUM_CANDIDATES,\n",
    "#                 hyps[b],\n",
    "#                 [batch[\"ref_txt\"][b]] * NUM_CANDIDATES\n",
    "#             ).numpy()\n",
    "            \n",
    "#             # Pairwise comparison\n",
    "#             dom_matrix = np.zeros((NUM_CANDIDATES, NUM_CANDIDATES))\n",
    "#             for i in range(NUM_CANDIDATES):\n",
    "#                 for j in range(NUM_CANDIDATES):\n",
    "#                     if i == j: continue\n",
    "#                     dom_matrix[i,j] = np.all(scores[i] >= scores[j]) and np.any(scores[i] > scores[j])\n",
    "            \n",
    "#             # Calculate dominance scores\n",
    "#             dom_counts = dom_matrix.sum(axis=1)\n",
    "#             scalar_rewards = 2 * (dom_counts / (NUM_CANDIDATES-1)) - 1\n",
    "#             rewards.append(scalar_rewards)\n",
    "        \n",
    "#         # PPO step\n",
    "#         stats = ppo_trainer.step(\n",
    "#             queries=[ids]*NUM_CANDIDATES,\n",
    "#             responses=outs[0].unbind(),\n",
    "#             scores=[torch.tensor(r, dtype=torch.float32) for r in rewards]\n",
    "#         )\n",
    "\n",
    "#         if batch_idx % 2 == 0:\n",
    "#             print(f\"Batch {batch_idx} | Avg Reward: {np.mean(scalar_rewards):.2f}\")\n",
    "#             print(f\"Sample: {hyps[0][0][:60]}...\")\n",
    "\n",
    "# print(\"✅ Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbpqhsh9VBDS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "felD429xVBA0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5RVdHTvVA-j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1aTsFlHB4TO"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54xrB9vo3nxQ"
   },
   "outputs": [],
   "source": [
    "pip install transformers datasets torch trl pandas tqdm bitsandbytes #better we add these in requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 955
    },
    "id": "DLCW1kUT3B7t",
    "outputId": "ebc67941-1066-48d0-e023-d3ddbc7659ce"
   },
   "outputs": [],
   "source": [
    "# Colab-friendly two-stage fine-tuning & inference\n",
    "# ------------------------------------------------\n",
    "# 1) MedMCQA adaptation on BART-base\n",
    "# 2) Dialogue→structured-summary adaptation on clinical_notes.csv (column “note”)\n",
    "# 3) Batch inference with final model\n",
    "#\n",
    "# Requirements (install in Colab):\n",
    "#   !pip install transformers datasets torch bitsandbytes peft pandas tqdm\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    pipeline\n",
    ")\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "# ———————————————————————————————\n",
    "# Configuration\n",
    "# ———————————————————————————————\n",
    "DEVICE       = \"cpu\"\n",
    "BASE_MODEL   = \"facebook/bart-base\"\n",
    "MED_FT_DIR   = \"/content/bart_medmcqa_ft\"\n",
    "FINAL_FT_DIR = \"/content/bart_clinical_ft\"\n",
    "CSV_PATH     = \"/content/clinical_notes.csv\"  # must have columns: dialogue, note\n",
    "\n",
    "# ———————————————————————————————\n",
    "# 1) Datasets\n",
    "# ———————————————————————————————\n",
    "class MedMCQADataset(TorchDataset):\n",
    "    def __init__(self, hf_ds, tok, max_src=256, max_tgt=16):\n",
    "        self.tok, self.max_src, self.max_tgt = tok, max_src, max_tgt\n",
    "        self.examples = []\n",
    "        for row in hf_ds:\n",
    "            q = str(row.get(\"question\",\"\"))\n",
    "            opts = [str(row.get(k,\"\")) for k in (\"opa\",\"opb\",\"opc\",\"opd\")]\n",
    "            ans = str(row.get(\"cop\",\"\"))\n",
    "            prompt = f\"Question: {q} Options: A){opts[0]} B){opts[1]} C){opts[2]} D){opts[3]}\"\n",
    "            self.examples.append((prompt, ans))\n",
    "    def __len__(self): return len(self.examples)\n",
    "    def __getitem__(self,i):\n",
    "        prompt, ans = self.examples[i]\n",
    "        src = self.tok(prompt, truncation=True, padding=\"max_length\",\n",
    "                       max_length=self.max_src, return_tensors=\"pt\")\n",
    "        tgt = self.tok(ans,    truncation=True, padding=\"max_length\",\n",
    "                       max_length=self.max_tgt, return_tensors=\"pt\")\n",
    "        labels = tgt.input_ids.clone()\n",
    "        labels[labels==self.tok.pad_token_id] = -100\n",
    "        return {\n",
    "            \"input_ids\":      src.input_ids.squeeze(),\n",
    "            \"attention_mask\": src.attention_mask.squeeze(),\n",
    "            \"labels\":         labels.squeeze(),\n",
    "        }\n",
    "\n",
    "class DialogueSummaryDataset(TorchDataset):\n",
    "    def __init__(self, hf_ds, tok, max_src=512, max_tgt=256):\n",
    "        self.ds, self.tok = hf_ds, tok\n",
    "        self.max_src, self.max_tgt = max_src, max_tgt\n",
    "    def __len__(self): return len(self.ds)\n",
    "    def __getitem__(self,i):\n",
    "        row = self.ds[i]\n",
    "        src_txt = str(row[\"dialogue\"])\n",
    "        tgt_txt = str(row[\"note\"])  # use \"note\" column\n",
    "        src = self.tok(src_txt, truncation=True, padding=\"max_length\",\n",
    "                       max_length=self.max_src, return_tensors=\"pt\")\n",
    "        tgt = self.tok(tgt_txt, truncation=True, padding=\"max_length\",\n",
    "                       max_length=self.max_tgt, return_tensors=\"pt\")\n",
    "        labels = tgt.input_ids.clone()\n",
    "        labels[labels==self.tok.pad_token_id] = -100\n",
    "        return {\n",
    "            \"input_ids\":      src.input_ids.squeeze(),\n",
    "            \"attention_mask\": src.attention_mask.squeeze(),\n",
    "            \"labels\":         labels.squeeze(),\n",
    "        }\n",
    "\n",
    "# ———————————————————————————————\n",
    "# 2) Stage 1: MedMCQA fine-tuning\n",
    "# ———————————————————————————————\n",
    "print(\"=== Stage 1: MedMCQA fine-tuning ===\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL).to(DEVICE)\n",
    "\n",
    "med_ds = load_dataset(\"openlifescienceai/medmcqa\")\n",
    "train_med = med_ds[\"train\"].select(range(5000))\n",
    "eval_med  = med_ds[\"validation\"].select(range(500))\n",
    "\n",
    "train_med_ds = MedMCQADataset(train_med, tokenizer)\n",
    "eval_med_ds  = MedMCQADataset(eval_med,  tokenizer)\n",
    "\n",
    "args1 = TrainingArguments(\n",
    "    output_dir=MED_FT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "trainer1 = Trainer(\n",
    "    model=model,\n",
    "    args=args1,\n",
    "    train_dataset=train_med_ds,\n",
    "    eval_dataset=eval_med_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer1.train()\n",
    "trainer1.save_model(MED_FT_DIR)\n",
    "tokenizer.save_pretrained(MED_FT_DIR)\n",
    "\n",
    "# ———————————————————————————————\n",
    "# 3) Stage 2: Clinical notes fine-tuning\n",
    "# ———————————————————————————————\n",
    "print(\"=== Stage 2: Clinical notes fine-tuning ===\")\n",
    "# reload on CPU for fixes\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(MED_FT_DIR, device_map=\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MED_FT_DIR)\n",
    "\n",
    "# fix pad_token & resize embeddings\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)  # must have columns 'dialogue','note'\n",
    "hf_clin = HFDataset.from_pandas(df)\n",
    "\n",
    "train_clin = hf_clin.shuffle(42).select(range(400))\n",
    "eval_clin  = hf_clin.shuffle(123).select(range(400,464))\n",
    "\n",
    "train_ds2 = DialogueSummaryDataset(train_clin, tokenizer)\n",
    "eval_ds2  = DialogueSummaryDataset(eval_clin,  tokenizer)\n",
    "\n",
    "args2 = TrainingArguments(\n",
    "    output_dir=FINAL_FT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "trainer2 = Trainer(\n",
    "    model=model,\n",
    "    args=args2,\n",
    "    train_dataset=train_ds2,\n",
    "    eval_dataset=eval_ds2,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer2.train()\n",
    "trainer2.save_model(FINAL_FT_DIR)\n",
    "tokenizer.save_pretrained(FINAL_FT_DIR)\n",
    "\n",
    "# ———————————————————————————————\n",
    "# 4) Batch inference\n",
    "# ———————————————————————————————\n",
    "print(\"=== Stage 3: Batch inference ===\")\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "batch_size  = 4\n",
    "num_samples = len(eval_ds2)\n",
    "num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "predictions, references = [], []\n",
    "for i in range(num_batches):\n",
    "    start, end = i*batch_size, min((i+1)*batch_size, num_samples)\n",
    "    convs = [str(x) for x in eval_clin[\"dialogue\"][start:end]]\n",
    "    refs  = [str(x) for x in eval_clin[\"note\"][start:end]]\n",
    "    prompts = [\n",
    "        f\"Summarize the following conversation:\\n\\n{c}\"\n",
    "        for c in convs if len(c.strip())>10\n",
    "    ]\n",
    "    if not prompts:\n",
    "        continue\n",
    "    outs = summarizer(prompts, max_new_tokens=120, do_sample=False)\n",
    "    predictions.extend([o[\"summary_text\"] for o in outs])\n",
    "    references.extend(refs)\n",
    "\n",
    "print(f\"Generated {len(predictions)} summaries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcyIQFs8bRc5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xeN6bGIK3B5E",
    "outputId": "834ccc25-74a6-4e26-97c4-56e9bc2c6dc0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "# 1) Reload & repair checkpoint\n",
    "MODEL_DIR = \"/content/drive/MyDrive/bart_clinical_ft\"\n",
    "device    = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR, device_map=\"cuda\")\n",
    "\n",
    "# force pad_token = eos_token and resize to avoid any OOB ID\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# 2) Load your eval set\n",
    "df      = pd.read_csv(\"/content/clinical_notes.csv\")  # columns: dialogue, note\n",
    "hf_clin = HFDataset.from_pandas(df).shuffle(seed=42)\n",
    "eval_ds = hf_clin.select(range(400,464))\n",
    "\n",
    "# 3) Batch‐wise manual generation\n",
    "batch_size  = 4\n",
    "num_items   = len(eval_ds)\n",
    "predictions = []\n",
    "references  = []\n",
    "\n",
    "for i in range(0, num_items, batch_size):\n",
    "    # grab slices of the two columns as plain Python lists\n",
    "    convs = [str(x) for x in eval_ds[\"dialogue\"][i : i + batch_size]]\n",
    "    refs  = [str(x) for x in eval_ds[\"note\"][i : i + batch_size]]\n",
    "\n",
    "    # build prompts and skip short ones\n",
    "    prompts = [\n",
    "        f\"Summarize the following conversation:\\n\\n{c}\"\n",
    "        for c in convs if len(c.strip()) > 10\n",
    "    ]\n",
    "    if not prompts:\n",
    "        continue\n",
    "\n",
    "    # tokenize *with* truncation & max_length\n",
    "    enc = tokenizer(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    # generate on the same device\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            input_ids=enc.input_ids,\n",
    "            attention_mask=enc.attention_mask,\n",
    "            max_new_tokens=120,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    # decode & store\n",
    "    dec = tokenizer.batch_decode(out_ids, skip_special_tokens=True)\n",
    "    predictions.extend(dec)\n",
    "    references.extend(refs)\n",
    "\n",
    "print(f\"✅ Done — generated {len(predictions)} summaries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxXm02qV3B2r"
   },
   "outputs": [],
   "source": [
    "! CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rN6GVR8kcND"
   },
   "outputs": [],
   "source": [
    "pip install -U \"transformers>=4.39\" datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTpKqvKkjTWq"
   },
   "outputs": [],
   "source": [
    "import os, torch, pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "                          Trainer, TrainingArguments)\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from datasets import Dataset as HFDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWgEVCiCjTTu"
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BASE_MODEL = \"facebook/bart-base\"        # starting checkpoint\n",
    "MED_FT_DIR  = \"./bart_medmcqa_ft\"        # after Stage‑1\n",
    "FINAL_FT_DIR = \"./bart_clinical_ft\"      # after Stage‑2\n",
    "CSV_PATH = \"/content/clinical_notes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SS5wlobbjTQ3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. DATASET HELPERS\n",
    "# -------------------------------------------------------------\n",
    "class MedMCQADataset(TorchDataset):\n",
    "    def __init__(self, hf_ds, tokenizer,\n",
    "                 max_src=256, max_tgt=16):\n",
    "        self.tok, self.max_src, self.max_tgt = tokenizer, max_src, max_tgt\n",
    "        self.examples = []\n",
    "\n",
    "        # ----------  PATCH START  ----------\n",
    "        # safe helper that returns a clean string\n",
    "        def safe_str(val):\n",
    "            return str(val) if val is not None else \"\"\n",
    "        # loop over the HF examples\n",
    "        for row in hf_ds:\n",
    "            self.examples.append(\n",
    "                {\n",
    "                    \"q\": safe_str(row[\"question\"]),\n",
    "                    \"options\": [safe_str(row[\"opa\"]),\n",
    "                                safe_str(row[\"opb\"]),\n",
    "                                safe_str(row[\"opc\"]),\n",
    "                                safe_str(row[\"opd\"])],\n",
    "                    \"ans\": safe_str(row[\"cop\"]),\n",
    "                }\n",
    "            )\n",
    "        # ----------  PATCH END  ----------\n",
    "\n",
    "    def __len__(self): return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.examples[idx]\n",
    "        prompt = f\"Question: {ex['q']} Options: \" \\\n",
    "                 f\"A) {ex['options'][0]} B) {ex['options'][1]} \" \\\n",
    "                 f\"C) {ex['options'][2]} D) {ex['options'][3]}\"\n",
    "        target = ex['ans']\n",
    "        src = self.tok(prompt, truncation=True, max_length=self.max_src,\n",
    "                       padding=\"max_length\", return_tensors=\"pt\")\n",
    "        tgt = self.tok(target, truncation=True, max_length=self.max_tgt,\n",
    "                       padding=\"max_length\", return_tensors=\"pt\")\n",
    "        labels = tgt[\"input_ids\"].clone()\n",
    "        labels[labels == self.tok.pad_token_id] = -100\n",
    "        return {\"input_ids\": src[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": src[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": labels.squeeze()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTRO9p-NjTNw"
   },
   "outputs": [],
   "source": [
    "class DialogueSummaryDataset(TorchDataset):\n",
    "    \"\"\"\n",
    "    dialogue  ->  structured summary\n",
    "    Expects a HF `Dataset` with columns 'dialogue' & 'summary'\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_ds, tokenizer,\n",
    "                 max_src=512, max_tgt=256):\n",
    "        self.tok, self.max_src, self.max_tgt = tokenizer, max_src, max_tgt\n",
    "        self.dialogues = hf_ds[\"dialogue\"]\n",
    "        self.summaries = hf_ds[\"note\"]\n",
    "\n",
    "    def __len__(self): return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_txt = self.dialogues[idx]\n",
    "        tgt_txt = self.summaries[idx]\n",
    "        src = self.tok(src_txt, truncation=True, max_length=self.max_src,\n",
    "                       padding=\"max_length\", return_tensors=\"pt\")\n",
    "        tgt = self.tok(tgt_txt, truncation=True, max_length=self.max_tgt,\n",
    "                       padding=\"max_length\", return_tensors=\"pt\")\n",
    "        labels = tgt[\"input_ids\"].clone()\n",
    "        labels[labels == self.tok.pad_token_id] = -100\n",
    "        return {\"input_ids\": src[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": src[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": labels.squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEbfoDYgjTK8"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "FQUSg-A9jTID",
    "outputId": "9cd5bc26-28d9-4a9c-81c8-bd59553f1dd4"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 3. STAGE‑1 : MedMCQA FINE‑TUNING\n",
    "# -------------------------------------------------------------\n",
    "print(\"\\n=== Stage 1: MedMCQA fine‑tuning ===\")\n",
    "med_ds = load_dataset(\"openlifescienceai/medmcqa\")\n",
    "train_med = med_ds[\"train\"].select(range(5000))       # small subset\n",
    "eval_med  = med_ds[\"validation\"].select(range(500))   # small subset\n",
    "\n",
    "train_med_ds = MedMCQADataset(train_med, tokenizer)\n",
    "eval_med_ds  = MedMCQADataset(eval_med, tokenizer)\n",
    "\n",
    "args_med = TrainingArguments(\n",
    "    output_dir=MED_FT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer_med = Trainer(\n",
    "    model=model,\n",
    "    args=args_med,\n",
    "    train_dataset=train_med_ds,\n",
    "    eval_dataset=eval_med_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer_med.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kGsmkV9cjSvV",
    "outputId": "df230ec9-b6e0-460a-d616-fbe7b1e7f819"
   },
   "outputs": [],
   "source": [
    "trainer_med.save_model(MED_FT_DIR)\n",
    "tokenizer.save_pretrained(MED_FT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "-cZ_pP5PjVDg",
    "outputId": "6a22d90a-177c-4e8c-9ebd-14e3a0bbc30a"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 4. STAGE‑2 : Dialogue‑>Structured Summary FINE‑TUNING\n",
    "# -------------------------------------------------------------\n",
    "print(\"\\n=== Stage 2: Clinical‑notes fine‑tuning ===\")\n",
    "\n",
    "# 1️⃣  Always load on CPU first\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MED_FT_DIR, device_map=\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MED_FT_DIR)\n",
    "\n",
    "# 2️⃣  Ensure vocabulary / embedding sizes match\n",
    "vocab_len = len(tokenizer)\n",
    "if model.config.vocab_size != vocab_len:\n",
    "    print(f\"‑ Resizing embeddings: {model.config.vocab_size}  →  {vocab_len}\")\n",
    "    model.resize_token_embeddings(vocab_len)\n",
    "\n",
    "# 3️⃣  OPTIONAL: sanity‑check for NaNs / Infs in the checkpoint\n",
    "with torch.no_grad():\n",
    "    for name, p in model.named_parameters():\n",
    "        if torch.isnan(p).any() or torch.isinf(p).any():\n",
    "            raise RuntimeError(f\"NaNs/Infs found in {name}\")\n",
    "\n",
    "# 4️⃣  Now move to GPU\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "\n",
    "# Load your CSV → HF Dataset\n",
    "df = pd.read_csv(CSV_PATH, sep=\",\")          # adjust sep if tabs\n",
    "hf_clinical = Dataset.from_pandas(df)\n",
    "\n",
    "train_clin = hf_clinical.shuffle(seed=42).select(range(400))\n",
    "eval_clin  = hf_clinical.shuffle(seed=123).select(range(400,464))\n",
    "\n",
    "train_clin_ds = DialogueSummaryDataset(train_clin, tokenizer)\n",
    "eval_clin_ds  = DialogueSummaryDataset(eval_clin, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "mDbIDMdrjVA6",
    "outputId": "736ccab9-4f82-4dbd-e956-6efdf0851bf3"
   },
   "outputs": [],
   "source": [
    "args_clin = TrainingArguments(\n",
    "    output_dir=FINAL_FT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,    # effective 8\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer_clin = Trainer(\n",
    "    model=model,\n",
    "    args=args_clin,\n",
    "    train_dataset=train_clin_ds,\n",
    "    eval_dataset=eval_clin_ds,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer_clin.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RqZBKpozjU-G",
    "outputId": "b76422cc-789f-47b6-afd0-afe62cf4a416"
   },
   "outputs": [],
   "source": [
    "trainer_clin.save_model(FINAL_FT_DIR)\n",
    "tokenizer.save_pretrained(FINAL_FT_DIR)\n",
    "\n",
    "print(\"\\n✅ Two‑stage fine‑tuning complete. Final model saved to:\", FINAL_FT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "tdQ0VRZZjU69",
    "outputId": "a10b1e3a-3e9a-4cda-9844-cbc2cba3d360"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# 5. QUICK TEST (optional)\n",
    "# -------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    from transformers import pipeline\n",
    "    summarizer = pipeline(\"summarization\",\n",
    "                          model=FINAL_FT_DIR,\n",
    "                          tokenizer=FINAL_FT_DIR,\n",
    "                          device=0 if torch.cuda.is_available() else -1)\n",
    "    test_dialogue = df[\"dialogue\"][0]\n",
    "    print(\"\\n--- SAMPLE SUMMARY ---\")\n",
    "    print(summarizer(test_dialogue, max_length=200,\n",
    "                     min_length=80, do_sample=False)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "SHaKAOrEjU4A",
    "outputId": "78f7b97d-1235-4d1f-e82a-1cfb722ec261"
   },
   "outputs": [],
   "source": [
    "# two_stage_bart_ft.py  ─────────────────────────────────────────\n",
    "#   Two‑stage BART fine‑tune:\n",
    "#     1. MedMCQA  (QA adaptation)\n",
    "#     2. clinical_notes.csv  (dialogue ➜ structured summary)\n",
    "#   Requires: transformers >=4.39  datasets  torch  pandas  tqdm\n",
    "# ----------------------------------------------------------------\n",
    "import os, torch, pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "                          Trainer, TrainingArguments)\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "BASE_MODEL   = \"facebook/bart-base\"\n",
    "MED_FT_DIR   = \"./bart_medmcqa_ft\"\n",
    "FINAL_FT_DIR = \"./bart_clinical_ft\"\n",
    "CSV_PATH     = \"./clinical_notes.csv\"        # 👈 your file (dialogue, summary)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "#  Dataset helpers\n",
    "# ----------------------------------------------------------------\n",
    "class MedMCQADataset(TorchDataset):\n",
    "    \"\"\" Question + 4 options  -->  correct option letter \"\"\"\n",
    "    def __init__(self, hf_ds, tokenizer, max_src=256, max_tgt=16):\n",
    "        self.tok, self.max_src, self.max_tgt = tokenizer, max_src, max_tgt\n",
    "        self.examples = []\n",
    "\n",
    "        def s(val):               # safe cast\n",
    "            return str(val) if val is not None else \"\"\n",
    "\n",
    "        for row in hf_ds:\n",
    "            self.examples.append(\n",
    "                dict(\n",
    "                    prompt=f\"Question: {s(row['question'])} Options: \"\n",
    "                           f\"A) {s(row['opa'])} B) {s(row['opb'])} \"\n",
    "                           f\"C) {s(row['opc'])} D) {s(row['opd'])}\",\n",
    "                    answer=s(row['cop']),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def __len__(self): return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.examples[idx]\n",
    "        src = self.tok(ex[\"prompt\"], max_length=self.max_src,\n",
    "                       truncation=True, padding=\"max_length\",\n",
    "                       return_tensors=\"pt\")\n",
    "        tgt = self.tok(ex[\"answer\"], max_length=self.max_tgt,\n",
    "                       truncation=True, padding=\"max_length\",\n",
    "                       return_tensors=\"pt\")\n",
    "        labels = tgt[\"input_ids\"]\n",
    "        labels[labels == self.tok.pad_token_id] = -100\n",
    "        return {\"input_ids\": src[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": src[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": labels.squeeze()}\n",
    "\n",
    "\n",
    "class DialogueSummaryDataset(TorchDataset):\n",
    "    \"\"\" dialogue  ->  structured summary \"\"\"\n",
    "    def __init__(self, hf_ds, tokenizer, max_src=512, max_tgt=256):\n",
    "        self.tok, self.max_src, self.max_tgt = tokenizer, max_src, max_tgt\n",
    "        self.dialogues = hf_ds[\"dialogue\"]\n",
    "        self.summaries = hf_ds[\"note\"]\n",
    "\n",
    "    def __len__(self): return len(self.dialogues)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_txt, tgt_txt = self.dialogues[idx], self.summaries[idx]\n",
    "        src = self.tok(src_txt, max_length=self.max_src,\n",
    "                       truncation=True, padding=\"max_length\",\n",
    "                       return_tensors=\"pt\")\n",
    "        tgt = self.tok(tgt_txt, max_length=self.max_tgt,\n",
    "                       truncation=True, padding=\"max_length\",\n",
    "                       return_tensors=\"pt\")\n",
    "        labels = tgt[\"input_ids\"]\n",
    "        labels[labels == self.tok.pad_token_id] = -100\n",
    "        return {\"input_ids\": src[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": src[\"attention_mask\"].squeeze(),\n",
    "                \"labels\": labels.squeeze()}\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "#  Stage‑1 : MedMCQA fine‑tuning\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n=== Stage 1 : MedMCQA fine‑tuning ===\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)\n",
    "\n",
    "med_ds = load_dataset(\"openlifescienceai/medmcqa\")\n",
    "train_med = med_ds[\"train\"].select(range(5000))\n",
    "eval_med  = med_ds[\"validation\"].select(range(500))\n",
    "\n",
    "train_med_ds = MedMCQADataset(train_med, tokenizer)\n",
    "eval_med_ds  = MedMCQADataset(eval_med, tokenizer)\n",
    "\n",
    "args_med = TrainingArguments(\n",
    "    output_dir=MED_FT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "Trainer(\n",
    "    model=model,\n",
    "    args=args_med,\n",
    "    train_dataset=train_med_ds,\n",
    "    eval_dataset=eval_med_ds,\n",
    "    tokenizer=tokenizer,\n",
    ").train()\n",
    "\n",
    "model.save_pretrained(MED_FT_DIR)\n",
    "tokenizer.save_pretrained(MED_FT_DIR)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "#  Stage‑2 : clinical_notes.csv fine‑tuning\n",
    "# ----------------------------------------------------------------\n",
    "print(\"\\n=== Stage 2 : Clinical‑notes fine‑tuning ===\")\n",
    "\n",
    "# 1️⃣ load on CPU first\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MED_FT_DIR, device_map=\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MED_FT_DIR)\n",
    "\n",
    "# 2️⃣ ensure pad / eos ids are valid\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# 3️⃣ resize embeddings ↔ tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 4️⃣ move to GPU (now safe)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# build HF dataset from CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "hf_clin = Dataset.from_pandas(df)\n",
    "\n",
    "# small sample for demo\n",
    "train_clin = hf_clin.shuffle(seed=42).select(range(400))\n",
    "eval_clin  = hf_clin.shuffle(seed=123).select(range(400, 464))\n",
    "\n",
    "train_clin_ds = DialogueSummaryDataset(train_clin, tokenizer)\n",
    "eval_clin_ds  = DialogueSummaryDataset(eval_clin, tokenizer)\n",
    "\n",
    "args_clin = TrainingArguments(\n",
    "    output_dir=FINAL_FT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,   # effective 8\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "Trainer(\n",
    "    model=model,\n",
    "    args=args_clin,\n",
    "    train_dataset=train_clin_ds,\n",
    "    eval_dataset=eval_clin_ds,\n",
    "    tokenizer=tokenizer,\n",
    ").train()\n",
    "\n",
    "model.save_pretrained(FINAL_FT_DIR)\n",
    "tokenizer.save_pretrained(FINAL_FT_DIR)\n",
    "\n",
    "print(\"\\n✅ Finished.  Final model stored at:\", FINAL_FT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "vf5p1CRBjU1X",
    "outputId": "552573ca-27d5-4a25-8f92-425ff5ac1a7a"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"text-generation\",\n",
    "                      model=FINAL_FT_DIR,\n",
    "                      tokenizer=FINAL_FT_DIR,\n",
    "                      device=0 if torch.cuda.is_available() else -1)\n",
    "sample = df[\"dialogue\"][0]\n",
    "print(\"\\n--- SAMPLE SUMMARY ---\")\n",
    "print(summarizer(sample, max_length=200,\n",
    "                  min_length=80, do_sample=False)[0][\"summary_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zpg9hDJijUh5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTDNPYsbjVoK"
   },
   "source": [
    "OLD CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "rmcMd59J3L5l",
    "outputId": "4057ae42-a942-4ce0-fa1c-d8100c137e10"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Custom Dataset class for MedMCQA\n",
    "class MedMCQADataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_input_length=256, max_output_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Convert dataset to list of dictionaries for easy iteration\n",
    "        self.data = hf_dataset.to_dict()\n",
    "        num_examples = len(next(iter(self.data.values())))\n",
    "\n",
    "        # Build list of examples as dicts\n",
    "        self.examples = [\n",
    "            {key: self.data[key][i] for key in self.data}\n",
    "            for i in range(num_examples)\n",
    "        ]\n",
    "\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "\n",
    "        # Construct inputs and targets as strings\n",
    "        for item in self.examples:\n",
    "            # Ensure all fields exist and are strings; replace missing values if needed\n",
    "            question = str(item.get('question', ''))\n",
    "            opa = str(item.get('opa', ''))\n",
    "            opb = str(item.get('opb', ''))\n",
    "            opc = str(item.get('opc', ''))\n",
    "            opd = str(item.get('opd', ''))\n",
    "            cop = str(item.get('cop', ''))\n",
    "\n",
    "            self.inputs.append(f\"Question: {question} Options: {opa} {opb} {opc} {opd}\")\n",
    "            self.targets.append(cop)\n",
    "\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        target_text = self.targets[idx]\n",
    "\n",
    "        input_encodings = self.tokenizer(\n",
    "            input_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_input_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        target_encodings = self.tokenizer(\n",
    "            target_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_output_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "        labels = target_encodings['input_ids']\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encodings['input_ids'].squeeze(),\n",
    "            \"attention_mask\": input_encodings['attention_mask'].squeeze(),\n",
    "            \"labels\": labels.squeeze()\n",
    "        }\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "base_model_name = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Load the MedMCQA dataset\n",
    "med_dataset = load_dataset(\"openlifescienceai/medmcqa\")\n",
    "\n",
    "# Select subsets for training and evaluation\n",
    "train_med_subset = med_dataset['train'].select(range(5000))\n",
    "eval_med_subset = med_dataset['validation'].select(range(500))\n",
    "\n",
    "# Create dataset objects using the custom class\n",
    "train_med_dataset = MedMCQADataset(train_med_subset, tokenizer)\n",
    "eval_med_dataset = MedMCQADataset(eval_med_subset, tokenizer)\n",
    "\n",
    "# Define training arguments for medical fine-tuning\n",
    "training_args_med = TrainingArguments(\n",
    "    output_dir=\"./medical_ft_model\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer_med = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_med,\n",
    "    train_dataset=train_med_dataset,\n",
    "    eval_dataset=eval_med_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start medical fine-tuning\n",
    "trainer_med.train()\n",
    "\n",
    "# Save model and tokenizer after medical fine-tuning\n",
    "trainer_med.save_model(\"./medical_ft_model\")\n",
    "tokenizer.save_pretrained(\"./medical_ft_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "tGfefn623S0Q",
    "outputId": "4c9e03c4-4699-422c-880a-ddb2207794c6"
   },
   "outputs": [],
   "source": [
    "from transformers import (AutoModelForSeq2SeqLM, AutoTokenizer,\n",
    "                          Trainer, TrainingArguments, DataCollatorForSeq2Seq)\n",
    "import torch, pandas as pd\n",
    "from datasets import Dataset as HFDataset\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  PyTorch Dataset wrapper\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "class ConversationSummaryDataset(TorchDataset):\n",
    "    \"\"\"\n",
    "    Expects an HF Dataset with columns: 'dialogue' and 'summary'\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_ds, tokenizer,\n",
    "                 max_input_len=512, max_output_len=256):\n",
    "        self.ds, self.tok = hf_ds, tokenizer\n",
    "        self.max_in, self.max_out = max_input_len, max_output_len\n",
    "\n",
    "    def __len__(self): return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.ds[idx]\n",
    "        src_txt = str(row[\"dialogue\"])\n",
    "        tgt_txt = str(row[\"note\"])      # <- was 'note'\n",
    "\n",
    "        src = self.tok(src_txt, max_length=self.max_in,\n",
    "                       truncation=True, padding=\"max_length\",\n",
    "                       return_tensors=\"pt\")\n",
    "        tgt = self.tok(tgt_txt, max_length=self.max_out,\n",
    "                       truncation=True, padding=\"max_length\",\n",
    "                       return_tensors=\"pt\")\n",
    "\n",
    "        labels = tgt[\"input_ids\"].clone()\n",
    "        labels[labels == self.tok.pad_token_id] = -100\n",
    "        return {\"input_ids\":      src[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": src[\"attention_mask\"].squeeze(),\n",
    "                \"labels\":         labels.squeeze()}\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Load Stage‑1 checkpoint\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "MODEL_DIR = \"./medical_ft_model\"\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "\n",
    "# make sure pad token is defined\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Build HF Dataset from CSV\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(\"/content/clinical_notes.csv\")  # columns: dialogue, summary\n",
    "hf_clin = HFDataset.from_pandas(df)\n",
    "\n",
    "train_ds = hf_clin.shuffle(seed=42).select(range(400))\n",
    "eval_ds  = hf_clin.shuffle(seed=123).select(range(400, 464))\n",
    "\n",
    "train_dataset = ConversationSummaryDataset(train_ds, tokenizer)\n",
    "eval_dataset  = ConversationSummaryDataset(eval_ds, tokenizer)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Training setup\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./final_sft_model\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_strategy=\"epoch\",          # <- fixed name\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Fine‑tune and save\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "trainer.train()\n",
    "trainer.save_model(\"./final_sft_model\")\n",
    "tokenizer.save_pretrained(\"./final_sft_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-hdo5p6ujlC",
    "outputId": "a4b87ddb-1157-4c04-b2b7-6057cf8b35b1"
   },
   "outputs": [],
   "source": [
    "!zip -r /content/final_sft_model.zip /content/final_sft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQb3JJm6ujbN"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"/content/final_sft_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kSIQDAhu24g",
    "outputId": "8adcd1da-886c-4886-e163-8f5fd2df9665"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "29Ix4PNz3Upu",
    "outputId": "477953aa-3f61-494d-8059-8a6ce30c73cc"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the summarization pipeline with the fine-tuned model\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"./final_sft_model\",\n",
    "    tokenizer=\"./final_sft_model\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Define a custom conversation\n",
    "custom_conversation = \"\"\"\n",
    "Doctor: Hi, Mr. X, I'm Dr. Y. How are you feeling today?\n",
    "Patient: Not too good, doctor. I've been feeling really sick lately.\n",
    "Doctor: I understand. Can you tell me what symptoms you're experiencing?\n",
    "Patient: Yes, I've been having a fever, a dry cough, and dyspnea.\n",
    "Doctor: I see. You were hospitalized due to moderate ARDS from COVID-19, is that correct?\n",
    "Patient: Yes, that's correct.\n",
    "Doctor: During your physical therapy, we encountered some difficulties. Can you tell me more about that?\n",
    "Patient: Yes, I had trouble with position changes and deep breathing. Every time I tried to change my position or take a deep breath, I would start coughing and it would make me really short of breath.\n",
    "Doctor: I understand. To avoid rapid deterioration and respiratory failure, we instructed you to change positions very slowly and step-by-step, right?\n",
    "Patient: Yes, that's right.\n",
    "Doctor: And I see that this approach increased your oxygen saturation, for example, on day 5 with 6 L/min of oxygen from 93% to 97%.\n",
    "Patient: Yes, that's correct.\n",
    "Doctor: Good. We also had to adapt your breathing exercises to avoid prolonged coughing and oxygen desaturation. Can you tell me more about that?\n",
    "Patient: Yes, I was instructed to stop every deep breath before coughing and to hold my breath for better air distribution.\n",
    "Doctor: I see that you performed the breathing exercises well and managed to increase your oxygen saturation.\n",
    "Patient: Yes, I did my best.\n",
    "Doctor: You also had difficulty maintaining sufficient oxygen saturation during physical activity, is that correct?\n",
    "Patient: Yes, I did. But with close monitoring and frequent breaks, I was able to perform low-level strength and walking exercises without any significant deoxygenation.\n",
    "Doctor: I see that your exercise progression was low on days 1 to 5, but then increased daily until your hospital discharge to a rehabilitation clinic on day 10.\n",
    "Patient: Yes, that's correct.\n",
    "Doctor: Great. I'd like to keep monitoring your progress and see how you're doing. Can you keep me updated on any changes in your symptoms?\n",
    "Patient: Yes, of course, doctor.\n",
    "Doctor: Alright, let's keep in touch. If you have any questions or concerns, don't hesitate to reach out to me.\n",
    "Patient: Thank you, doctor.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary\n",
    "summary = summarizer(custom_conversation, min_length=40, do_sample=False)\n",
    "print(\"Generated Summary:\")\n",
    "print(summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLj6aRH7ym0R"
   },
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMA_ZEOSyvBj"
   },
   "outputs": [],
   "source": [
    "!pip install rouge_score # Install the rouge_score dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgY3FfAxMykR"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUgvzvMI1RyE"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset # import the library\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Load the dataset\n",
    "notechat = load_dataset(\"akemiH/NoteChat\") # load the dataset and assign it to a variable\n",
    "\n",
    "eval_df = notechat[\"train\"].select(range(3000, 3500)).to_pandas()\n",
    "\n",
    "# Check for missing values and clean if necessary\n",
    "if eval_df.isnull().values.any():\n",
    "    print(\"Found missing values in the evaluation set. Dropping them.\")\n",
    "    eval_df = eval_df.dropna()\n",
    "\n",
    "batch_size = 4\n",
    "num_samples = len(eval_df)\n",
    "num_batches = num_samples // batch_size + int(num_samples % batch_size != 0)\n",
    "\n",
    "predictions = []\n",
    "references = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTHxs4vLyz29"
   },
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", device=0)  # Use device=-1 for CPU if GPU is unavailable\n",
    "\n",
    "# Iterate through batches and generate summaries\n",
    "for i in tqdm(range(num_batches), desc=\"Generating Summaries\"):\n",
    "    # Define the start and end indices for the current batch\n",
    "    start = i * batch_size\n",
    "    end = min(start + batch_size, num_samples)\n",
    "\n",
    "    # Extract batch conversations and references\n",
    "    batch_conversations = eval_df[\"conversation\"][start:end].tolist()\n",
    "    batch_refs = eval_df[\"data\"][start:end].tolist()  # Adjust column name if necessary\n",
    "\n",
    "    # Prepare prompts for summarization\n",
    "    prompts = [\n",
    "        f\"Summarize the following conversation:\\n\\n{conv}\" for conv in batch_conversations\n",
    "    ]\n",
    "\n",
    "    # Generate summaries for the batch\n",
    "    results = summarizer(\n",
    "        prompts,\n",
    "        max_new_tokens=100,  # Adjust token limit based on your model's capabilities\n",
    "        do_sample=False,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Extract the predicted summaries and add them to the list\n",
    "    predictions.extend([r[\"summary_text\"] for r in results])\n",
    "\n",
    "    # Collect the references for the batch\n",
    "    references.extend(batch_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "BO-dWGnEuIdR",
    "outputId": "1b008f07-fd46-4f7e-ce51-ada7f1ca839c"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"/content/drive/MyDrive/bart_clinical_ft\",\n",
    "    tokenizer=\"/content/drive/MyDrive/final_sft_model\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "\n",
    "batch_size = 4\n",
    "num_samples = len(eval_ds)\n",
    "num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for i in tqdm(range(num_batches), desc=\"Generating summaries\"):\n",
    "    start, end = i * batch_size, min((i + 1) * batch_size, num_samples)\n",
    "\n",
    "    batch_conversations = [str(c) for c in eval_ds[\"dialogue\"][start:end]]\n",
    "    batch_refs = [str(r) for r in eval_ds[\"note\"][start:end]]\n",
    "\n",
    "    prompts = []\n",
    "    for conv in batch_conversations:\n",
    "        if isinstance(conv, str) and len(conv.strip()) > 5:\n",
    "            prompts.append(f\"Summarize the following conversation:\\n\\n{conv}\")\n",
    "\n",
    "    if not prompts:\n",
    "        continue  # skip empty batch\n",
    "\n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        if input_ids.shape[-1] > 900:    # avoid too long prompts\n",
    "            print(f\"⚠️ Skipping overly long prompt with {input_ids.shape[-1]} tokens\")\n",
    "            continue\n",
    "\n",
    "        result = summarizer(\n",
    "            prompt,\n",
    "            max_new_tokens=120,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        predictions.append(result[0][\"summary_text\"])\n",
    "\n",
    "    references.extend(batch_refs)\n",
    "\n",
    "print(f\"Generated {len(predictions)} summaries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "HECi5ABwxiw7",
    "outputId": "43d62ac8-4371-4195-a0ba-4c006ac9af87"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "import torch, pandas as pd\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# 1) Load & repair (CPU), then move to GPU yourself\n",
    "# ──────────────────────────────────────────────────────────\n",
    "MODEL_DIR = \"/content/drive/MyDrive/final_sft_model\"\n",
    "\n",
    "# a) load normally (no device_map)\n",
    "model     = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# b) fix pad/EOS and resize embeddings\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "\n",
    "# c) move to GPU (or CPU)\n",
    "device_id = 0 if torch.cuda.is_available() else -1\n",
    "model = model.to(f\"cuda:{device_id}\" if device_id >= 0 else \"cpu\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# 2) Build the pipeline with the same device\n",
    "# ──────────────────────────────────────────────────────────\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device_id            # <— this makes inputs & model live together\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────\n",
    "# 3) Generate in batches (all on GPU)\n",
    "# ──────────────────────────────────────────────────────────\n",
    "df     = pd.read_csv(\"/content/clinical_notes.csv\")  # columns: dialogue, note\n",
    "hf_clin = HFDataset.from_pandas(df).shuffle(seed=42)\n",
    "eval_ds = hf_clin.select(range(400, 464))\n",
    "\n",
    "batch_size  = 4\n",
    "num_samples = len(eval_ds)\n",
    "num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "predictions, references = [], []\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start, end = i * batch_size, min((i + 1) * batch_size, num_samples)\n",
    "    convs = [str(x) for x in eval_ds[\"dialogue\"][start:end]]\n",
    "    refs  = [str(x) for x in eval_ds[\"note\"][start:end]]\n",
    "\n",
    "    prompts = [\n",
    "        f\"Summarize the following conversation:\\n\\n{c}\"\n",
    "        for c in convs if len(c.strip()) > 10\n",
    "    ]\n",
    "    if not prompts:\n",
    "        continue\n",
    "\n",
    "    # this call now runs entirely on GPU\n",
    "    outputs = summarizer(prompts, max_new_tokens=100, do_sample=False)\n",
    "\n",
    "    predictions.extend([o[\"summary_text\"] for o in outputs])\n",
    "    references.extend(refs)\n",
    "\n",
    "print(f\"Generated {len(predictions)} summaries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_WhUZ7v0fjO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpyPyjXJvj2J",
    "outputId": "a9aae780-fb95-4cfe-e5de-023912d7d3ed"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yndxwUUY6tU"
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nnuabXz0jXCs",
    "outputId": "c937fd89-6fed-4fc9-d29b-d7129a459374"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/maszhongming/UniEval.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmP8-Udrf5uu"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/UniEval\")  # if needed to make sure your Python can import from the UniEval folder\n",
    "\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDO-xENejhKt",
    "outputId": "8322cde1-1a40-42ef-d02a-adb6976b933a"
   },
   "outputs": [],
   "source": [
    "!pip install -r UniEval/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6X_b20vqjkKW"
   },
   "outputs": [],
   "source": [
    "# Lists for UniEval\n",
    "src_list = eval_clin[\"dialogue\"]          # already a list\n",
    "ref_list = eval_clin[\"note\"]           # <- was \"note\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T72qHKbjpne9"
   },
   "outputs": [],
   "source": [
    "output_list = []\n",
    "for pred in predictions: # Loop over each conversation string\n",
    "    output_list.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMpkdWjIZQmD"
   },
   "outputs": [],
   "source": [
    "data = convert_to_json(\n",
    "    src_list=src_list,\n",
    "    ref_list=ref_list,\n",
    "    output_list=output_list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgKrpksEDEM4"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Assuming you already have `data` from convert_to_json\n",
    "with open(\"/content/unieval_data.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LY3T2P5KCph3"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/content/unieval_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "p0jGErpcCxnY",
    "outputId": "8de20526-efc5-4299-d778-274d3dda9ebc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[“CUDA_LAUNCH_BLOCKING”] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "APAuZ8x5ZY3U",
    "outputId": "d32038d1-1d30-436e-8bb5-0e14252ee6a6"
   },
   "outputs": [],
   "source": [
    "task = \"summarization, fact\"\n",
    "evaluator = get_evaluator(task, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPIgFNotaeSH",
    "outputId": "636d6514-3942-46c7-b2fd-019e189d1293"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the standard 'punkt' tokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tY17Vj6gBOLz"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "efT--NhWZsLy",
    "outputId": "e25fc3b3-f5cc-4e27-9937-881185682f69"
   },
   "outputs": [],
   "source": [
    "eval_scores = evaluator.evaluate(data, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yq-0Tw5bA_k2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGSuk_D045Kp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKMMhhR245Hk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4nhsODk449e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "KKeE2VqK446C",
    "outputId": "0f4106f8-8ff7-4dca-a947-0b9861b6c128"
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, AutoTokenizer,\n",
    "    AutoModelForCausalLM, BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1) LOAD YOUR SFT‐FINE‐TUNED BART MODEL (no need to re‐SFT)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "SFT_DIR = \"/content/drive/MyDrive/bart_clinical_ft\"\n",
    "# If you used Seq2SeqLM for summarization, load with that:\n",
    "gen_cfg = GenerationConfig.from_pretrained(SFT_DIR)\n",
    "gen_cfg.early_stopping = False\n",
    "\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(SFT_DIR, device_map=\"cpu\", generation_config=gen_cfg,)\n",
    "# Or if you prefer causal‐LM interface, switch to AutoModelForCausalLM.\n",
    "\n",
    "tok  = AutoTokenizer.from_pretrained(SFT_DIR)\n",
    "tok.pad_token = tok.eos_token\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2) (Optional) QUANTIZE + PREPARE FOR K‐BIT + LoRA ADAPTER\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "# re‐load base model in 4‑bit directly:\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    SFT_DIR,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    generation_config=gen_cfg,\n",
    ")\n",
    "# freeze all except LoRA\n",
    "base = prepare_model_for_kbit_training(base)\n",
    "base.gradient_checkpointing_enable()\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg).to(DEVICE)\n",
    "\n",
    "# make sure pad token is set\n",
    "model.config.pad_token_id = tok.eos_token_id\n",
    "model.resize_token_embeddings(len(tok))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFU50vNw5K_3"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ConvSumDS(Dataset):\n",
    "    def __init__(self, hf_ds, tokenizer, max_in=512, max_out=256):\n",
    "        self.ds, self.tok = hf_ds, tokenizer\n",
    "        self.max_in, self.max_out = max_in, max_out\n",
    "    def __len__(self): return len(self.ds)\n",
    "    def __getitem__(self, i):\n",
    "        row = self.ds[i]\n",
    "        src, tgt = str(row[\"dialogue\"]), str(row[\"note\"])\n",
    "        enc_in  = self.tok(src, max_length=self.max_in,\n",
    "                           truncation=True, padding=\"max_length\",\n",
    "                           return_tensors=\"pt\")\n",
    "        enc_out = self.tok(tgt, max_length=self.max_out,\n",
    "                           truncation=True, padding=\"max_length\",\n",
    "                           return_tensors=\"pt\")\n",
    "        labels = enc_out[\"input_ids\"]\n",
    "        labels[labels == self.tok.pad_token_id] = -100\n",
    "        return {\"input_ids\":enc_in[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\":enc_in[\"attention_mask\"].squeeze(),\n",
    "                \"labels\":labels.squeeze()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHOG9dC05x-Y"
   },
   "outputs": [],
   "source": [
    "# ---- PCGrad (Yu et al. 2020) -------------\n",
    "class PCGrad:\n",
    "    def __init__(self, optimizer):\n",
    "        self._optim = optimizer\n",
    "    def zero_grad(self): self._optim.zero_grad()\n",
    "    @torch.no_grad()\n",
    "    def pc_backward(self, grads: list[torch.Tensor]):\n",
    "        \"\"\"grads = list of flat grad tensors (one per objective)\"\"\"\n",
    "        # 1) project conflicting grads\n",
    "        for i in range(len(grads)):\n",
    "            for j in range(i+1, len(grads)):\n",
    "                gij = torch.dot(grads[i], grads[j])\n",
    "                if gij < 0:    # conflict\n",
    "                    grads[i] -= (gij / grads[j].norm()**2) * grads[j]\n",
    "        # 2) average the (now non‑conflicting) grads\n",
    "        merged = torch.stack(grads, dim=0).mean(0)\n",
    "        # 3) copy into model params\n",
    "        idx = 0\n",
    "        for p in model.parameters():\n",
    "            if p.requires_grad:\n",
    "                sz = p.numel()\n",
    "                p.grad = merged[idx:idx+sz].view_as(p).clone()\n",
    "                idx += sz\n",
    "    def step(self): self._optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsWDkOsn5y25"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/UniEval\")\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "# 1) Instantiate two evaluators **on CPU**:\n",
    "sum_eval  = get_evaluator(\"summarization\", device=\"cpu\")   # coherence, consistency, fluency (+ relevance)\n",
    "fact_eval = get_evaluator(\"fact\",          device=\"cpu\")   # factual consistency\n",
    "\n",
    "def batched_unieval(src_list, hyp_list):\n",
    "    \"\"\"\n",
    "    Given a batch of sources and hypotheses (summaries),\n",
    "    returns a dict of lists with keys\n",
    "      'coherence', 'consistency', 'fluency', 'factual'\n",
    "    each list has length == len(src_list).\n",
    "    \"\"\"\n",
    "    # 2) prepare JSON for UniEval\n",
    "    data = convert_to_json(output_list=hyp_list, src_list=src_list)\n",
    "\n",
    "    # 3) summarization dims in one call\n",
    "    #    dims can be any subset of ['coherence','consistency','fluency','relevance']\n",
    "    sum_scores = sum_eval.evaluate(\n",
    "        data,\n",
    "        dims=[\"coherence\",\"consistency\",\"fluency\"],\n",
    "        individual=True,      # one dict per example\n",
    "        overall=False\n",
    "    )\n",
    "    # sum_scores: numpy array of shape (batch,3)\n",
    "\n",
    "    # 4) split into python lists\n",
    "    coherence   = sum_scores[:,0].tolist()\n",
    "    consistency = sum_scores[:,1].tolist()\n",
    "    fluency     = sum_scores[:,2].tolist()\n",
    "\n",
    "    # 5) factual consistency\n",
    "    fact_dicts  = fact_eval.evaluate(data)    # returns list[{'consistency':…},…]\n",
    "    factual     = [d[\"consistency\"] for d in fact_dicts]\n",
    "\n",
    "    return {\n",
    "        \"coherence\":   coherence,\n",
    "        \"consistency\": consistency,\n",
    "        \"fluency\":     fluency,\n",
    "        \"factual\":     factual\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "UmMiLbF950XZ",
    "outputId": "bef0a486-d70d-4df3-e952-1d6df79dd443"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "onP7p_Nv7yi_",
    "outputId": "02d7c8a6-87dd-4381-c8e0-5efdb2938eef"
   },
   "outputs": [],
   "source": [
    "# ════════════════════════════════════════════════════════════════\n",
    "#  pip install -q \"trl==0.7.1\" \"transformers>=4.39\" bitsandbytes peft\n",
    "#  git clone https://github.com/yangkevin2/UniEval.git   # (or pip install)\n",
    "# ════════════════════════════════════════════════════════════════\n",
    "import os, sys, gc, torch, pandas as pd, numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1) UniEval helpers  (CPU only, load once)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "sys.path.append(\"/content/UniEval\")                # path to UniEval\n",
    "from utils            import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "sum_eval  = get_evaluator(\"summarization\", device=\"cpu\")\n",
    "fact_eval = get_evaluator(\"fact\",          device=\"cpu\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def unieval_4way(src: list[str], hyp: list[str]) -> torch.Tensor:\n",
    "    \"\"\"returns (B,4) tensor: [coh,cons,flu,fact]\"\"\"\n",
    "    data = convert_to_json(output_list=hyp, src_list=src)\n",
    "\n",
    "    tri = sum_eval.evaluate(data,\n",
    "                            dims=[\"coherence\",\"consistency\",\"fluency\"],\n",
    "                            individual=True, overall=False)      # ndarray\n",
    "    coh, con, flu = tri.T.tolist()\n",
    "    fact = [d[\"consistency\"] for d in fact_eval.evaluate(data)]\n",
    "    return torch.tensor([coh, con, flu, fact]).T       # on CPU\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2) load 4‑bit BART‑SFT  +  LoRA adapter\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "                          BitsAndBytesConfig, GenerationConfig)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl  import PPOConfig, PPOTrainer, AutoModelForSeq2SeqLMWithValueHead\n",
    "\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CKPT_DIR = \"/content/drive/MyDrive/final_sft_model\"     # ★ your ckpt\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 2‑A)  load WITHOUT passing generation_config\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    CKPT_DIR, quantization_config=bnb_cfg, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 2‑B) patch the generation config safely\n",
    "gc_obj = base.generation_config or GenerationConfig()\n",
    "if gc_obj.early_stopping is None:\n",
    "    gc_obj.early_stopping = False          # must be bool / \"never\"\n",
    "base.generation_config = gc_obj\n",
    "\n",
    "# 2‑C) make it train‑friendly & add LoRA\n",
    "base = prepare_model_for_kbit_training(base)\n",
    "base.gradient_checkpointing_enable(); base.config.use_cache=False\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "        task_type=\"SEQ_2_SEQ_LM\", r=8, lora_alpha=32, lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"])\n",
    "\n",
    "model = get_peft_model(base, lora_cfg).to(DEVICE)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(CKPT_DIR, use_fast=False)\n",
    "tok.pad_token = tok.eos_token\n",
    "model.resize_token_embeddings(len(tok))\n",
    "\n",
    "# Value‑head wrappers for trl\n",
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\n",
    "              model, peft_config=lora_cfg).to(DEVICE)\n",
    "ppo_ref   = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(\n",
    "              model, peft_config=lora_cfg).to(DEVICE).eval()\n",
    "for p in ppo_ref.parameters(): p.requires_grad=False\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3) tiny sample dataset (200 rows)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "df = pd.read_csv(\"/content/clinical_notes.csv\")[[\"dialogue\",\"note\"]]\n",
    "\n",
    "class NoteSet(Dataset):\n",
    "    def __init__(self, frame, tok, L=512):\n",
    "        self.f   = frame.reset_index(drop=True)\n",
    "        self.tok = tok; self.L = L\n",
    "    def __len__(self):  return len(self.f)\n",
    "    def __getitem__(self, i):\n",
    "        txt = str(self.f.iloc[i][\"dialogue\"])\n",
    "        enc = self.tok(txt, truncation=True, max_length=self.L,\n",
    "                       return_tensors=\"pt\")\n",
    "        return {\"input_ids\":enc[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\":enc[\"attention_mask\"].squeeze(),\n",
    "                \"src_txt\":txt}\n",
    "\n",
    "loader = DataLoader(NoteSet(df.sample(200,random_state=0),tok),\n",
    "                    batch_size=2, shuffle=True, pin_memory=True)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4) PPO trainer (trl‑0.7.1 only needs bs & mb_size)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "ppo_cfg = PPOConfig(batch_size=2, mini_batch_size=2, output_dir='/content/ppologs')\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, ppo_model.parameters()), lr=2e-5)\n",
    "\n",
    "ppo_trainer = PPOTrainer(ppo_cfg, ppo_model,\n",
    "                         ref_model = ppo_ref,\n",
    "                        )\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5) PC‑Grad utilities\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def pc_merge(flat_grads):\n",
    "    for i in range(len(flat_grads)):\n",
    "        for j in range(i+1, len(flat_grads)):\n",
    "            dot = torch.dot(flat_grads[i], flat_grads[j])\n",
    "            if dot < 0:\n",
    "                flat_grads[i] -= (dot / (flat_grads[j].norm()**2 + 1e-12)) * flat_grads[j]\n",
    "    return torch.stack(flat_grads).mean(0)\n",
    "\n",
    "def flat_param_grads(model):\n",
    "    return torch.cat([p.grad.flatten() for p in model.parameters()\n",
    "                      if p.grad is not None])\n",
    "\n",
    "def scatter_flat_grads(model, flat):\n",
    "    idx = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None: continue\n",
    "        n = p.grad.numel()\n",
    "        p.grad.data = flat[idx:idx+n].view_as(p).clone()\n",
    "        idx += n\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 6)   PPO  ✕  PC‑Grad  ✕  UniEval  training loop\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "gen_kwargs = dict(max_new_tokens=64, do_sample=True,\n",
    "                  pad_token_id=tok.eos_token_id)\n",
    "\n",
    "for ep in range(3):\n",
    "    for batch in loader:\n",
    "        queries = batch[\"input_ids\"].to(DEVICE)\n",
    "        attn    = batch[\"attention_mask\"].to(DEVICE)\n",
    "        src_txt = batch[\"src_txt\"]\n",
    "\n",
    "        # rollout\n",
    "        with torch.no_grad():\n",
    "            responses = ppo_model.generate(queries, attention_mask=attn,\n",
    "                                           **gen_kwargs)\n",
    "\n",
    "        hyp_txt = tok.batch_decode(responses, skip_special_tokens=True)\n",
    "\n",
    "        # 4‑way reward (CPU → GPU)\n",
    "        R = unieval_4way(src_txt, hyp_txt).to(DEVICE)   # (B,4)\n",
    "\n",
    "        # collect per‑objective grads\n",
    "        flat_grads = []\n",
    "        for k in range(4):\n",
    "            rew_k = R[:, k]\n",
    "            loss_k, *_ = ppo_trainer._loss(queries, responses, rew_k)\n",
    "            optimizer.zero_grad()\n",
    "            loss_k.backward(retain_graph=True)\n",
    "            flat_grads.append(flat_param_grads(ppo_model))\n",
    "\n",
    "        # PC‑Grad merge → apply\n",
    "        merged = pc_merge(flat_grads)\n",
    "        scatter_flat_grads(ppo_model, merged)\n",
    "        torch.nn.utils.clip_grad_norm_(ppo_model.parameters(), 1.0)\n",
    "        optimizer.step(); optimizer.zero_grad()\n",
    "\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "    print(f\"✓ epoch {ep+1}/3 finished\")\n",
    "\n",
    "print(\"done ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MTzsJ2K7Pfsj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvZRsRV78ayo",
    "outputId": "e28dd501-0b75-494c-b72c-f52a8aeccf26"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet --no-cache-dir \\\n",
    "    \"transformers==4.39.3\" \\\n",
    "    \"trl==0.7.1\" \\\n",
    "    bitsandbytes peft datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 636
    },
    "id": "yQ5zA6EaBLzt",
    "outputId": "11965980-fd1f-4fde-c75c-64fafbc57eb8"
   },
   "outputs": [],
   "source": [
    "import importlib, pkg_resources, sys, warnings\n",
    "print(\"transformers →\", pkg_resources.get_distribution(\"transformers\").version)\n",
    "print(\"trl           →\", pkg_resources.get_distribution(\"trl\").version)\n",
    "\n",
    "# quick sanity check that failed before\n",
    "from trl import PPOConfig\n",
    "warnings.filterwarnings(\"ignore\")   # suppress the cuda‑cache msg\n",
    "print(\"✅  PPOConfig imported – versions are compatible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crhGuWFcUK65",
    "outputId": "3952adc4-a960-4509-ae98-ab89a6bf5289"
   },
   "outputs": [],
   "source": [
    "pip install -U --no‑cache‑dir \"transformers==4.37.0\" \"trl==0.7.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZSUB-dgUWR5",
    "outputId": "d264f741-e782-4033-a1fc-c8474a48dab6"
   },
   "outputs": [],
   "source": [
    "# 1️⃣  remove the too‑new wheel\n",
    "!pip uninstall -y transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "t4zUbejpUbao",
    "outputId": "6d2c6bed-99bd-4e93-eb86-d6aab66ce1c7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 2️⃣  install the last compatible release\n",
    "!pip install --no-cache-dir \"transformers==4.37.0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SclI37zCUiFs",
    "outputId": "88dfced7-84d2-4037-d55e-758b92afcaa2"
   },
   "outputs": [],
   "source": [
    "#  (optional) re‑install trl in case pip removed dependencies\n",
    "!pip install --no-cache-dir \"trl==0.7.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "d9tyKybeU43h",
    "outputId": "79529a96-0f48-4e5a-d979-dba6e5574228"
   },
   "outputs": [],
   "source": [
    "import importlib.util, transformers, peft\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"peft       :\", peft.__version__)\n",
    "\n",
    "from trl import PPOConfig\n",
    "print(\"✅  trl and peft now import together!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 914
    },
    "id": "A2V4GqHVVDvg",
    "outputId": "d416b2b8-1710-424b-cb99-9ad54386d96c"
   },
   "outputs": [],
   "source": [
    "pip install --no-cache-dir peft==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k070pMrnVSkg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (kshitij)",
   "language": "python",
   "name": "kshitij"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
