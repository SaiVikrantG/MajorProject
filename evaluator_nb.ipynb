{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ca6532-0d4b-4c70-a864-97180356f004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r\"UniEval\")\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "425cbe60-ec5f-4317-9639-128402850980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"D:\\hf-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c4c851-0398-4ab4-891f-f2b0ba4e3443",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'summarization'\n",
    "\n",
    "# Initialize evaluator for a specific task\n",
    "evaluator = get_evaluator(task)\n",
    "evaluator1 = get_evaluator('fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ee22457-5c2b-4fda-bf66-1a83828da77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 20 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 93/93 [01:06<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 20 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 93/93 [00:05<00:00, 15.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 20 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation scores are shown below:\n",
      "+-------------+----------+\n",
      "|  Dimensions |  Score   |\n",
      "+-------------+----------+\n",
      "| consistency | 0.64519  |\n",
      "|   fluency   | 0.865415 |\n",
      "|  relevance  | 0.931619 |\n",
      "|   overall   | 0.814075 |\n",
      "+-------------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load JSON data from a file\n",
    "with open('nlg_evaluation_data-12-4-cleaned.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "# Get multi-dimensional evaluation scores\n",
    "eval_scores = evaluator.evaluate(data, print_result=True, dims=['consistency', 'fluency', 'relevance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c1dfe3-d0df-4087-a495-231d8fd3cad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 20 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 93/93 [01:07<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation scores are shown below:\n",
      "+-------------+---------+\n",
      "|  Dimensions |  Score  |\n",
      "+-------------+---------+\n",
      "| consistency | 0.70914 |\n",
      "+-------------+---------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_scores1 = evaluator1.evaluate(data, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d1775-7187-4375-82e4-76a4d0fdfd90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (kshitij)",
   "language": "python",
   "name": "kshitij"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
