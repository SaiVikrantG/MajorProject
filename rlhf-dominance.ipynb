{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZL9wcvcle3iT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZL9wcvcle3iT",
    "outputId": "6926df5c-52dd-4601-8af5-f324fc285fd2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install trl==0.7.4\n",
    "# !pip install datasets\n",
    "# !pip install transformers==4.38.2\n",
    "# !pip install peft==0.10.0\n",
    "# !pip install accelerate==0.28.0\n",
    "# test commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf67fc5a-c458-484f-869a-bfb2154e6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"D:\\hf-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa80b0e",
   "metadata": {
    "id": "2fa80b0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "def set_seed(seed_val=42):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "#Configuration options\n",
    "train_batch_size = 16\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 1e-5\n",
    "eval_batch_size = 1\n",
    "eval_steps = 500\n",
    "max_input_length = 550\n",
    "save_steps = 1000\n",
    "num_train_epochs = 20\n",
    "random.seed(42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da6b3d82-34f1-4718-9d02-ef80e8ca1625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018c6ca",
   "metadata": {
    "id": "4018c6ca"
   },
   "source": [
    "## Creating the policy model for human Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58ccc7e5-8a0e-4fb5-8cc6-4d9f4b9cac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined_clinical_notes.csv\")\n",
    "\n",
    "# First split: 60% train, 40% temp (eval + test)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "\n",
    "# Second split: 50% of temp (which is 20% of total) for eval, 50% for test\n",
    "eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d885cf72-3d15-48a5-b1a3-8e885b2537a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "encounter_id\n",
      "dialogue\n",
      "note\n",
      "source_file\n",
      "id\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    print(column)  # Prints each column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0670ff5e",
   "metadata": {
    "id": "0670ff5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:09<00:00,  2.29s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1) 4-bit quant config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# 2) Load base model in 4-bit\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# 3) Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# 4) Enable gradient checkpointing\n",
    "# model.enable_input_require_grads()\n",
    "# model.gradient_checkpointing_enable()\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# 5) Prepare data with smaller sequence length\n",
    "# notechat = load_dataset(\"akemiH/NoteChat\")\n",
    "# trainB = notechat[\"train\"].select(range(3000))\n",
    "# evalB = notechat[\"train\"].select(range(3000, 3500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4efa4266-3b5d-4c72-a1ef-663004a4ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLDRDataset(Dataset):\n",
    "    def __init__(self, train_path, tokenizer, split, max_length=4000):\n",
    "        self.post_list = []\n",
    "        dataset = train_df\n",
    "        self.labels = []\n",
    "\n",
    "        for sample in dataset.iterrows():\n",
    "            self.post_list.append(sample[1][\"dialogue\"])\n",
    "            self.labels.append(sample[1][\"note\"])\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.post_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.post_list[idx]\n",
    "        summary = self.labels[idx]\n",
    "        # label = self.labels[idx]\n",
    "\n",
    "        # encodings_dict = self.tokenizer(txt, truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "        # input_ids = torch.tensor(encodings_dict[\"input_ids\"])\n",
    "        # attn_masks = torch.tensor(encodings_dict[\"attention_mask\"])\n",
    "        # return {\n",
    "        #     \"input_ids\": input_ids,\n",
    "        #     \"attention_mask\": attn_masks,\n",
    "        #     \"labels\": labels_ids,\n",
    "        # }\n",
    "\n",
    "        txt = f\"CONVERSATION:\\n{conversation}\\n\\nSUMMARY: \\n{summary}\" #IMPORTANT!!!!!!!!!!\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            txt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "         # Labels should be the same as input_ids for causal LM training\n",
    "        # The model will automatically shift labels internally\n",
    "        encodings[\"labels\"] = encodings[\"input_ids\"].clone()\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": encodings[\"labels\"].squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f0a73ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328,
     "referenced_widgets": [
      "c0d5a5aca66640fabb8e8b4c6fb59392",
      "0d238f7dc3544a248e568b01b2b8f68b",
      "360c56bdbb1a4cf698cd6ee02fc0377c",
      "ec18abfee9fb4920919fff399355653a",
      "17774ec2b2814c989e559645c66c16dc",
      "3e9a90d9c01e41faa39e765d077940ad",
      "a7a3771791064414b7baa55dea4f9307",
      "f9140d1590e4477fa2a0f6ee0f0a29d4",
      "4f64e2615eb34e9e8bb496845194c4f0",
      "cec141a0a52a44029af02766578a7a86",
      "dd692ebfdd374157a28212e3c58dfdee",
      "edb52e11236a466ba3180a6700fcbe47",
      "d8806f44019a4be995bf8a00575a884c",
      "cd702006f8d64566b59db92ced9fa9a8",
      "010b68f7a4e74db1a691e73c6c0b668d",
      "effdcdf181e4451695eeef03a6994093",
      "d8d51ab046d8479fb6c55d58beaeab69",
      "dce9a626e1ac48faa5b3da071cf39bf1",
      "9a88ceb27ac54478b1806f1417e3f56e",
      "4b1628d06c674e7e9542e4de56942440",
      "5b80c804ea87468aa681a4346c9cfb3e",
      "54e1f5359144444faad92a5af1208b20",
      "3c9db78225634f4ba56dd620875a6152",
      "6df03bc0feae4dcf8c0f5f539e837aa3",
      "98e9fab21039484cb336ce0f781d1f7f",
      "a7ee81d9ba61402fb8f18bb17416f640",
      "3c7c8b864d5942a58a476a7123e4c919",
      "24d9d364d21b47d8b19217cf28958b98",
      "4ff64e6f540e4900b40e564330dd0192",
      "e408dbc1a67a4dcf89e3876437a35da2",
      "697def36105e47f987bc8595102a45ce",
      "8b78c25aecda45db921b0351401d0bf9",
      "d08ee3cbabfd44819328f076bb075337",
      "68cb590000b94ab397dc97cdfc05c4b6",
      "154be340981b436c9caf3289aa6a39b4",
      "f088edd1f0474d1a97ad0c008f4cfd43",
      "76104008746b45a59c7065a90fef5ffd",
      "09c0865087fb4cadb82bedd9e4fe224f",
      "32467bbd99394b86b3d5db9cc6572791",
      "b9f148a70e67439aa7208c8580d3f843",
      "f323e77044ae48c6a3d88ea620735366",
      "37429d08cb6a48adadab9e1459d274d2",
      "cde30c20c0774408851a356a23412864",
      "e3b0bb095a1e405cbc48908ca48bdb7b",
      "69a7a00f8f1b48948ada90f62fcdc9a6",
      "04dd6f2423b04009b0b53e45eb24b1a7",
      "74734805983e42c9906bf8cd10b2974e",
      "fc633082a04e4480bafec26d1c2ccb7a",
      "bb9ddba4dd9b4ddbbf65cfa91f2b521d",
      "281ed0eb1bed44cb837b0977b6c50e0b",
      "c9587d181b8f499ea983f53d38e4d1f5",
      "e6e5ade438ab45c8a05e9f68a1838380",
      "7db18b1334424e84ac6c64003b7a91ed",
      "25ca2e5e59134e45ae4addccde4dc618",
      "ee3c992c640c481cbdb5927cb5b113da",
      "92382a78fc554b2a94f3a99fa064bf3a",
      "437363a2959441e19842e19b67388db1",
      "d8a8c1f8b5a347408ffe1bed07e87c6d",
      "00ebae8e2b0a46539b3bc297976a1f35",
      "dd35c1969a73488789164f4c59d7b1de",
      "b2b1a073a7b14f759bda8df3bae6db1f",
      "ffe0b0cc000444c98d0d43a1f9d4843c",
      "6ea4187ab5124426b662badc73ac1b10",
      "b2e22b238f214dca87827d6b3b5eaeca",
      "d29cfa7e295f4949b100d3a21126174f",
      "2850146e7d3f42109539343d97c936c6",
      "666db4066bec4d7581cc19e6f5bc98bc",
      "72df76a9cdf8459cb63f67d992b10749",
      "f6409c1155eb4b92998e20999b642c04",
      "58f0850759b8457b9657d36b326ddf2e",
      "1bc25a94b13b4189b8fc3f7ce41b9086",
      "6ee1b4bc76cb4106962fb29f05cc0825",
      "76d96ad5ee9344978487022753a3adfa",
      "6f272cac47734ea9abb212d32a65c382",
      "a5ec7fadd5bc439db5119626b8e6afbb",
      "71e7360d713c4c70a33e9fd9e0cb8412",
      "c21e2109534b4e1cb176332e1db6e59f",
      "7a473fedaf4645cdaf78189b1bfbdd6d",
      "5afba4b64a114f948309aa11d3ba5c34",
      "2accfcbededc4afa817fc4beedaee153",
      "477d5050997045318236932dad8d8418",
      "cfa386b4863f4fd29120e79be0e317ce",
      "788c6efc3c41484d9559b57690401142",
      "b026473caa4f4234b8b90c2d7965115b",
      "b6109109cd9746dfb361860ae6d26cac",
      "ee699d35db9d4b6ab0a74713ad5929d4",
      "812bf7bc5a6f4985b0ecc0de236b45a0",
      "c592e8dcb00349c6817b05382263c784"
     ]
    },
    "id": "7f0a73ab",
    "outputId": "24038f05-5422-492e-9a38-dca119a91b2d"
   },
   "outputs": [],
   "source": [
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# model.config.end_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Disable caching (already done, but double-check)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Enable gradient checkpointing (already done, but confirm)\n",
    "# model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbc6aad5",
   "metadata": {
    "id": "dbc6aad5"
   },
   "outputs": [],
   "source": [
    "# Set up the datasets\n",
    "data_path = \"NA\"\n",
    "train_dataset = TLDRDataset(\n",
    "    data_path,\n",
    "    tokenizer,\n",
    "    \"train\",\n",
    "    max_length=256,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f16a19aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f16a19aa",
    "outputId": "37314f8f-f319-4d58-d968-cfc62bf5ab89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5790, 72226,  3495,   510,    58, 36983,    60,   773, 13580,  1536,\n",
      "          374,   264,   220,    20,    21,   481,  3157,  6284,  8593,   879,\n",
      "        18404,  3351, 41026,   315, 16829, 90647,   323,   702,   264,  3267,\n",
      "         6457,  3840,   315, 19754,   323,  1550,  6543,  7262,   773, 13580,\n",
      "         1536,  3291,   752,  1128,   594,  2087,   389,   448,   697, 90647,\n",
      "         3491,   319,    58, 22722,    60,  1632,   432,   594,  1101,  1012,\n",
      "         2494,   429,   600,   614, 13686,   916,   279,  1537,   498,  1414,\n",
      "         3040,   311,  4236,  5555,   432,  4977,  1075,  2494,   374,  2677,\n",
      "        15700,   304,   847, 27591,   323,   498,  1414,  1101,   600, 33390,\n",
      "          728,   911,   847,  1899,   323,   432, 33390, 81354,   752,   498,\n",
      "         1414,   264,  2632,  2699,  7025,   264,  2632, 25118,   323,   979,\n",
      "          600,  1430,   311,  8180,   432,  1101,  4977,  1075,   600,   614,\n",
      "          311,   600,   653,   308,   944,  1414,   979,   432,   594,  2494,\n",
      "         6437,   600, 33390,   614,   311, 11369,   432,  1495,   389, 13101,\n",
      "          600,   600,  3003,  2581,  2167,  7225, 93643,   714,   600,   614,\n",
      "          311, 16698,   429,   600,  3003,  3381,   911, 13581, 90520,   319,\n",
      "           58, 36983,    60, 16910,   323,   374,   432,  2803,   311,   387,\n",
      "        15700,   304,   697, 27591,   476,  1558,   432,  2803,   311,   728,\n",
      "          304,   697, 44174,  3082,   476,   525,   498, 39600,   287,   448,\n",
      "          419,   518,   678,   476,  1101, 10008,   979,   498,  2299, 90647,\n",
      "          319,    58, 22722,    60,   902, 39600,   287,  1101, 33390,   979,\n",
      "          600, 41176,   432,   594,   678, 16910,   448, 66749,   714,   979,\n",
      "          600, 33390, 41176,   498,  1414,  1075,   264, 22721,   315,   264,\n",
      "        27874,  7025,   432,  1101, 11074,  1075,  1075,   600,  1101,  1184,\n",
      "          264,  2632,  1492, 17461,   432,  1495,   319,    58, 36983,    60,\n",
      "        16910,   323,   374,   432,   803,  3093]) tensor([ 5790, 72226,  3495,   510,    58, 36983,    60,   773, 13580,  1536,\n",
      "          374,   264,   220,    20,    21,   481,  3157,  6284,  8593,   879,\n",
      "        18404,  3351, 41026,   315, 16829, 90647,   323,   702,   264,  3267,\n",
      "         6457,  3840,   315, 19754,   323,  1550,  6543,  7262,   773, 13580,\n",
      "         1536,  3291,   752,  1128,   594,  2087,   389,   448,   697, 90647,\n",
      "         3491,   319,    58, 22722,    60,  1632,   432,   594,  1101,  1012,\n",
      "         2494,   429,   600,   614, 13686,   916,   279,  1537,   498,  1414,\n",
      "         3040,   311,  4236,  5555,   432,  4977,  1075,  2494,   374,  2677,\n",
      "        15700,   304,   847, 27591,   323,   498,  1414,  1101,   600, 33390,\n",
      "          728,   911,   847,  1899,   323,   432, 33390, 81354,   752,   498,\n",
      "         1414,   264,  2632,  2699,  7025,   264,  2632, 25118,   323,   979,\n",
      "          600,  1430,   311,  8180,   432,  1101,  4977,  1075,   600,   614,\n",
      "          311,   600,   653,   308,   944,  1414,   979,   432,   594,  2494,\n",
      "         6437,   600, 33390,   614,   311, 11369,   432,  1495,   389, 13101,\n",
      "          600,   600,  3003,  2581,  2167,  7225, 93643,   714,   600,   614,\n",
      "          311, 16698,   429,   600,  3003,  3381,   911, 13581, 90520,   319,\n",
      "           58, 36983,    60, 16910,   323,   374,   432,  2803,   311,   387,\n",
      "        15700,   304,   697, 27591,   476,  1558,   432,  2803,   311,   728,\n",
      "          304,   697, 44174,  3082,   476,   525,   498, 39600,   287,   448,\n",
      "          419,   518,   678,   476,  1101, 10008,   979,   498,  2299, 90647,\n",
      "          319,    58, 22722,    60,   902, 39600,   287,  1101, 33390,   979,\n",
      "          600, 41176,   432,   594,   678, 16910,   448, 66749,   714,   979,\n",
      "          600, 33390, 41176,   498,  1414,  1075,   264, 22721,   315,   264,\n",
      "        27874,  7025,   432,  1101, 11074,  1075,  1075,   600,  1101,  1184,\n",
      "          264,  2632,  1492, 17461,   432,  1495,   319,    58, 36983,    60,\n",
      "        16910,   323,   374,   432,   803,  3093])\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataset:\n",
    "    print(i[\"input_ids\"], i[\"labels\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "500301ac",
   "metadata": {
    "id": "500301ac"
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "rPWzhVtfjXQf",
   "metadata": {
    "id": "rPWzhVtfjXQf"
   },
   "outputs": [],
   "source": [
    "output_dir = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune-logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3801788",
   "metadata": {
    "id": "d3801788"
   },
   "outputs": [],
   "source": [
    "# Prepare the trainer and start training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "#     per_device_eval_batch_size=eval_batch_size,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_drop_last=True,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baefacaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baefacaf",
    "outputId": "917e4e82-2c45-4cc7-da64-356b9f5a5dfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.device.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90ba4710",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "90ba4710",
    "outputId": "398ff6cf-790c-4b2c-eae7-c4e92a1776f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\accelerate\\accelerator.py:463: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 01:23, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=2.3734092712402344, metrics={'train_runtime': 111.3858, 'train_samples_per_second': 4.992, 'train_steps_per_second': 0.036, 'total_flos': 5564529698144256.0, 'train_loss': 2.3734092712402344, 'epoch': 1.88})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     data_collator=default_data_collator,\n",
    "#     preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "# trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dfc50c2",
   "metadata": {
    "id": "3dfc50c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\")   ##path to save policy model\n",
    "tokenizer.save_pretrained(r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\")\n",
    "model.save_pretrained(r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune-peft-adapaters\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b373c19c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b373c19c",
    "outputId": "caf7af75-5a02-47b4-b1ef-d5421c6e5ae8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:14<00:00,  3.54s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   58, 36983,    60, 15588,  1154, 23902,  1168,   659,  1246,   525,\n",
       "           498,   937,   319,    58, 22722,    60,   600,  2776,  1661,   659,\n",
       "          1246,   911,   498,   937,   319,    58, 36983,    60,   600,  2776,\n",
       "          1661,   659,   773,   512,    12,   525,   498,  5527,   311,   633,\n",
       "          3855,   937,   319,    58, 22722,    60,   600,  1079, 79141,    58,\n",
       "         36983,    60, 16910,   659, 23902,  1168,   374,   264,   220,    20,\n",
       "            21,  4666,  6284,  8593,  1588,   448, 34563, 10072, 14613,   659,\n",
       "           773,  1154,   600,  3003,  6617,   498,  1033,   304,   279,  2714,\n",
       "          1154, 23902,  1168,  1154,   323,   807,  1730,   429,   498,  1030,\n",
       "           264,  3347, 17280, 93755, 79141,    58, 22722,    60, 75446, 79141,\n",
       "            58, 36983,    60,  1033,   498,  3432,  1045,   294, 96645,   323,\n",
       "          1045,   326,  1090, 11417,   291,  2090,   937,   319,    58, 22722,\n",
       "            60,   600,   572,  1602,   326,  1090, 11417,   291,   659,   600,\n",
       "            12,   600,   653,   308,   944,  1414,   659,  1602,   326,  1090,\n",
       "         11417,   291, 79141,    58, 36983,    60, 16910,   659,   323,   614,\n",
       "           498, 13686, 34663,   504, 12379,   937,   319,    58, 22722,    60,\n",
       "           600,   614,   537,   659,   600,   614,   308,   944, 12898,  7037,\n",
       "           304,  5008,   264,  1393,   659,  7196,   264,  8112, 15988,   504,\n",
       "           264, 21430,  1393, 96287,  1045, 44597,  1154,   714,  4302,   803,\n",
       "          1091,   429, 79141,    58, 36983,    60,   714,   902,  6543,   304,\n",
       "           697, 89772,   476, 69127,    58, 22722,    60,   902, 79141,    58,\n",
       "         36983,    60,   659,  4113,  1075,   429,   937,   319,    58, 22722,\n",
       "            60,   902, 79141,    58, 36983,    60, 16910,   659,   323,   894,\n",
       "           943,   315,  4680,  4709,   476, 24938, 37475,   476,  3729, 10546,\n",
       "          1862,   937, 39600,    82,   937,   319,    58, 22722,    60, 43744,\n",
       "          1154,   274,    12, 10078, 24938, 37475]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune-logs\")\n",
    "model_path = \"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation=True, max_length=256, padding=\"max_length\")\n",
    "text = train_df.iloc[2][\"dialogue\"]\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\", max_length=256)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92769534",
   "metadata": {
    "id": "92769534"
   },
   "source": [
    "# Policy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da46d612-6e91-4dd6-a60b-2866f28c58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"D:\\hf-cache\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb9cb217",
   "metadata": {
    "id": "bb9cb217"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from trl import RewardTrainer\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d0acdb",
   "metadata": {
    "id": "50d0acdb"
   },
   "outputs": [],
   "source": [
    "##model path\n",
    "# MODEL_PATH = \"/content/drive/MyDrive/Medical Dialogue Summarization using PPO/rm_model\"\n",
    "MODEL_PATH = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e36852d7-2923-420a-a4da-c2464a7aecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af5cda-6919-4b1c-bedb-28f86f9c4d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████| 4/4 [00:07<00:00,  1.90s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n",
      "WARNING:root:The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from transformers import BitsAndBytesConfig  \n",
    "\n",
    "# ---- Device Setup ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---- Paths ----\n",
    "MODEL_PATH = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\"\n",
    "PEFT_ADAPTER_PATH = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune-peft-adapaters\"\n",
    "REF_MODEL_PATH = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\"\n",
    "\n",
    "# ---- 1) 4-bit Quantization Configuration ----\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# ---- 2) Load Base Model in 4-bit ----\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# Prepare the model for k-bit training (this typically freezes most parameters except adapter ones)\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "base_model.gradient_checkpointing_disable()  # Disable checkpointing\n",
    "\n",
    "# ---- 3) Load Tokenizer ----\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# ---- 4) Load the PEFT Adapter (LoRA) ----\n",
    "# This reloads your fine-tuned adapter weights onto your base model.\n",
    "model_with_lora = PeftModel.from_pretrained(base_model, PEFT_ADAPTER_PATH)\n",
    "\n",
    "# ---- 5) Convert to PPO-Compatible ValueHead Model ----\n",
    "# When converting, pass the peft_config from the adapter model to ensure proper initialization.\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model_with_lora,\n",
    "    peft_config=lora_config\n",
    ").to(device)\n",
    "\n",
    "# ---- 6) Optionally, Load a Reference Model for KL (e.g., reward model) ----\n",
    "ppo_model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model_with_lora,\n",
    ").to(device)\n",
    "ppo_model_ref.eval()  # Disable dropout/etc\n",
    "for param in ppo_model_ref.parameters():\n",
    "    param.requires_grad = False  # Freeze all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051a38d5-02b9-456c-9cbc-31f07bf6f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starcoder_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH) ## tokenizer of step 1 model., here since we are using same model for step 1 and 2 it doesnot matter\n",
    "# starcoder_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)\n",
    "tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"left\"\n",
    "model.config.pad_token_id = tok.eos_token_id\n",
    "model.resize_token_embeddings(len(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05f02097",
   "metadata": {
    "id": "05f02097"
   },
   "outputs": [],
   "source": [
    "sentiment_pipe_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\"}\n",
    "project_kwargs={\"logging_dir\": r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-rl-logs\"}\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=MODEL_PATH, ppo_epochs=1, project_kwargs=project_kwargs, gradient_accumulation_steps=2, steps=5, batch_size=2, mini_batch_size=1, learning_rate=2e-5, log_with='tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1a2e38",
   "metadata": {
    "id": "2e1a2e38"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "# optimizer = torch.optim.SGD(starcoder_model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "optimizer = bnb.optim.Adam8bit(ppo_model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "ppo_trainer = PPOTrainer(config, ppo_model,ppo_model_ref, tok, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5493f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "df = pd.read_csv(\"combined_clinical_notes.csv\")[[\"dialogue\", \"note\"]]\n",
    "\n",
    "class ClinDS(Dataset):\n",
    "    def __init__(self, df, tok, max_len=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tok = tok\n",
    "        self.L = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        conv = str(self.df.iloc[i][\"dialogue\"])\n",
    "        ref = str(self.df.iloc[i][\"note\"])\n",
    "        prompt = f\"Summarize the following conversation:\\n\\n{conv}\"\n",
    "        enc = self.tok(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.L,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc.input_ids.squeeze(),\n",
    "            \"attention_mask\": enc.attention_mask.squeeze(),\n",
    "            \"src_txt\": prompt,\n",
    "            \"ref_txt\": ref,\n",
    "        }\n",
    "\n",
    "loader = DataLoader(\n",
    "    ClinDS(df.sample(200, random_state=0), tok),\n",
    "    batch_size=1, shuffle=True, pin_memory=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3366a32e-601a-44f8-9dbd-6038c72b23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974cb44",
   "metadata": {
    "id": "2974cb44"
   },
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tok.eos_token_id,\n",
    "    \"max_new_tokens\": 32,\n",
    "    \"eos_token_id\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d3a744ce-d939-49a2-9914-94d853750476",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDICAL_PROMPT = \"\"\"\n",
    "Please generate a medical summary based on the following clinical notes. The summary should include the following sections: \n",
    "\n",
    "CHIEF COMPLAINT\n",
    "A concise statement of the patient's primary concern or reason for visiting the clinic.\n",
    "\n",
    "HISTORY OF PRESENT ILLNESS\n",
    "A detailed narrative about the patient's symptoms, their onset, duration, and any relevant medical history or previous treatments.\n",
    "\n",
    "VITALS\n",
    "Include any relevant vital signs (e.g., oxygen saturation, blood pressure) if available.\n",
    "\n",
    "PHYSICAL EXAM \n",
    "Summarize the findings from the physical examination, including any notable abnormalities.\n",
    "\n",
    "RESULTS \n",
    "Summarize the results of any diagnostic tests performed (e.g., lab work, imaging studies).\n",
    "\n",
    "ASSESSMENT\n",
    "The doctor's assessment of the patient's condition or diagnosis.\n",
    "\n",
    "PLAN\n",
    "The treatment plan, including prescribed medications, lifestyle recommendations, and follow-up instructions.\n",
    "\n",
    "INSTRUCTIONS\n",
    "Specific instructions for the patient regarding their treatment plan and follow-up care.\n",
    "\n",
    "Important Note: If any section lacks relevant information, omit that section from the generated summary. Only include sections for which there is sufficient information available.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b205a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")\n",
    "\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "sum_eval = get_evaluator(\"summarization\", device=\"cpu\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def unieval_4way(src, hyp, ref):\n",
    "    \"\"\"\n",
    "    src, hyp, ref: lists of strings, length B\n",
    "    returns: Tensor (B,4) with [coherence, consistency, fluency, relevance]\n",
    "    \"\"\"\n",
    "    data = convert_to_json(\n",
    "        output_list=hyp,\n",
    "        src_list=src,\n",
    "        ref_list=ref,\n",
    "    )\n",
    "    raw = sum_eval.evaluate(data, print_result=True)\n",
    "    scores = [\n",
    "        [d[\"coherence\"], d[\"consistency\"], d[\"fluency\"], d[\"relevance\"]]\n",
    "        for d in raw\n",
    "    ]\n",
    "    return torch.tensor(scores, dtype=torch.float32)  # CPU (B,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d6cc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "253d6cc8",
    "outputId": "696512ba-ab95-481c-e4d8-ef7d3065b28f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.17it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|███████████▏                                                             | 2/13 [00:00<00:00, 12.74it/s]\u001b[A\n",
      " 31%|██████████████████████▍                                                  | 4/13 [00:00<00:00, 13.25it/s]\u001b[A\n",
      " 46%|█████████████████████████████████▋                                       | 6/13 [00:00<00:00, 13.34it/s]\u001b[A\n",
      " 62%|████████████████████████████████████████████▉                            | 8/13 [00:00<00:00, 13.39it/s]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████▍                | 10/13 [00:00<00:00, 13.45it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 13.56it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|███████████▏                                                             | 2/13 [00:00<00:00, 13.37it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 41.50it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating factual consistency of 2 samples !!!\n",
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|█████▌                                                                   | 1/13 [00:00<00:08,  1.38it/s]\u001b[A\n",
      " 15%|███████████▏                                                             | 2/13 [00:01<00:08,  1.37it/s]\u001b[A\n",
      " 23%|████████████████▊                                                        | 3/13 [00:02<00:07,  1.36it/s]\u001b[A\n",
      " 31%|██████████████████████▍                                                  | 4/13 [00:02<00:06,  1.38it/s]\u001b[A\n",
      " 38%|████████████████████████████                                             | 5/13 [00:03<00:05,  1.38it/s]\u001b[A\n",
      " 46%|█████████████████████████████████▋                                       | 6/13 [00:04<00:05,  1.39it/s]\u001b[A\n",
      " 54%|███████████████████████████████████████▎                                 | 7/13 [00:05<00:04,  1.39it/s]\u001b[A\n",
      " 62%|████████████████████████████████████████████▉                            | 8/13 [00:05<00:03,  1.39it/s]\u001b[A\n",
      " 69%|██████████████████████████████████████████████████▌                      | 9/13 [00:06<00:02,  1.39it/s]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████▍                | 10/13 [00:07<00:02,  1.38it/s]\u001b[A\n",
      " 85%|████████████████████████████████████████████████████████████▉           | 11/13 [00:07<00:01,  1.38it/s]\u001b[A\n",
      " 92%|██████████████████████████████████████████████████████████████████▍     | 12/13 [00:08<00:00,  1.39it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 13/13 [00:09<00:00,  1.41it/s]\u001b[A\n",
      "  2%|█▌                                                                    | 1/46 [05:48<4:21:13, 348.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 23.73it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/19 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|███▊                                                                     | 1/19 [00:00<00:15,  1.14it/s]\u001b[A\n",
      " 11%|███████▋                                                                 | 2/19 [00:01<00:14,  1.13it/s]\u001b[A\n",
      " 16%|███████████▌                                                             | 3/19 [00:02<00:14,  1.14it/s]\u001b[A\n",
      " 21%|███████████████▎                                                         | 4/19 [00:03<00:13,  1.14it/s]\u001b[A\n",
      " 26%|███████████████████▏                                                     | 5/19 [00:04<00:12,  1.14it/s]\u001b[A\n",
      " 32%|███████████████████████                                                  | 6/19 [00:05<00:11,  1.14it/s]\u001b[A\n",
      " 37%|██████████████████████████▉                                              | 7/19 [00:06<00:10,  1.14it/s]\u001b[A\n",
      " 42%|██████████████████████████████▋                                          | 8/19 [00:07<00:09,  1.14it/s]\u001b[A\n",
      " 47%|██████████████████████████████████▌                                      | 9/19 [00:07<00:08,  1.14it/s]\u001b[A\n",
      " 53%|█████████████████████████████████████▉                                  | 10/19 [00:08<00:07,  1.14it/s]\u001b[A\n",
      " 58%|█████████████████████████████████████████▋                              | 11/19 [00:09<00:07,  1.14it/s]\u001b[A\n",
      " 63%|█████████████████████████████████████████████▍                          | 12/19 [00:10<00:06,  1.14it/s]\u001b[A\n",
      " 68%|█████████████████████████████████████████████████▎                      | 13/19 [00:11<00:05,  1.14it/s]\u001b[A\n",
      " 74%|█████████████████████████████████████████████████████                   | 14/19 [00:12<00:04,  1.14it/s]\u001b[A\n",
      " 79%|████████████████████████████████████████████████████████▊               | 15/19 [00:13<00:03,  1.14it/s]\u001b[A\n",
      " 84%|████████████████████████████████████████████████████████████▋           | 16/19 [00:14<00:02,  1.14it/s]\u001b[A\n",
      " 89%|████████████████████████████████████████████████████████████████▍       | 17/19 [00:14<00:01,  1.14it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████████████████████████████████▏   | 18/19 [00:15<00:00,  1.14it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 19/19 [00:16<00:00,  1.16it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/19 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|███▊                                                                     | 1/19 [00:00<00:15,  1.14it/s]\u001b[A\n",
      " 11%|███████▋                                                                 | 2/19 [00:01<00:14,  1.14it/s]\u001b[A\n",
      " 32%|███████████████████████                                                  | 6/19 [00:01<00:02,  4.50it/s]\u001b[A\n",
      " 47%|██████████████████████████████████▌                                      | 9/19 [00:01<00:01,  7.15it/s]\u001b[A\n",
      " 68%|█████████████████████████████████████████████████▎                      | 13/19 [00:02<00:00, 11.45it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 19/19 [00:02<00:00,  8.38it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating factual consistency of 2 samples !!!\n",
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/19 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|███▊                                                                     | 1/19 [00:01<00:25,  1.43s/it]\u001b[A\n",
      " 11%|███████▋                                                                 | 2/19 [00:09<01:33,  5.48s/it]\u001b[A\n",
      " 16%|███████████▌                                                             | 3/19 [00:11<00:57,  3.60s/it]\u001b[A\n",
      " 21%|███████████████▎                                                         | 4/19 [00:16<01:05,  4.39s/it]\u001b[A\n",
      " 26%|███████████████████▏                                                     | 5/19 [00:19<00:51,  3.66s/it]\u001b[A\n",
      " 32%|███████████████████████                                                  | 6/19 [00:21<00:43,  3.34s/it]\u001b[A\n",
      " 37%|██████████████████████████▉                                              | 7/19 [00:24<00:36,  3.04s/it]\u001b[A\n",
      " 42%|██████████████████████████████▋                                          | 8/19 [00:30<00:45,  4.14s/it]\u001b[A\n",
      " 47%|██████████████████████████████████▌                                      | 9/19 [00:33<00:36,  3.60s/it]\u001b[A\n",
      " 53%|█████████████████████████████████████▉                                  | 10/19 [00:39<00:40,  4.50s/it]\u001b[A\n",
      " 58%|█████████████████████████████████████████▋                              | 11/19 [00:42<00:30,  3.86s/it]\u001b[A\n",
      " 63%|█████████████████████████████████████████████▍                          | 12/19 [00:48<00:32,  4.66s/it]\u001b[A\n",
      " 68%|█████████████████████████████████████████████████▎                      | 13/19 [00:50<00:23,  3.98s/it]\u001b[A\n",
      " 74%|█████████████████████████████████████████████████████                   | 14/19 [00:57<00:23,  4.73s/it]\u001b[A\n",
      " 79%|████████████████████████████████████████████████████████▊               | 15/19 [00:59<00:16,  4.04s/it]\u001b[A\n",
      " 84%|████████████████████████████████████████████████████████████▋           | 16/19 [01:06<00:14,  4.78s/it]\u001b[A\n",
      " 89%|████████████████████████████████████████████████████████████████▍       | 17/19 [01:08<00:08,  4.07s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████████████████████████████████▏   | 18/19 [01:15<00:04,  4.79s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 19/19 [01:20<00:00,  4.26s/it]\u001b[A\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py:1266: UserWarning: KL divergence is starting to become negative: -541.01 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
      "  warnings.warn(\n",
      "  4%|███                                                                   | 2/46 [18:38<7:17:37, 596.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.00it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|█████▌                                                                   | 1/13 [00:00<00:10,  1.12it/s]\u001b[A\n",
      " 15%|███████████▏                                                             | 2/13 [00:01<00:09,  1.13it/s]\u001b[A\n",
      " 23%|████████████████▊                                                        | 3/13 [00:02<00:08,  1.13it/s]\u001b[A\n",
      " 31%|██████████████████████▍                                                  | 4/13 [00:03<00:07,  1.14it/s]\u001b[A\n",
      " 38%|████████████████████████████                                             | 5/13 [00:04<00:07,  1.14it/s]\u001b[A\n",
      " 46%|█████████████████████████████████▋                                       | 6/13 [00:05<00:06,  1.14it/s]\u001b[A\n",
      " 54%|███████████████████████████████████████▎                                 | 7/13 [00:06<00:05,  1.14it/s]\u001b[A\n",
      " 62%|████████████████████████████████████████████▉                            | 8/13 [00:07<00:04,  1.14it/s]\u001b[A\n",
      " 69%|██████████████████████████████████████████████████▌                      | 9/13 [00:07<00:03,  1.14it/s]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████▍                | 10/13 [00:08<00:02,  1.14it/s]\u001b[A\n",
      " 85%|████████████████████████████████████████████████████████████▉           | 11/13 [00:09<00:01,  1.14it/s]\u001b[A\n",
      " 92%|██████████████████████████████████████████████████████████████████▍     | 12/13 [00:10<00:00,  1.14it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 13/13 [00:11<00:00,  1.17it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|█████▌                                                                   | 1/13 [00:00<00:10,  1.14it/s]\u001b[A\n",
      " 46%|█████████████████████████████████▋                                       | 6/13 [00:00<00:00,  7.87it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 13/13 [00:02<00:00,  6.44it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating factual consistency of 2 samples !!!\n",
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                 | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|█████▌                                                                   | 1/13 [00:01<00:17,  1.43s/it]\u001b[A\n",
      " 15%|███████████▏                                                             | 2/13 [00:09<01:00,  5.47s/it]\u001b[A\n",
      " 23%|████████████████▊                                                        | 3/13 [00:11<00:35,  3.60s/it]\u001b[A\n",
      " 31%|██████████████████████▍                                                  | 4/13 [00:16<00:39,  4.39s/it]\u001b[A\n",
      " 38%|████████████████████████████                                             | 5/13 [00:19<00:29,  3.66s/it]\u001b[A\n",
      " 46%|█████████████████████████████████▋                                       | 6/13 [00:21<00:23,  3.34s/it]\u001b[A\n",
      " 54%|███████████████████████████████████████▎                                 | 7/13 [00:24<00:18,  3.04s/it]\u001b[A\n",
      " 62%|████████████████████████████████████████████▉                            | 8/13 [00:30<00:20,  4.14s/it]\u001b[A\n",
      " 69%|██████████████████████████████████████████████████▌                      | 9/13 [00:33<00:14,  3.60s/it]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████▍                | 10/13 [00:39<00:13,  4.49s/it]\u001b[A\n",
      " 85%|████████████████████████████████████████████████████████████▉           | 11/13 [00:42<00:07,  3.86s/it]\u001b[A\n",
      " 92%|██████████████████████████████████████████████████████████████████▍     | 12/13 [00:48<00:04,  4.66s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████| 13/13 [00:54<00:00,  4.17s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from random import choices\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "for epoch in range(1):\n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        # Prepare inputs\n",
    "        ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attn_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        src_txt = batch[\"src_txt\"]  # list[str]\n",
    "        ref_txt = batch[\"ref_txt\"]  # list[str]\n",
    "\n",
    "        # Generate multiple candidates per prompt\n",
    "        NUM_CANDIDATES = 1\n",
    "        all_outs = []\n",
    "        for _ in range(NUM_CANDIDATES):\n",
    "            with torch.no_grad():\n",
    "                # original_notes = src_txt\n",
    "            \n",
    "                # # Combine with medical prompt only during generation\n",
    "                # full_prompt = original_notes + \"\\n\\n\" + MEDICAL_PROMPT\n",
    "                # full_prompt_tensor = tok.encode(full_prompt, return_tensors=\"pt\").to(device).squeeze(0)\n",
    "                \n",
    "                out = ppo_model.generate(\n",
    "                    input_ids=ids,\n",
    "                    attention_mask=attn_mask,\n",
    "                    **gen_kwargs\n",
    "                )\n",
    "            all_outs.append(out)\n",
    "\n",
    "        # Stack outputs (B, K, L)\n",
    "        outs = torch.stack(all_outs, dim=1)\n",
    "\n",
    "        # Decode all candidates\n",
    "        hyps = [\n",
    "            [tok.decode(outs[b, k], skip_special_tokens=True)\n",
    "            for k in range(NUM_CANDIDATES)]\n",
    "            for b in range(outs.size(0))\n",
    "        ]\n",
    "\n",
    "        # Compute rewards using UniEval and dominance scoring\n",
    "        rewards = []\n",
    "        for b in range(len(src_txt)):\n",
    "            # Get scores for all candidates (K, 4)\n",
    "            scores = unieval_4way(\n",
    "                [src_txt[b]] * NUM_CANDIDATES,\n",
    "                hyps[b],\n",
    "                [ref_txt[b]] * NUM_CANDIDATES\n",
    "            ).numpy()\n",
    "\n",
    "            # Compute dominance counts\n",
    "            dom_counts = np.zeros(NUM_CANDIDATES)\n",
    "            for i in range(NUM_CANDIDATES):\n",
    "                for j in range(NUM_CANDIDATES):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    # Check if i dominates j\n",
    "                    if np.all(scores[i] >= scores[j]) and np.any(scores[i] > scores[j]):\n",
    "                        dom_counts[i] += 1\n",
    "\n",
    "            # Normalize to [-1, 1]\n",
    "            max_dom = NUM_CANDIDATES - 1\n",
    "            scalar_rewards = 2 * (dom_counts / max_dom) - 1\n",
    "            rewards.append(scalar_rewards)  \n",
    "\n",
    "        # Flatten for PPO\n",
    "        flat_queries = []\n",
    "        flat_responses = []\n",
    "        flat_rewards = []\n",
    "\n",
    "        for b in range(len(src_txt)):\n",
    "            for k in range(NUM_CANDIDATES):\n",
    "                flat_queries.append(ids[b])\n",
    "                flat_responses.append(outs[b, k])\n",
    "                flat_rewards.append(torch.tensor([rewards[b][k]], device=DEVICE))\n",
    "\n",
    "        # PPO step\n",
    "        stats = ppo_trainer.step(\n",
    "            queries=flat_queries,\n",
    "            responses=flat_responses,\n",
    "            scores=flat_rewards\n",
    "        )\n",
    "\n",
    "        # Logging\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}\")\n",
    "            print(f\"Sample output: {hyps[0][0][:100]}...\")\n",
    "            print(f\"Average reward: {np.mean([r.item() for r in flat_rewards]):.4f}\")\n",
    "\n",
    "    print(f\"✅ Epoch {epoch+1}/3 complete\")\n",
    "\n",
    "print(\"🎉 PPO fine-tuning done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bad48f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0bad48f",
    "outputId": "ccde4117-6c85-49d3-db21-e0d77a29cd45"
   },
   "outputs": [],
   "source": [
    "###saving the model\n",
    "# starcoder_model.save_pretrained(\"rhlfmodel/\")\n",
    "# starcoder_tokenizer.save_pretrained(\"rhlfmodel/\")\n",
    "\n",
    "ppo_trainer.model.pretrained_model.save_pretrained(\"D:\\kshitij-weights-folder\\qwen-aloe-rl-12-4-ppo-tuned\")\n",
    "tok.save_pretrained(\"D:\\kshitij-weights-folder\\qwen-aloe-rl-12-4-ppo-tuned\")\n",
    "\n",
    "if isinstance(ppo_trainer.model.pretrained_model, PeftModel):\n",
    "    ppo_trainer.model.pretrained_model.save_adapter(\n",
    "        \"D:/kshitij-weights-folder/qwen-aloe-rl-12-4-ppo-tuned-lora\",\n",
    "        \"lora_adapter\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48706b19-356f-463c-b07b-fb12cfb4c747",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "907f5127-9db1-4fbc-94cc-8eed816aefa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"D:\\hf-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dc0cb2c-cddb-46f3-bfaf-7187cb15c574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from trl import RewardTrainer\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig  \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a0c73a-f47b-4375-b6c6-794ca2f13cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>note</th>\n",
       "      <th>source_file</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>aci</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doctor] so tyler is a 56 -year-old male who p...</td>\n",
       "      <td>SUBJECTIVE\\r\\n\\r\\nDifficulty swallowing. Tyler...</td>\n",
       "      <td>src_experiment_data\\test1_aci_asrcorr.csv</td>\n",
       "      <td>ACI084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset encounter_id                                           dialogue  \\\n",
       "246     aci          NaN  [doctor] so tyler is a 56 -year-old male who p...   \n",
       "\n",
       "                                                  note  \\\n",
       "246  SUBJECTIVE\\r\\n\\r\\nDifficulty swallowing. Tyler...   \n",
       "\n",
       "                                   source_file      id  \n",
       "246  src_experiment_data\\test1_aci_asrcorr.csv  ACI084  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA = r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\"\n",
    "df = pd.read_csv(DATA)\n",
    "\n",
    "# First split: 60% train, 40% temp (eval + test)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "\n",
    "# Second split: 50% of temp (which is 20% of total) for eval, 50% for test\n",
    "eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16e0a26b-d34a-41c4-83f4-f2ee6b577e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 4/4 [00:07<00:00,  1.85s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# -- Path to your LoRA weights + tokenizer --\n",
    "model_dir = \"D:\\kshitij-weights-folder\\qwen-aloe-rl-9-4-ppo-tuned\"  \n",
    "\n",
    "# -- 4-bit quantization config (same as training) --\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# -- 1) Load the *base* Qwen2.5 model in 4-bit --\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "# model = PeftModel.from_pretrained(model, adapter_model_name)\n",
    "\n",
    "# -- 2) Load your fine-tuned LoRA adapters into the base model --\n",
    "# The directory should contain adapter_model.bin, adapter_config.json, etc.\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# -- 3) Load the tokenizer you saved to ./aloe_qwen --\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce493f64-d50b-47f0-9ea7-32275b714475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prompt(conv):\n",
    "    prompt = f\"\"\"\n",
    "    CONVERSATION:\n",
    "    {conv}\n",
    "    \n",
    "    SUMMARY:\n",
    "    Please generate a medical summary based on the following clinical notes. The summary should include the following sections: Chief Complaint, History of Present Illness, Vitals, Physical Exam, Results, Assessment, Plan, and Instructions.\n",
    "    Please format the response as plain text, without using markdown or special formatting, and with clear headings for each section, like this:\n",
    "    \n",
    "    \n",
    "    CHIEF COMPLAINT\n",
    "    A concise statement of the patient's primary concern or reason for visiting the clinic.\n",
    "    \n",
    "    HISTORY OF PRESENT ILLNESS\n",
    "    A detailed narrative about the patient's symptoms, their onset, duration, and any relevant medical history or previous treatments.\n",
    "    \n",
    "    VITALS\n",
    "    Include any relevant vital signs (e.g., oxygen saturation, blood pressure) if available.\n",
    "    \n",
    "    PHYSICAL EXAM \n",
    "    Summarize the findings from the physical examination, including any notable abnormalities.\n",
    "    \n",
    "    RESULTS \n",
    "    Summarize the results of any diagnostic tests performed (e.g., lab work, imaging studies).\n",
    "    \n",
    "    ASSESSMENT\n",
    "    The doctor's assessment of the patient's condition or diagnosis.\n",
    "    \n",
    "    PLAN\n",
    "    The treatment plan, including prescribed medications, lifestyle recommendations, and follow-up instructions.\n",
    "    \n",
    "    INSTRUCTIONS\n",
    "    Specific instructions for the patient regarding their treatment plan and follow-up care.\n",
    "    \n",
    "    Important Note: If any section lacks relevant information or if specific details are not provided (e.g., vitals are not mentioned, no abnormal findings in the physical exam), omit that section from the generated summary. Only include sections for which there is sufficient information available.\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89c025c7-a863-4c92-8a93-ee663ebe8b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>note</th>\n",
       "      <th>source_file</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>aci</td>\n",
       "      <td>D2N053</td>\n",
       "      <td>[doctor] so barbara i i know you are here for ...</td>\n",
       "      <td>CHIEF COMPLAINT\\r\\n\\r\\nItchy scalp pain.\\r\\n\\r...</td>\n",
       "      <td>challenge_data\\train.csv</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset encounter_id                                           dialogue  \\\n",
       "172     aci       D2N053  [doctor] so barbara i i know you are here for ...   \n",
       "\n",
       "                                                  note  \\\n",
       "172  CHIEF COMPLAINT\\r\\n\\r\\nItchy scalp pain.\\r\\n\\r...   \n",
       "\n",
       "                  source_file   id  \n",
       "172  challenge_data\\train.csv  NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = test_df\n",
    "\n",
    "eval_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfdfc560-3f25-4034-91cc-dc5d8dfbcfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Generating Summaries:   0%|                                                                      | 0/5 [00:00<?, ?it/s]C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:427: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating Summaries: 100%|█████████████████████████████████████████████████████████████| 5/5 [17:40<00:00, 212.09s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",  # Automatically place on GPU if available\n",
    ")\n",
    "\n",
    "# text_gen_pipeline = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=lora_model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device_map=\"auto\",   # place on GPU if available\n",
    "# )\n",
    "\n",
    "# Now generate text using the pipeline\n",
    "# response = text_gen_pipeline(\n",
    "#     prompt,\n",
    "#     max_new_tokens=900,\n",
    "#     do_sample=False,  # Deterministic for demonstration\n",
    "# )\n",
    "\n",
    "# notechat = load_dataset(\"akemiH/NoteChat\")\n",
    "# eval_df = notechat[\"train\"].select(range(5000, 7001)).to_pandas()\n",
    "eval_df = test_df[:20]\n",
    "\n",
    "# 2) Clean up missing data if present\n",
    "# if eval_df.isnull().values.any():\n",
    "#     print(\"Found missing values in the evaluation set. Dropping them.\")\n",
    "#     eval_df = eval_df.dropna()\n",
    "\n",
    "\n",
    "# 3) Prepare batching parameters\n",
    "batch_size = 4\n",
    "num_samples = len(eval_df)\n",
    "num_batches = (num_samples // batch_size) + int(num_samples % batch_size != 0)\n",
    "\n",
    "# Lists to store predictions and references\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "# 4) Batching loop to generate summaries\n",
    "for i in tqdm(range(num_batches), desc=\"Generating Summaries\"):\n",
    "    start = i * batch_size\n",
    "    end = min(start + batch_size, num_samples)\n",
    "    \n",
    "    # Extract conversation and reference summary columns\n",
    "    batch_conversations = eval_df[\"dialogue\"][start:end].tolist()\n",
    "    batch_refs = eval_df[\"note\"][start:end].tolist()  # \"data\" column for references\n",
    "\n",
    "    # Prepare prompts replicating training format\n",
    "    prompts = [\n",
    "        return_prompt(conv)\n",
    "        for conv in batch_conversations\n",
    "    ]\n",
    "    \n",
    "    # Generate summaries\n",
    "    results = summarizer(\n",
    "        prompts,\n",
    "        max_new_tokens=900,\n",
    "        do_sample=False,\n",
    "        # truncation=True,\n",
    "        # num_return_sequences=1  # Default is 1\n",
    "    )\n",
    "    \n",
    "    # Parse results and extract summaries\n",
    "    for item in results:\n",
    "        # 'item' is a list with 1 dict => {\"generated_text\": \"...\"}\n",
    "        output_dict = item[0]\n",
    "        generated_text = output_dict[\"generated_text\"]\n",
    "        \n",
    "        # Extract only the part after \"SUMMARY:\"\n",
    "        # if \"SUMMARY:\" in full_text:\n",
    "        #     extracted_summary = generated_text.split(\"SUMMARY:\", 1)[-1].strip()\n",
    "        # else:\n",
    "        #     extracted_summary = generated_text  # Fallback if marker not found\n",
    "\n",
    "        # first_occurrence = generated_text.lower().find(\"chief complaint\")\n",
    "        # second_occurrence = generated_text.lower().find(\"chief complaint\", first_occurrence + 1)\n",
    "        # third_occurrence = generated_text.lower().find(\"chief complaint\", second_occurrence + 1)\n",
    "        # fourth_occurrence = generated_text.lower().find(\"chief complaint\", third_occurrence + 1)\n",
    "        \n",
    "        # if fourth_occurrence != -1:\n",
    "        #     # Extract everything from the second occurrence of \"CHIEF COMPLAINT\" onward\n",
    "        #     extracted_summary = generated_text[fourth_occurrence:].strip()\n",
    "        # else:\n",
    "        #     # If the second \"CHIEF COMPLAINT\" is not found, just use the original text\n",
    "        #     extracted_summary = generated_text.strip()\n",
    "        \n",
    "        predictions.append(generated_text)\n",
    "    \n",
    "    # Append the reference texts\n",
    "    references.extend(batch_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b3fdb7c-91af-4cad-9649-13d028a754af",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list = eval_df[\"dialogue\"].tolist()\n",
    "ref_list = eval_df[\"note\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "105b41e9-d8ce-44aa-85f5-9529a2db946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = []\n",
    "for pred in predictions:\n",
    "    # Ensure that \"Summary:\" exists in the string to avoid errors\n",
    "    if len(pred) > 0:\n",
    "        output_list.append(pred)\n",
    "    else:\n",
    "        # Handle cases where \"Summary:\" is missing (optional)\n",
    "        output_list.append(\"\")  # Or handle differently based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07efead8-7a4c-4937-b7de-6a5fa448fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_to_json(\n",
    "    src_list=src_list,\n",
    "    ref_list=ref_list,\n",
    "    output_list=output_list\n",
    ")\n",
    "\n",
    "filtered_data = [\n",
    "    entry for entry in data\n",
    "    if entry[\"system_output\"].strip()  # Ensure non-empty system_output\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1218e277-c0fa-4255-a3af-e2af86924bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nlg_evaluation_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf2489",
   "metadata": {
    "id": "23bf2489"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "model_path = \"/content/drive/MyDrive/Medical Dialogue Summarization using PPO/rhlfmodel\"\n",
    "set_seed(42)\n",
    "pipe = pipeline(\"text-generation\",model=model_path, tokenizer=model_path, max_length=40, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e2c6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "026e2c6c",
    "outputId": "7eaea838-051e-48b5-c578-619e0e5b9a08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TL;DR:  My girlfriend and I broke up after she went through my Facebook account without my permission.<|endoftext|>Citizens for the Republic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'TL;DR:  My girlfriend and I broke up after she went through my Facebook account without my permission.<|endoftext|>Citizens for the Republic'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = dataset[\"rejected\"][0]\n",
    "print(text)\n",
    "pipe(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NpppcYveAzWa",
   "metadata": {
    "id": "NpppcYveAzWa"
   },
   "outputs": [],
   "source": [
    "save_directory = \"/content/drive/MyDrive/Medical Dialogue Summarization using PPO/rhlfmodel\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(save_directory).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llCvj_yCJY9S",
   "metadata": {
    "id": "llCvj_yCJY9S"
   },
   "outputs": [],
   "source": [
    "conversation = '''\n",
    "Doctor: Hi, Mr. X, I'm Dr. Y. How are you feeling today?\n",
    "\n",
    "Patient: Not too good, doctor. I've been feeling really sick lately.\n",
    "\n",
    "Doctor: I understand. Can you tell me what symptoms you're experiencing?\n",
    "\n",
    "Patient: Yes, I've been having a fever, a dry cough, and dyspnea.\n",
    "\n",
    "Doctor: I see. You were hospitalized due to moderate ARDS from COVID-19, is that correct?\n",
    "\n",
    "Patient: Yes, that's correct.\n",
    "\n",
    "Doctor: During your physical therapy, we encountered some difficulties. Can you tell me more about that?\n",
    "\n",
    "Patient: Yes, I had trouble with position changes and deep breathing. Every time I tried to change my position or take a deep breath, I would start coughing and it would make me really short of breath.\n",
    "\n",
    "Doctor: I understand. To avoid rapid deterioration and respiratory failure, we instructed you to change positions very slowly and step-by-step, right?\n",
    "\n",
    "Patient: Yes, that's right. It took about 30 minutes to change to the prone position.\n",
    "\n",
    "Doctor: And I see that this approach increased your oxygen saturation, for example, on day 5 with 6 L/min of oxygen from 93% to 97%.\n",
    "\n",
    "Patient: Yes, that's correct.\n",
    "\n",
    "Doctor: Good. We also had to adapt your breathing exercises to avoid prolonged coughing and oxygen desaturation. Can you tell me more about that?\n",
    "\n",
    "Patient: Yes, I was instructed to stop every deep breath before coughing and to hold my breath for better air distribution.\n",
    "\n",
    "Doctor: I see that you performed the breathing exercises well and managed to increase your oxygen saturation.\n",
    "\n",
    "Patient: Yes, I did my best.\n",
    "\n",
    "Doctor: You also had difficulty maintaining sufficient oxygen saturation during physical activity, is that correct?\n",
    "\n",
    "Patient: Yes, I did. But with close monitoring and frequent breaks, I was able to perform low-level strength and walking exercises without any significant deoxygenation.\n",
    "\n",
    "Doctor: I see that your exercise progression was low on days 1 to 5, but then increased daily until your hospital discharge to a rehabilitation clinic on day 10.\n",
    "\n",
    "Patient: Yes, that's correct.\n",
    "\n",
    "Doctor: Great. I'd like to keep monitoring your progress and see how you're doing. Can you keep me updated on any changes in your symptoms?\n",
    "\n",
    "Patient: Yes, of course, doctor.\n",
    "\n",
    "Doctor: Alright, let's keep in touch. If you have any questions or concerns, don't hesitate to reach out to me.\n",
    "\n",
    "Patient: Thank you, doctor.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DOxLTH1QAKh7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOxLTH1QAKh7",
    "outputId": "ceb93bc4-3129-41de-c6dc-7380c7a32a95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      " Generate a summary for the below conversation. Dont give me the prompt back. I just want the summary to be returned to me\n",
      "\n",
      "\n",
      "Doctor: Hi, Mr. X, I'm Dr. Y. How are you feeling today?\n",
      "\n",
      "Patient: Not too good, doctor. I've been feeling really sick lately.\n",
      "\n",
      "Doctor: I understand. Can you tell me what symptoms you're experiencing?\n",
      "\n",
      "Patient: Yes, I've been having a fever, a dry cough, and dyspnea.\n",
      "\n",
      "Doctor: I see. You were hospitalized due to moderate ARDS from COVID-19, is that correct?\n",
      "\n",
      "Patient: Yes, that's correct.\n",
      "\n",
      "Doctor: During your physical therapy, we encountered some difficulties. Can you tell me more about that?\n",
      "\n",
      "Patient: Yes, I had trouble with position changes and deep breathing. Every time I tried to change my position or take a deep breath, I would start coughing and it would make me really short of breath.\n",
      "\n",
      "Doctor: I understand. To avoid rapid deterioration and respiratory failure, we instructed you to change positions very slowly and step-by-step, right?\n",
      "\n",
      "Patient: Yes, that's right. It took about 30 minutes to change to the prone position.\n",
      "\n",
      "Doctor: And I see that this approach increased your oxygen saturation, for example, on day 5 with 6 L/min of oxygen from 93% to 97%.\n",
      "\n",
      "Patient: Yes, that's correct.\n",
      "\n",
      "Doctor: Good. We also had to adapt your breathing exercises to avoid prolonged coughing and oxygen desaturation. Can you tell me more about that?\n",
      "\n",
      "Patient: Yes, I was instructed to stop every deep breath before coughing and to hold my breath for better air distribution.\n",
      "\n",
      "Doctor: I see that you performed the breathing exercises well and managed to increase your oxygen saturation.\n",
      "\n",
      "Patient: Yes, I did my best.\n",
      "\n",
      "Doctor: You also had difficulty maintaining sufficient oxygen saturation during physical activity, is that correct?\n",
      "\n",
      "Patient: Yes, I did. But with close monitoring and frequent breaks, I was able to perform low-level strength and walking exercises without any significant deoxygenation.\n",
      "\n",
      "Doctor: I see that your exercise progression was low on days 1 to 5, but then increased daily until your hospital discharge to a rehabilitation clinic on day 10.\n",
      "\n",
      "Patient: Yes, that's correct.\n",
      "\n",
      "Doctor: Great. I'd like to keep monitoring your progress and see how you're doing. Can you keep me updated on any changes in your symptoms?\n",
      "\n",
      "Patient: Yes, of course, doctor.\n",
      "\n",
      "Doctor: Alright, let's keep in touch. If you have any questions or concerns, don't hesitate to reach out to me.\n",
      "\n",
      "Patient: Thank you, doctor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_response(prompt, model, tokenizer, max_new_tokens=1000, temperature=0.1):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"Generate a summary for the below conversation. Dont give me the prompt back. I just want the summary to be returned to me\\n\\n\" + conversation\n",
    "response = generate_response(prompt, model, tokenizer)\n",
    "print(\"Generated Response:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dH7ZXN5XGf3q",
   "metadata": {
    "id": "dH7ZXN5XGf3q"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"rlhfmodel/\")\n",
    "model_path = \"bigcode/tiny_starcoder_py\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation=True, max_length=256, padding=\"max_length\")\n",
    "text = df.iloc[2][\"prompt\"]\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\", max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7239817f-7f97-458f-92c5-554a998e2ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\hf-cache\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating coherence of 4 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 4 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 4 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 22.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation scores are shown below:\n",
      "+-------------+----------+\n",
      "|  Dimensions |  Score   |\n",
      "+-------------+----------+\n",
      "|  coherence  | 0.449252 |\n",
      "| consistency | 0.64936  |\n",
      "|   fluency   | 0.385504 |\n",
      "|   overall   | 0.494705 |\n",
      "+-------------+----------+\n",
      "[[0.76403181 0.84311455 0.48589937]\n",
      " [0.45844862 0.71566303 0.44122299]\n",
      " [0.55973144 0.72223429 0.35135146]\n",
      " [0.0147952  0.31642992 0.26354302]]\n",
      "[np.float64(0.3308542634136033), np.float64(-0.11693050848950663), np.float64(0.3167152151413462), np.float64(-0.4794280308898093)]\n",
      "[tensor([0.3309], dtype=torch.float64), tensor([-0.1169], dtype=torch.float64), tensor([0.3167], dtype=torch.float64), tensor([-0.4794], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\n",
    "        'source': \"Doctor: Hello, how are you feeling today?\\nPatient: I've been feeling a bit tired and dizzy.\\nDoctor: How long has this been happening?\\nPatient: For about a week now. I also have trouble sleeping.\\nDoctor: I see. Have you been under a lot of stress lately?\\nPatient: Yes, work has been quite stressful.\\nDoctor: That could be contributing. Let’s do some tests to rule out other issues.\",\n",
    "        'system_output': \"Patient reports tiredness, dizziness, and difficulty sleeping for a week. Work-related stress may be a factor. Doctor will conduct tests to check for other problems.\"\n",
    "    },\n",
    "    {\n",
    "        'source': \"Doctor: What brings you in today?\\nPatient: I’ve been having some chest pain and shortness of breath.\\nDoctor: How severe is the pain?\\nPatient: It’s sharp, and it comes and goes.\\nDoctor: When did it start?\\nPatient: It started two days ago.\\nDoctor: Any history of heart problems?\\nPatient: Yes, my father had heart disease.\\nDoctor: We’ll need to do an ECG and some blood tests to check your heart health.\",\n",
    "        'system_output': \"Patient has sharp chest pain and shortness of breath for two days. Family history of heart disease. Doctor will perform an ECG and blood tests to assess heart health.\"\n",
    "    },\n",
    "    {\n",
    "        'source': \"Doctor: How are you feeling today?\\nPatient: I’ve had a sore throat and a cough for the past few days.\\nDoctor: Any fever or difficulty swallowing?\\nPatient: Yes, I’ve had a low fever, but swallowing is fine.\\nDoctor: Any history of allergies or similar symptoms?\\nPatient: Not really.\\nDoctor: It could be a viral infection. I recommend rest, fluids, and maybe some over-the-counter medicine.\",\n",
    "        'system_output': \"Patient reports sore throat, cough, and a low fever. Doctor advises rest, fluids, and over-the-counter medication as the symptoms suggest a viral infection.\"\n",
    "    },\n",
    "    {\n",
    "        'source': \"Doctor: What’s bothering you today?\\nPatient: I’ve been experiencing frequent headaches and some nausea.\\nDoctor: How often do you get the headaches?\\nPatient: It’s been almost every day for the past week.\\nDoctor: Any other symptoms like blurred vision or dizziness?\\nPatient: No, just the headache and nausea.\\nDoctor: We’ll schedule an MRI to get a better understanding of the issue.\",\n",
    "        'system_output': \"Patient complains of daily headaches and nausea for the past week. No blurred vision or dizziness. Doctor will schedule an MRI for further evaluation.\"\n",
    "    }\n",
    "]\n",
    "score = evaluate(data, print_result=True)\n",
    "print(score)\n",
    "\n",
    "weights = np.array([1, 2, 3]) #'coherence', 'consistency', 'fluency'\n",
    "weighted_score = []\n",
    "\n",
    "for array1 in score:\n",
    "    result = np.where(\n",
    "        array1 < 0.5,          # Condition\n",
    "        -array1 * weights,     # If True: make product negative\n",
    "        array1 * weights       # If False: normal multiplication\n",
    "    )\n",
    "    \n",
    "    sum_products = np.sum(result)\n",
    "    final_result = sum_products/3\n",
    "    \n",
    "    weighted_score.append(final_result)\n",
    "\n",
    "        \n",
    "print(weighted_score)\n",
    "\n",
    "scores = [torch.tensor([x], dtype=torch.float64) for x in weighted_score]\n",
    "print(scores)  # Output: torch.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a748299d-0934-4815-8f1a-b419b6ada364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'source': '[positive] [doctor] karen is a 34 -year-old female with a history of chronic migraines and hypertension who is here today with abdominal pain so hi', 'system_output': \" dr. Ochsner, nice to meet you.\\n\\nDr. Ochsner: good to meet you as well. It's great to see you back in\"}]\n"
     ]
    }
   ],
   "source": [
    "sample_data = []\n",
    "\n",
    "for q,r in zip(game_data[\"query\"], game_data[\"response\"]):\n",
    "    temp = {}\n",
    "\n",
    "    temp[\"source\"] = q\n",
    "    temp[\"system_output\"] = r\n",
    "\n",
    "    sample_data.append(temp)\n",
    "\n",
    "    break\n",
    "\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "b777e40f-73d3-4f64-a929-6f970cb13860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating coherence of 1 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 1 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 1 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 22.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation scores are shown below:\n",
      "+-------------+----------+\n",
      "|  Dimensions |  Score   |\n",
      "+-------------+----------+\n",
      "|  coherence  | 0.702612 |\n",
      "| consistency | 0.684789 |\n",
      "|   fluency   | 0.566372 |\n",
      "+-------------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score = evaluate(sample_data, print_result=True, overall=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef4c1da-dc68-478a-8f9a-363c52128653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (kshitij)",
   "language": "python",
   "name": "kshitij"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
