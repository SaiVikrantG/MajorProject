{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ZL9wcvcle3iT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZL9wcvcle3iT",
    "outputId": "6926df5c-52dd-4601-8af5-f324fc285fd2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install trl==0.7.4\n",
    "# !pip install datasets\n",
    "# !pip install transformers==4.38.2\n",
    "# !pip install peft==0.10.0\n",
    "# !pip install accelerate==0.28.0\n",
    "# test commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf67fc5a-c458-484f-869a-bfb2154e6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"D:\\hf-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fa80b0e",
   "metadata": {
    "id": "2fa80b0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "def set_seed(seed_val=42):\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "#Configuration options\n",
    "train_batch_size = 16\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 1e-5\n",
    "eval_batch_size = 1\n",
    "eval_steps = 500\n",
    "max_input_length = 550\n",
    "save_steps = 1000\n",
    "num_train_epochs = 20\n",
    "random.seed(42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6b3d82-34f1-4718-9d02-ef80e8ca1625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018c6ca",
   "metadata": {
    "id": "4018c6ca"
   },
   "source": [
    "## Creating the policy model for human Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58ccc7e5-8a0e-4fb5-8cc6-4d9f4b9cac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"combined_clinical_notes.csv\")\n",
    "\n",
    "# First split: 60% train, 40% temp (eval + test)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "\n",
    "# Second split: 50% of temp (which is 20% of total) for eval, 50% for test\n",
    "eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d885cf72-3d15-48a5-b1a3-8e885b2537a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "encounter_id\n",
      "dialogue\n",
      "note\n",
      "source_file\n",
      "id\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    print(column)  # Prints each column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0670ff5e",
   "metadata": {
    "id": "0670ff5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\hf-cache\\hub\\models--HPAI-BSC--Qwen2.5-Aloe-Beta-7B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards:   0%|                                                                        | 0/4 [03:01<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1) 4-bit quant config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# 2) Load base model in 4-bit\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# 3) Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# 4) Enable gradient checkpointing\n",
    "# model.enable_input_require_grads()\n",
    "# model.gradient_checkpointing_enable()\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# 5) Prepare data with smaller sequence length\n",
    "# notechat = load_dataset(\"akemiH/NoteChat\")\n",
    "# trainB = notechat[\"train\"].select(range(3000))\n",
    "# evalB = notechat[\"train\"].select(range(3000, 3500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efa4266-3b5d-4c72-a1ef-663004a4ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLDRDataset(Dataset):\n",
    "    def __init__(self, train_path, tokenizer, split, max_length=4000):\n",
    "        self.post_list = []\n",
    "        dataset = test_df\n",
    "        self.labels = []\n",
    "\n",
    "        for sample in dataset.iterrows():\n",
    "            self.post_list.append(sample[1][\"dialogue\"])\n",
    "            self.labels.append(sample[1][\"note\"])\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.post_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.post_list[idx]\n",
    "        summary = self.labels[idx]\n",
    "        # label = self.labels[idx]\n",
    "\n",
    "        # encodings_dict = self.tokenizer(txt, truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
    "        # input_ids = torch.tensor(encodings_dict[\"input_ids\"])\n",
    "        # attn_masks = torch.tensor(encodings_dict[\"attention_mask\"])\n",
    "        # return {\n",
    "        #     \"input_ids\": input_ids,\n",
    "        #     \"attention_mask\": attn_masks,\n",
    "        #     \"labels\": labels_ids,\n",
    "        # }\n",
    "\n",
    "        txt = f\"CONVERSATION:\\n{conversation}\\n\\nSUMMARY: \\n{summary}\" #IMPORTANT!!!!!!!!!!\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            txt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "         # Labels should be the same as input_ids for causal LM training\n",
    "        # The model will automatically shift labels internally\n",
    "        encodings[\"labels\"] = encodings[\"input_ids\"].clone()\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": encodings[\"labels\"].squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a73ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328,
     "referenced_widgets": [
      "c0d5a5aca66640fabb8e8b4c6fb59392",
      "0d238f7dc3544a248e568b01b2b8f68b",
      "360c56bdbb1a4cf698cd6ee02fc0377c",
      "ec18abfee9fb4920919fff399355653a",
      "17774ec2b2814c989e559645c66c16dc",
      "3e9a90d9c01e41faa39e765d077940ad",
      "a7a3771791064414b7baa55dea4f9307",
      "f9140d1590e4477fa2a0f6ee0f0a29d4",
      "4f64e2615eb34e9e8bb496845194c4f0",
      "cec141a0a52a44029af02766578a7a86",
      "dd692ebfdd374157a28212e3c58dfdee",
      "edb52e11236a466ba3180a6700fcbe47",
      "d8806f44019a4be995bf8a00575a884c",
      "cd702006f8d64566b59db92ced9fa9a8",
      "010b68f7a4e74db1a691e73c6c0b668d",
      "effdcdf181e4451695eeef03a6994093",
      "d8d51ab046d8479fb6c55d58beaeab69",
      "dce9a626e1ac48faa5b3da071cf39bf1",
      "9a88ceb27ac54478b1806f1417e3f56e",
      "4b1628d06c674e7e9542e4de56942440",
      "5b80c804ea87468aa681a4346c9cfb3e",
      "54e1f5359144444faad92a5af1208b20",
      "3c9db78225634f4ba56dd620875a6152",
      "6df03bc0feae4dcf8c0f5f539e837aa3",
      "98e9fab21039484cb336ce0f781d1f7f",
      "a7ee81d9ba61402fb8f18bb17416f640",
      "3c7c8b864d5942a58a476a7123e4c919",
      "24d9d364d21b47d8b19217cf28958b98",
      "4ff64e6f540e4900b40e564330dd0192",
      "e408dbc1a67a4dcf89e3876437a35da2",
      "697def36105e47f987bc8595102a45ce",
      "8b78c25aecda45db921b0351401d0bf9",
      "d08ee3cbabfd44819328f076bb075337",
      "68cb590000b94ab397dc97cdfc05c4b6",
      "154be340981b436c9caf3289aa6a39b4",
      "f088edd1f0474d1a97ad0c008f4cfd43",
      "76104008746b45a59c7065a90fef5ffd",
      "09c0865087fb4cadb82bedd9e4fe224f",
      "32467bbd99394b86b3d5db9cc6572791",
      "b9f148a70e67439aa7208c8580d3f843",
      "f323e77044ae48c6a3d88ea620735366",
      "37429d08cb6a48adadab9e1459d274d2",
      "cde30c20c0774408851a356a23412864",
      "e3b0bb095a1e405cbc48908ca48bdb7b",
      "69a7a00f8f1b48948ada90f62fcdc9a6",
      "04dd6f2423b04009b0b53e45eb24b1a7",
      "74734805983e42c9906bf8cd10b2974e",
      "fc633082a04e4480bafec26d1c2ccb7a",
      "bb9ddba4dd9b4ddbbf65cfa91f2b521d",
      "281ed0eb1bed44cb837b0977b6c50e0b",
      "c9587d181b8f499ea983f53d38e4d1f5",
      "e6e5ade438ab45c8a05e9f68a1838380",
      "7db18b1334424e84ac6c64003b7a91ed",
      "25ca2e5e59134e45ae4addccde4dc618",
      "ee3c992c640c481cbdb5927cb5b113da",
      "92382a78fc554b2a94f3a99fa064bf3a",
      "437363a2959441e19842e19b67388db1",
      "d8a8c1f8b5a347408ffe1bed07e87c6d",
      "00ebae8e2b0a46539b3bc297976a1f35",
      "dd35c1969a73488789164f4c59d7b1de",
      "b2b1a073a7b14f759bda8df3bae6db1f",
      "ffe0b0cc000444c98d0d43a1f9d4843c",
      "6ea4187ab5124426b662badc73ac1b10",
      "b2e22b238f214dca87827d6b3b5eaeca",
      "d29cfa7e295f4949b100d3a21126174f",
      "2850146e7d3f42109539343d97c936c6",
      "666db4066bec4d7581cc19e6f5bc98bc",
      "72df76a9cdf8459cb63f67d992b10749",
      "f6409c1155eb4b92998e20999b642c04",
      "58f0850759b8457b9657d36b326ddf2e",
      "1bc25a94b13b4189b8fc3f7ce41b9086",
      "6ee1b4bc76cb4106962fb29f05cc0825",
      "76d96ad5ee9344978487022753a3adfa",
      "6f272cac47734ea9abb212d32a65c382",
      "a5ec7fadd5bc439db5119626b8e6afbb",
      "71e7360d713c4c70a33e9fd9e0cb8412",
      "c21e2109534b4e1cb176332e1db6e59f",
      "7a473fedaf4645cdaf78189b1bfbdd6d",
      "5afba4b64a114f948309aa11d3ba5c34",
      "2accfcbededc4afa817fc4beedaee153",
      "477d5050997045318236932dad8d8418",
      "cfa386b4863f4fd29120e79be0e317ce",
      "788c6efc3c41484d9559b57690401142",
      "b026473caa4f4234b8b90c2d7965115b",
      "b6109109cd9746dfb361860ae6d26cac",
      "ee699d35db9d4b6ab0a74713ad5929d4",
      "812bf7bc5a6f4985b0ecc0de236b45a0",
      "c592e8dcb00349c6817b05382263c784"
     ]
    },
    "id": "7f0a73ab",
    "outputId": "24038f05-5422-492e-9a38-dca119a91b2d"
   },
   "outputs": [],
   "source": [
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "# model.config.end_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Disable caching (already done, but double-check)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Enable gradient checkpointing (already done, but confirm)\n",
    "# model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6aad5",
   "metadata": {
    "id": "dbc6aad5"
   },
   "outputs": [],
   "source": [
    "# Set up the datasets\n",
    "data_path = \"NA\"\n",
    "train_dataset = TLDRDataset(\n",
    "    data_path,\n",
    "    tokenizer,\n",
    "    \"train\",\n",
    "    max_length=256,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a19aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f16a19aa",
    "outputId": "37314f8f-f319-4d58-d968-cfc62bf5ab89"
   },
   "outputs": [],
   "source": [
    "for i in train_dataset:\n",
    "    print(i[\"input_ids\"], i[\"labels\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500301ac",
   "metadata": {
    "id": "500301ac"
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rPWzhVtfjXQf",
   "metadata": {
    "id": "rPWzhVtfjXQf"
   },
   "outputs": [],
   "source": [
    "output_dir = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune-logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3801788",
   "metadata": {
    "id": "d3801788"
   },
   "outputs": [],
   "source": [
    "# Prepare the trainer and start training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "#     per_device_eval_batch_size=eval_batch_size,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_drop_last=True,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefacaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baefacaf",
    "outputId": "917e4e82-2c45-4cc7-da64-356b9f5a5dfb"
   },
   "outputs": [],
   "source": [
    "training_args.device.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba4710",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "90ba4710",
    "outputId": "398ff6cf-790c-4b2c-eae7-c4e92a1776f1"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     data_collator=default_data_collator,\n",
    "#     preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "# trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc50c2",
   "metadata": {
    "id": "3dfc50c2"
   },
   "outputs": [],
   "source": [
    "# trainer.save_model(r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\")   ##path to save policy model\n",
    "# tokenizer.save_pretrained(r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\")\n",
    "# model.save_pretrained(r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune-peft-adapaters\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373c19c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b373c19c",
    "outputId": "caf7af75-5a02-47b4-b1ef-d5421c6e5ae8"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune-logs\")\n",
    "model_path = \"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation=True, max_length=256, padding=\"max_length\")\n",
    "text = train_df.iloc[2][\"dialogue\"]\n",
    "tokenized_text = tokenizer(text, return_tensors=\"pt\", max_length=256)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92769534",
   "metadata": {
    "id": "92769534"
   },
   "source": [
    "# Policy Model Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da46d612-6e91-4dd6-a60b-2866f28c58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"D:\\hf-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb9cb217",
   "metadata": {
    "id": "bb9cb217"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from trl import RewardTrainer\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d0acdb",
   "metadata": {
    "id": "50d0acdb"
   },
   "outputs": [],
   "source": [
    "##model path\n",
    "# MODEL_PATH = \"/content/drive/MyDrive/Medical Dialogue Summarization using PPO/rm_model\"\n",
    "MODEL_PATH = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e36852d7-2923-420a-a4da-c2464a7aecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9af5cda-6919-4b1c-bedb-28f86f9c4d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████| 4/4 [00:10<00:00,  2.58s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n",
      "WARNING:root:The `device_map` argument is not provided. We will override the device_map argument. to set the entire model on the current device. If you want to set the model on multiple devices, please provide a custom `device_map` argument.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from transformers import BitsAndBytesConfig  \n",
    "\n",
    "# ---- Device Setup ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---- Paths ----\n",
    "MODEL_PATH = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\"\n",
    "PEFT_ADAPTER_PATH = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune-peft-adapaters\"\n",
    "REF_MODEL_PATH = r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-base-fine-tune\"\n",
    "\n",
    "# ---- 1) 4-bit Quantization Configuration ----\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# ---- 2) Load Base Model in 4-bit ----\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "# Prepare the model for k-bit training (this typically freezes most parameters except adapter ones)\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "base_model.gradient_checkpointing_disable()  # Disable checkpointing\n",
    "\n",
    "# ---- 3) Load Tokenizer ----\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# ---- 4) Load the PEFT Adapter (LoRA) ----\n",
    "# This reloads your fine-tuned adapter weights onto your base model.\n",
    "model_with_lora = PeftModel.from_pretrained(base_model, PEFT_ADAPTER_PATH)\n",
    "\n",
    "# ---- 5) Convert to PPO-Compatible ValueHead Model ----\n",
    "# When converting, pass the peft_config from the adapter model to ensure proper initialization.\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model_with_lora,\n",
    "    peft_config=lora_config\n",
    ").to(device)\n",
    "\n",
    "# ---- 6) Optionally, Load a Reference Model for KL (e.g., reward model) ----\n",
    "ppo_model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model_with_lora,\n",
    ").to(device)\n",
    "ppo_model_ref.eval()  # Disable dropout/etc\n",
    "for param in ppo_model_ref.parameters():\n",
    "    param.requires_grad = False  # Freeze all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "051a38d5-02b9-456c-9cbc-31f07bf6f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH) ## tokenizer of step 1 model., here since we are using same model for step 1 and 2 it doesnot matter\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# tok = AutoTokenizer.from_pretrained(MODEL_PATH, use_fast=False)\n",
    "# tok.pad_token = tok.eos_token\n",
    "# tok.padding_side = \"left\"\n",
    "# ppo_model.config.pad_token_id = tok.eos_token_id\n",
    "# ppo_model.resize_token_embeddings(len(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05f02097",
   "metadata": {
    "id": "05f02097"
   },
   "outputs": [],
   "source": [
    "sentiment_pipe_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\"}\n",
    "project_kwargs={\"logging_dir\": r\"D:\\kshitij-weights-folder\\qwen-aloe-9-4-rl-logs\"}\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=MODEL_PATH, ppo_epochs=1, project_kwargs=project_kwargs, gradient_accumulation_steps=2, steps=5, batch_size=2, mini_batch_size=1, learning_rate=2e-5, log_with='tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e1a2e38",
   "metadata": {
    "id": "2e1a2e38"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py:254: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "# optimizer = torch.optim.SGD(starcoder_model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "optimizer = bnb.optim.Adam8bit(ppo_model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "ppo_trainer = PPOTrainer(config, ppo_model,ppo_model_ref, tokenizer, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5493f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "df = pd.read_csv(\"combined_clinical_notes.csv\")[[\"dialogue\", \"note\"]]\n",
    "\n",
    "class ClinDS(Dataset):\n",
    "    def __init__(self, df, tok, max_len=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tok = tok\n",
    "        self.L = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        conv = str(self.df.iloc[i][\"dialogue\"])\n",
    "        ref = str(self.df.iloc[i][\"note\"])\n",
    "        prompt = f\"Summarize the following conversation:\\n\\n{conv}\"\n",
    "        enc = self.tok(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.L,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc.input_ids.squeeze(),\n",
    "            \"attention_mask\": enc.attention_mask.squeeze(),\n",
    "            \"src_txt\": prompt,\n",
    "            \"ref_txt\": ref,\n",
    "        }\n",
    "\n",
    "loader = DataLoader(\n",
    "    ClinDS(df.sample(200, random_state=0), tokenizer),\n",
    "    batch_size=1, shuffle=True, pin_memory=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3366a32e-601a-44f8-9dbd-6038c72b23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2974cb44",
   "metadata": {
    "id": "2974cb44"
   },
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 32,\n",
    "    \"eos_token_id\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3a744ce-d939-49a2-9914-94d853750476",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDICAL_PROMPT = \"\"\"\n",
    "Please generate a medical summary based on the following clinical notes. The summary should include the following sections: \n",
    "\n",
    "CHIEF COMPLAINT\n",
    "A concise statement of the patient's primary concern or reason for visiting the clinic.\n",
    "\n",
    "HISTORY OF PRESENT ILLNESS\n",
    "A detailed narrative about the patient's symptoms, their onset, duration, and any relevant medical history or previous treatments.\n",
    "\n",
    "VITALS\n",
    "Include any relevant vital signs (e.g., oxygen saturation, blood pressure) if available.\n",
    "\n",
    "PHYSICAL EXAM \n",
    "Summarize the findings from the physical examination, including any notable abnormalities.\n",
    "\n",
    "RESULTS \n",
    "Summarize the results of any diagnostic tests performed (e.g., lab work, imaging studies).\n",
    "\n",
    "ASSESSMENT\n",
    "The doctor's assessment of the patient's condition or diagnosis.\n",
    "\n",
    "PLAN\n",
    "The treatment plan, including prescribed medications, lifestyle recommendations, and follow-up instructions.\n",
    "\n",
    "INSTRUCTIONS\n",
    "Specific instructions for the patient regarding their treatment plan and follow-up care.\n",
    "\n",
    "Important Note: If any section lacks relevant information, omit that section from the generated summary. Only include sections for which there is sufficient information available.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18b205a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")\n",
    "\n",
    "from utils import convert_to_json\n",
    "from metric.evaluator import get_evaluator\n",
    "\n",
    "sum_eval = get_evaluator(\"summarization\", device=\"cpu\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def unieval_4way(src, hyp, ref):\n",
    "    \"\"\"\n",
    "    src, hyp, ref: lists of strings, length B\n",
    "    returns: Tensor (B,4) with [coherence, consistency, fluency, relevance]\n",
    "    \"\"\"\n",
    "    data = convert_to_json(\n",
    "        output_list=hyp,\n",
    "        src_list=src,\n",
    "        ref_list=ref,\n",
    "    )\n",
    "    raw = sum_eval.evaluate(data)\n",
    "    scores = [\n",
    "        [d[\"coherence\"], d[\"consistency\"], d[\"fluency\"], d[\"relevance\"]]\n",
    "        for d in raw\n",
    "    ]\n",
    "    return torch.tensor(scores, dtype=torch.float32)  # CPU (B,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "253d6cc8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "253d6cc8",
    "outputId": "696512ba-ab95-481c-e4d8-ef7d3065b28f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:04<00:00,  4.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:04<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:02<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:04<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0\n",
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:04<00:00,  4.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:04<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:02<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:03<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:04<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:04<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:04<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating coherence of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:04<00:00,  4.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating consistency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:04<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fluency of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 1/1 [00:02<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating relevance of 2 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m rewards = []\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(src_txt)):\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# Get scores for all candidates (K, 4)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     scores = \u001b[43munieval_4way\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43msrc_txt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_CANDIDATES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhyps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mref_txt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_CANDIDATES\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.numpy()\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# Compute dominance counts\u001b[39;00m\n\u001b[32m     49\u001b[39m     dom_counts = np.zeros(NUM_CANDIDATES)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36munieval_4way\u001b[39m\u001b[34m(src, hyp, ref)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[33;03msrc, hyp, ref: lists of strings, length B\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03mreturns: Tensor (B,4) with [coherence, consistency, fluency, relevance]\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     15\u001b[39m data = convert_to_json(\n\u001b[32m     16\u001b[39m     output_list=hyp,\n\u001b[32m     17\u001b[39m     src_list=src,\n\u001b[32m     18\u001b[39m     ref_list=ref,\n\u001b[32m     19\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m raw = \u001b[43msum_eval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m scores = [\n\u001b[32m     22\u001b[39m     [d[\u001b[33m\"\u001b[39m\u001b[33mcoherence\u001b[39m\u001b[33m\"\u001b[39m], d[\u001b[33m\"\u001b[39m\u001b[33mconsistency\u001b[39m\u001b[33m\"\u001b[39m], d[\u001b[33m\"\u001b[39m\u001b[33mfluency\u001b[39m\u001b[33m\"\u001b[39m], d[\u001b[33m\"\u001b[39m\u001b[33mrelevance\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m raw\n\u001b[32m     24\u001b[39m ]\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.tensor(scores, dtype=torch.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\kshitij\\UniEval\\metric\\evaluator.py:77\u001b[39m, in \u001b[36mSumEvaluator.evaluate\u001b[39m\u001b[34m(self, data, dims, overall, print_result)\u001b[39m\n\u001b[32m     74\u001b[39m             ref_list.append(data[i][\u001b[33m'\u001b[39m\u001b[33mreference\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     75\u001b[39m     input_list = add_question(dimension=dim, output=output_list, \n\u001b[32m     76\u001b[39m                               src=src_list, ref=ref_list, task=\u001b[38;5;28mself\u001b[39m.task)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     score = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Please customize other dimensions here for summarization\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mThe input format for this dimension is still undefined. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33m                               Please customize it first.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\kshitij\\UniEval\\metric\\scorer.py:62\u001b[39m, in \u001b[36mUniEvaluator.score\u001b[39m\u001b[34m(self, inputs, batch_size)\u001b[39m\n\u001b[32m     58\u001b[39m src_mask = encoded_src[\u001b[33m'\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m'\u001b[39m].to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     60\u001b[39m tgt_tokens = encoded_tgt[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].to(\u001b[38;5;28mself\u001b[39m.device)[:, \u001b[32m0\u001b[39m].unsqueeze(-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_tokens\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m logits = output.logits.view(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.model.config.vocab_size)\n\u001b[32m     69\u001b[39m pos_score = \u001b[38;5;28mself\u001b[39m.softmax(logits)[:, \u001b[38;5;28mself\u001b[39m.pos_id] \u001b[38;5;66;03m# Yes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1711\u001b[39m, in \u001b[36mT5ForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1708\u001b[39m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[32m   1709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1710\u001b[39m     \u001b[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1711\u001b[39m     encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1716\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1717\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1718\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1719\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1720\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[32m   1721\u001b[39m     encoder_outputs = BaseModelOutput(\n\u001b[32m   1722\u001b[39m         last_hidden_state=encoder_outputs[\u001b[32m0\u001b[39m],\n\u001b[32m   1723\u001b[39m         hidden_states=encoder_outputs[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1724\u001b[39m         attentions=encoder_outputs[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1725\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1115\u001b[39m, in \u001b[36mT5Stack.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1100\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1101\u001b[39m         layer_module.forward,\n\u001b[32m   1102\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1112\u001b[39m         output_attentions,\n\u001b[32m   1113\u001b[39m     )\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[32m   1130\u001b[39m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[32m   1131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:755\u001b[39m, in \u001b[36mT5Block.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[39m\n\u001b[32m    752\u001b[39m     attention_outputs = attention_outputs + cross_attention_outputs[\u001b[32m2\u001b[39m:]\n\u001b[32m    754\u001b[39m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m755\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[32m    758\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hidden_states.dtype == torch.float16:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:344\u001b[39m, in \u001b[36mT5LayerFF.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[32m    343\u001b[39m     forwarded_states = \u001b[38;5;28mself\u001b[39m.layer_norm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     forwarded_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mDenseReluDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28mself\u001b[39m.dropout(forwarded_states)\n\u001b[32m    346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:312\u001b[39m, in \u001b[36mT5DenseGatedActDense.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     hidden_gelu = \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwi_0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    313\u001b[39m     hidden_linear = \u001b[38;5;28mself\u001b[39m.wi_1(hidden_states)\n\u001b[32m    314\u001b[39m     hidden_states = hidden_gelu * hidden_linear\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\kshitij\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from random import choices\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "for epoch in range(1):\n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        try:  # Wrap the entire batch processing in try-catch\n",
    "            # Prepare inputs\n",
    "            ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attn_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            src_txt = batch[\"src_txt\"]  # list[str]\n",
    "            ref_txt = batch[\"ref_txt\"]  # list[str]\n",
    "\n",
    "            # Generate multiple candidates per prompt\n",
    "            NUM_CANDIDATES = 2\n",
    "            all_outs = []\n",
    "            for _ in range(NUM_CANDIDATES):\n",
    "                with torch.no_grad():\n",
    "                    out = ppo_model.generate(\n",
    "                        input_ids=ids,\n",
    "                        attention_mask=attn_mask,\n",
    "                        **gen_kwargs\n",
    "                    )\n",
    "                all_outs.append(out)\n",
    "\n",
    "            # Stack outputs (B, K, L)\n",
    "            outs = torch.stack(all_outs, dim=1)\n",
    "\n",
    "            # Decode all candidates\n",
    "            hyps = [\n",
    "                [tokenizer.decode(outs[b, k], skip_special_tokens=True)\n",
    "                for k in range(NUM_CANDIDATES)]\n",
    "                for b in range(outs.size(0))\n",
    "            ]\n",
    "\n",
    "            # Compute rewards using UniEval and dominance scoring\n",
    "            rewards = []\n",
    "            for b in range(len(src_txt)):\n",
    "                # Get scores for all candidates (K, 4)\n",
    "                scores = unieval_4way(\n",
    "                    [src_txt[b]] * NUM_CANDIDATES,\n",
    "                    hyps[b],\n",
    "                    [ref_txt[b]] * NUM_CANDIDATES\n",
    "                ).numpy()\n",
    "\n",
    "                # Compute dominance counts\n",
    "                dom_counts = np.zeros(NUM_CANDIDATES)\n",
    "                for i in range(NUM_CANDIDATES):\n",
    "                    for j in range(NUM_CANDIDATES):\n",
    "                        if i == j:\n",
    "                            continue\n",
    "                        # Check if i dominates j\n",
    "                        if np.all(scores[i] >= scores[j]) and np.any(scores[i] > scores[j]):\n",
    "                            dom_counts[i] += 1\n",
    "\n",
    "                # Normalize to [-1, 1]\n",
    "                max_dom = NUM_CANDIDATES - 1\n",
    "                scalar_rewards = 2 * (dom_counts / max_dom) - 1\n",
    "                rewards.append(scalar_rewards)  \n",
    "\n",
    "            # Flatten for PPO\n",
    "            flat_queries = []\n",
    "            flat_responses = []\n",
    "            flat_rewards = []\n",
    "\n",
    "            for b in range(len(src_txt)):\n",
    "                for k in range(NUM_CANDIDATES):\n",
    "                    flat_queries.append(ids[b])\n",
    "                    flat_responses.append(outs[b, k])\n",
    "                    flat_rewards.append(torch.tensor([rewards[b][k]], device=DEVICE))\n",
    "\n",
    "            # PPO step with error handling\n",
    "            try:\n",
    "                stats = ppo_trainer.step(\n",
    "                    queries=flat_queries,\n",
    "                    responses=flat_responses,\n",
    "                    scores=flat_rewards\n",
    "                )\n",
    "            except ValueError as e:\n",
    "                print(f\"⚠️ PPO step failed (batch {batch_idx}): {str(e)}\")\n",
    "                print(\"Traceback:\", traceback.format_exc())\n",
    "                print(\"Skipping this batch and continuing...\")\n",
    "                exit()  # Skip to next batch\n",
    "\n",
    "            # Logging\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx}\")\n",
    "                \n",
    "        except Exception as e:  # Catch any other unexpected errors\n",
    "            print(f\"⚠️ Unexpected error in batch {batch_idx}: {str(e)}\")\n",
    "            print(\"Traceback:\", traceback.format_exc())\n",
    "            print(\"Skipping this batch and continuing...\")\n",
    "            exit()\n",
    "\n",
    "    print(f\"✅ Epoch {epoch+1}/3 complete\")\n",
    "\n",
    "print(\"🎉 PPO fine-tuning done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bad48f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0bad48f",
    "outputId": "ccde4117-6c85-49d3-db21-e0d77a29cd45"
   },
   "outputs": [],
   "source": [
    "# ###saving the model\n",
    "# # starcoder_model.save_pretrained(\"rhlfmodel/\")\n",
    "# # starcoder_tokenizer.save_pretrained(\"rhlfmodel/\")\n",
    "\n",
    "# ppo_trainer.model.pretrained_model.save_pretrained(\"D:\\kshitij-weights-folder\\qwen-aloe-rl-ppo-dom-10-5\")\n",
    "# tokenizer.save_pretrained(\"D:\\kshitij-weights-folder\\qwen-aloe-rl-ppo-dom-10-5\")\n",
    "\n",
    "# # if isinstance(ppo_trainer.model.pretrained_model, PeftModel):\n",
    "# #     ppo_trainer.model.pretrained_model.save_adapter(\n",
    "# #         \"D:/kshitij-weights-folder/qwen-aloe-rl-ppo-dom-10-5-lora\",\n",
    "# #         \"lora_adapter\"\n",
    "# #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48706b19-356f-463c-b07b-fb12cfb4c747",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "907f5127-9db1-4fbc-94cc-8eed816aefa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = r\"D:\\hf-cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1dc0cb2c-cddb-46f3-bfaf-7187cb15c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from trl import RewardTrainer\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig  \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60a0c73a-f47b-4375-b6c6-794ca2f13cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>note</th>\n",
       "      <th>source_file</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>aci</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[doctor] so tyler is a 56 -year-old male who p...</td>\n",
       "      <td>SUBJECTIVE\\n\\nDifficulty swallowing. Tyler Gre...</td>\n",
       "      <td>src_experiment_data\\test1_aci_asrcorr.csv</td>\n",
       "      <td>ACI084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset encounter_id                                           dialogue  \\\n",
       "246     aci          NaN  [doctor] so tyler is a 56 -year-old male who p...   \n",
       "\n",
       "                                                  note  \\\n",
       "246  SUBJECTIVE\\n\\nDifficulty swallowing. Tyler Gre...   \n",
       "\n",
       "                                   source_file      id  \n",
       "246  src_experiment_data\\test1_aci_asrcorr.csv  ACI084  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA = r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\combined_clinical_notes.csv\"\n",
    "df = pd.read_csv(DATA)\n",
    "\n",
    "# First split: 60% train, 40% temp (eval + test)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "\n",
    "# Second split: 50% of temp (which is 20% of total) for eval, 50% for test\n",
    "eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16e0a26b-d34a-41c4-83f4-f2ee6b577e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|█████████████| 4/4 [00:11<00:00,  2.87s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# -- Path to your LoRA weights + tokenizer --\n",
    "model_dir = r\"D:\\kshitij-weights-folder\\qwen-ppo-tuned-9-5\"  \n",
    "\n",
    "# -- 4-bit quantization config (same as training) --\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# -- 1) Load the *base* Qwen2.5 model in 4-bit --\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HPAI-BSC/Qwen2.5-Aloe-Beta-7B\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "# model = PeftModel.from_pretrained(model, adapter_model_name)\n",
    "\n",
    "# -- 2) Load your fine-tuned LoRA adapters into the base model --\n",
    "# The directory should contain adapter_model.bin, adapter_config.json, etc.\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# -- 3) Load the tokenizer you saved to ./aloe_qwen --\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce493f64-d50b-47f0-9ea7-32275b714475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prompt(conv):\n",
    "    prompt = f\"\"\"\n",
    "    CONVERSATION:\n",
    "    {conv}\n",
    "    \n",
    "    SUMMARY:\n",
    "    Please generate a medical summary based on the following clinical notes. The summary should include the following sections: Chief Complaint, History of Present Illness, Vitals, Physical Exam, Results, Assessment, Plan, and Instructions.\n",
    "    Please format the response as plain text, without using markdown or special formatting, and with clear headings for each section, like this:\n",
    "    \n",
    "    \n",
    "    CHIEF COMPLAINT\n",
    "    A concise statement of the patient's primary concern or reason for visiting the clinic.\n",
    "    \n",
    "    HISTORY OF PRESENT ILLNESS\n",
    "    A detailed narrative about the patient's symptoms, their onset, duration, and any relevant medical history or previous treatments.\n",
    "    \n",
    "    VITALS\n",
    "    Include any relevant vital signs (e.g., oxygen saturation, blood pressure) if available.\n",
    "    \n",
    "    PHYSICAL EXAM \n",
    "    Summarize the findings from the physical examination, including any notable abnormalities.\n",
    "    \n",
    "    RESULTS \n",
    "    Summarize the results of any diagnostic tests performed (e.g., lab work, imaging studies).\n",
    "    \n",
    "    ASSESSMENT\n",
    "    The doctor's assessment of the patient's condition or diagnosis.\n",
    "    \n",
    "    PLAN\n",
    "    The treatment plan, including prescribed medications, lifestyle recommendations, and follow-up instructions.\n",
    "    \n",
    "    INSTRUCTIONS\n",
    "    Specific instructions for the patient regarding their treatment plan and follow-up care.\n",
    "    \n",
    "    Important Note: If any section lacks relevant information or if specific details are not provided (e.g., vitals are not mentioned, no abnormal findings in the physical exam), omit that section from the generated summary. Only include sections for which there is sufficient information available.\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89c025c7-a863-4c92-8a93-ee663ebe8b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>note</th>\n",
       "      <th>source_file</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>aci</td>\n",
       "      <td>D2N053</td>\n",
       "      <td>[doctor] so barbara i i know you are here for ...</td>\n",
       "      <td>CHIEF COMPLAINT\\n\\nItchy scalp pain.\\n\\nREVIEW...</td>\n",
       "      <td>challenge_data\\train.csv</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset encounter_id                                           dialogue  \\\n",
       "172     aci       D2N053  [doctor] so barbara i i know you are here for ...   \n",
       "\n",
       "                                                  note  \\\n",
       "172  CHIEF COMPLAINT\\n\\nItchy scalp pain.\\n\\nREVIEW...   \n",
       "\n",
       "                  source_file   id  \n",
       "172  challenge_data\\train.csv  NaN  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = test_df\n",
    "\n",
    "eval_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfdfc560-3f25-4034-91cc-dc5d8dfbcfa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Generating Summaries:   0%|                          | 0/1 [00:00<?, ?it/s]C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\.conda\\envs\\kshitij\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:427: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "Generating Summaries: 100%|██████████████████| 1/1 [01:18<00:00, 78.91s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a directory for the output files if it doesn't exist\n",
    "output_dir = \"structured_predictions\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize summarizer pipeline\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lora_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "eval_df = df[:1]  # Using first 3 samples for evaluation\n",
    "# Prepare batching parameters\n",
    "batch_size = 4\n",
    "num_samples = len(eval_df)\n",
    "num_batches = (num_samples // batch_size) + int(num_samples % batch_size != 0)\n",
    "\n",
    "# Lists to store predictions and references\n",
    "predictions = []\n",
    "structured_predictions = []  # To store extracted sections\n",
    "references = []\n",
    "\n",
    "# Batching loop to generate summaries\n",
    "for i in tqdm(range(num_batches), desc=\"Generating Summaries\"):\n",
    "    start = i * batch_size\n",
    "    end = min(start + batch_size, num_samples)\n",
    "\n",
    "    batch_structured_predictions = []  \n",
    "    \n",
    "    # Extract conversation and reference summary columns\n",
    "    batch_conversations = eval_df[\"dialogue\"][start:end].tolist()\n",
    "    batch_refs = eval_df[\"note\"][start:end].tolist()\n",
    "\n",
    "    # Prepare prompts\n",
    "    prompts = [return_prompt(conv) for conv in batch_conversations]\n",
    "    \n",
    "    # Generate summaries\n",
    "    results = summarizer(\n",
    "        prompts,\n",
    "        max_new_tokens=900,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    \n",
    "    # Process results\n",
    "    for j, item in enumerate(results):\n",
    "        output_dict = item[0]\n",
    "        generated_text = output_dict[\"generated_text\"]\n",
    "        current_prompt = prompts[j]\n",
    "        \n",
    "        # Extract the generated summary (after prompt)\n",
    "        cleaned = generated_text[len(current_prompt):].strip()\n",
    "        predictions.append(cleaned)\n",
    "        \n",
    "        # Extract structured sections from the generated summary\n",
    "        # extracted_sections = extract_sections(cleaned)\n",
    "        # batch_structured_predictions.append(extracted_sections)\n",
    "        # structured_predictions.append(extracted_sections)\n",
    "    \n",
    "    # Append the reference texts\n",
    "    references.extend(batch_refs)\n",
    "    # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # batch_filename = f\"{output_dir}/batch_{i}_{timestamp}.json\"\n",
    "    \n",
    "    # with open(batch_filename, 'w') as f:\n",
    "    #     json.dump({\n",
    "    #         \"batch_number\": i,\n",
    "    #         \"start_index\": start,\n",
    "    #         \"end_index\": end,\n",
    "    #         \"structured_predictions\": batch_structured_predictions,\n",
    "    #         \"references\": batch_refs\n",
    "    #     }, f, indent=2)\n",
    "    \n",
    "    # print(f\"Saved batch {i} predictions to {batch_filename}\")\n",
    "\n",
    "\n",
    "# Now you have:\n",
    "# predictions - raw generated summaries\n",
    "# structured_predictions - dictionary of extracted sections for each summary\n",
    "# references - ground truth notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f1e50f3-aa0a-452a-b743-79fc7ab41bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHIEF COMPLAINT\n",
      "The patient presented with a chief complaint of high blood pressure, which was noted to be significantly elevated during an emergency room visit. She also reported experiencing lightheadedness and a recent episode of near-fainting while walking due to high blood pressure.\n",
      "\n",
      "HISTORY OF PRESENT ILLNESS\n",
      "Diane, a 28-year-old female with a history of depression and hypertension, presented for emergency room follow-up. She reported that her blood pressure was very high during the ER visit, reaching almost 200. She experienced a headache and lightheadedness, which led to a fall that was prevented by her boyfriend. Diane has a history of her blood pressure \"skyrocketing\" once a week or month, often associated with poor diet and lack of adherence to her medication regimen when traveling. She has been taking lisinopril as prescribed and has been compliant with her medication when at home. She recently started therapy for her depression, which she attends weekly, and reports having a strong support system.\n",
      "\n",
      "VITALS\n",
      "Blood pressure was recorded as 190/110 mmHg during the visit.\n",
      "\n",
      "PHYSICAL EXAM\n",
      "On examination, the patient had a slight two out of six systolic ejection murmur. Lung exam revealed clear lungs bilaterally, and there was trace pitting edema in both lower extremities. No carotid bruits were noted. An ECG showed changes consistent with chronic hypertension, and an echocardiogram confirmed a stable, slightly leaky heart valve.\n",
      "\n",
      "RESULTS\n",
      "Recent ECG and echocardiogram results were reviewed, showing no significant changes from previous assessments. Blood pressure readings from the emergency room visit were also reviewed.\n",
      "\n",
      "ASSESSMENT\n",
      "The patient's primary issues are uncontrolled hypertension and a stable, mild mitral regurgitation. Her depression is managed with ongoing psychotherapy, and she is compliant with her lisinopril when at home.\n",
      "\n",
      "PLAN\n",
      "The plan includes increasing the lisinopril dosage to 40 mg daily to better control her hypertension. She will continue to monitor her blood pressure at home and report these readings via the patient portal. If her blood pressure remains uncontrolled, a second antihypertensive medication may be added. For her depression, she will continue with her current therapy and can contact the doctor if additional support is needed. No new medications are indicated for her depression at this time.\n",
      "\n",
      "INSTRUCTIONS\n",
      "The patient is instructed to maintain her current lisinopril regimen and increase her dosage to 40 mg daily. She should monitor her blood pressure regularly and report these readings to the doctor. She is advised to continue attending her weekly psychotherapy sessions and to reach out to the doctor if her depression worsens or if she requires additional support. Follow-up in one month is recommended to assess the effectiveness of the increased lisinopril dosage and to further manage her hypertension and depression.  CHIEF COMPLAINT\n",
      "The patient presented with a chief complaint of high blood pressure, which was noted to be significantly elevated during an emergency room visit. She also reported experiencing lightheadedness and a recent episode of near-fainting while walking due to high blood pressure.\n",
      "\n",
      "HISTORY OF PRESENT ILLNESS\n",
      "Diane, a 28-year-old female with a history of depression and hypertension, presented for emergency room follow-up. She reported that her blood pressure was very high during the ER visit, reaching almost 200. She experienced a headache and lightheadedness, which led to a fall that was prevented by her boyfriend. Diane has a history of her blood pressure \"skyrocketing\" once a week or month, often associated with poor diet and lack of adherence to her medication regimen when traveling. She has been taking lisinopril as prescribed and has been compliant with her medication when at home. She recently started therapy for her depression, which she attends weekly, and reports having a strong support system.\n",
      "\n",
      "VITALS\n",
      "Blood pressure was recorded as 190/110 mmHg during the visit.\n",
      "\n",
      "PHYSICAL EXAM\n",
      "On examination, the patient had a slight two out of six systolic ejection murmur. Lung exam revealed clear lungs bilaterally, and there was trace pitting edema in both lower extremities. No carotid bruits were noted. An ECG showed changes consistent\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3fdb7c-91af-4cad-9649-13d028a754af",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list = eval_df[\"dialogue\"].tolist()\n",
    "ref_list = eval_df[\"note\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105b41e9-d8ce-44aa-85f5-9529a2db946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = []\n",
    "for pred in predictions:\n",
    "    # Ensure that \"Summary:\" exists in the string to avoid errors\n",
    "    if len(pred) > 0:\n",
    "        output_list.append(pred)\n",
    "    else:\n",
    "        # Handle cases where \"Summary:\" is missing (optional)\n",
    "        output_list.append(\"\")  # Or handle differently based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efead8-7a4c-4937-b7de-6a5fa448fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\BMSCE CSE.DESKTOP-IUB6THA\\Downloads\\kshitij\\UniEval\")\n",
    "\n",
    "data = convert_to_json(\n",
    "    src_list=src_list,\n",
    "    ref_list=ref_list,\n",
    "    output_list=output_list\n",
    ")\n",
    "\n",
    "filtered_data = [\n",
    "    entry for entry in data\n",
    "    if entry[\"system_output\"].strip()  # Ensure non-empty system_output\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218e277-c0fa-4255-a3af-e2af86924bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef4c1da-dc68-478a-8f9a-363c52128653",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'summarization'\n",
    "\n",
    "# Initialize evaluator for a specific task\n",
    "evaluator = get_evaluator(task)\n",
    "evaluator1 = get_evaluator('fact')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c09e1f6-0c85-4b1c-9887-6574692481fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multi-dimensional evaluation scores\n",
    "eval_scores = evaluator.evaluate(data, print_result=True, dims=['consistency', 'fluency', 'relevance','coherence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9cb83d-e11c-45c0-ad86-09dd6ef13fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores1 = evaluator1.evaluate(data, print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103bbab-84c7-4dcc-b742-63461bc28822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load JSON data from a file\n",
    "with open(\"structured_predictions/batch_0_20250508_200636.json\", 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04126023-5b07-465b-90cc-97b086188af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in data['structured_predictions'][1].items():\n",
    "    print(f\"{key}:\\n{value}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (kshitij)",
   "language": "python",
   "name": "kshitij"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
